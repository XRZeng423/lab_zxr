{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Import, Analysis and Clean up for Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sample text from the book -->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone \\n\\nCHAPTER ONE \\n\\nTHE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \\n\\nMr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. \\n\\nThe Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('HPBook1.txt', 'r', encoding = 'utf-8')\n",
    "text = text.read()\n",
    "\n",
    "print('Some sample text from the book -->')\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in book : 78449\n",
      "Total characters in book : 442744\n",
      "Unique words in book : 11897\n",
      "Unique characters in book : 80\n",
      "Total Paragraphs : 3033\n"
     ]
    }
   ],
   "source": [
    "total_words = len(text.split())\n",
    "total_characters = len(text)\n",
    "unique_words = len(set(text.split()))\n",
    "unique_characters = len(set(text))\n",
    "paragraphs = text.split('\\n\\n')\n",
    "\n",
    "print (\"Total words in book :\", total_words)\n",
    "print (\"Total characters in book :\", total_characters)\n",
    "print (\"Unique words in book :\", unique_words)\n",
    "print (\"Unique characters in book :\", unique_characters)\n",
    "print (\"Total Paragraphs :\", len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_words = Counter()\n",
    "for i in range(len(paragraphs)):\n",
    "    for x in paragraphs[i]:\n",
    "        most_words[x] +=1\n",
    "\n",
    "        \n",
    "#most_words.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c : i for i,c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "embeddings = np.array([vocab_to_int[i] for i in text], dtype=np.int32)\n",
    "\n",
    "#vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples encodings (inputs to our NN) -->>\n",
      "Harry Potter and the Sorcerer's Stone \n",
      "\n",
      "CHAPTER ONE \n",
      "\n",
      "THE BOY WHO LIVED \n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet D\n",
      "[31 51 68 68 75  1 39 65 70 70 55 68  1 51 64 54  1 70 58 55  1 42 65 68\n",
      " 53 55 68 55 68  4 69  1 42 70 65 64 55  1  0  0 26 31 24 39 43 28 41  1\n",
      " 38 37 28  1  0  0 43 31 28  1 25 38 48  1 46 31 38  1 35 32 45 28 27  1\n",
      "  0  0 36 68 10  1 51 64 54  1 36 68 69 10  1 27 71 68 69 62 55 75  8  1\n",
      " 65 56  1 64 71 63 52 55 68  1 56 65 71 68  8  1 39 68 59 72 55 70  1 27]\n"
     ]
    }
   ],
   "source": [
    "print (\"Examples encodings (inputs to our NN) -->>\")\n",
    "print (text[:120])\n",
    "print (embeddings[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with LSTM Model Build and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def placeholders(batch_size, step_size):\n",
    "    inputs = tf.placeholder(tf.int32, shape=(batch_size, step_size), name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=(batch_size, step_size), name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name= 'prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstms(lstm_size, layers, batch_size, keep_prob):\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(layers)])\n",
    "    state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, state\n",
    "\n",
    "def rnn_output(lstm_output, input_size, output_size):\n",
    "    sequence = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(sequence, [-1, input_size])\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=(input_size, output_size), stddev=0.1))\n",
    "        bias = tf.Variable(tf.zeros(output_size))\n",
    "        \n",
    "    logits = tf.matmul(x, weights) + bias\n",
    "    predictions = tf.nn.softmax(logits, name='predictions')                          \n",
    "    \n",
    "    return predictions, logits\n",
    "\n",
    "def losses(logits, y, lstm_size, num_classes):\n",
    "    labels = tf.reshape(tf.one_hot(y, num_classes), logits.get_shape())\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 51 68 68 75  1 39 65 70 70]\n",
      " [ 1 69 66 65 61 55  1 51 52 65]\n",
      " [70 69  1 52 55 51 61 10  1  0]\n",
      " [64 65 69 55 10  1  0  0  3 48]\n",
      " [70 58 55 59 68  1 56 51 53 55]\n",
      " [56 65 75 69  1 69 64 55 55 68]\n",
      " [64 70  1 57 51 59 64 69  1 70]\n",
      " [ 1 51 64 65 70 58 55 68  1 68]\n",
      " [51 68 54  1 58 59 63  1 51 64]\n",
      " [65 62 65 57 75  8  1 31 55 68]]\n",
      "\n",
      "y\n",
      " [[51 68 68 75  1 39 65 70 70 55]\n",
      " [69 66 65 61 55  1 51 52 65 71]\n",
      " [69  1 52 55 51 61 10  1  0  0]\n",
      " [65 69 55 10  1  0  0  3 48 55]\n",
      " [58 55 59 68  1 56 51 53 55 69]\n",
      " [65 75 69  1 69 64 55 55 68 59]\n",
      " [70  1 57 51 59 64 69  1 70 58]\n",
      " [51 64 65 70 58 55 68  1 68 55]\n",
      " [68 54  1 58 59 63  1 51 64 54]\n",
      " [62 65 57 75  8  1 31 55 68 63]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_steps = 100        \n",
    "lstm_size = 512 \n",
    "num_layers = 2         \n",
    "learning_rate = 0.001  \n",
    "prob = 0.5 \n",
    "grad_clip = 5\n",
    "num_classes = len(vocab)\n",
    "\n",
    "\n",
    "def generate_batches(inputs, batch_size, num_steps):\n",
    "    char_batch = batch_size * num_steps\n",
    "    num_batches = len(inputs)//char_batch\n",
    "    \n",
    "    idx = char_batch * num_batches \n",
    "    inputs = inputs[:idx]\n",
    "    inputs = inputs.reshape((batch_size, -1))\n",
    "    \n",
    "    for i in range(0, inputs.shape[1], num_steps):\n",
    "        x = inputs[:, i : i+num_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        \n",
    "        yield x, y\n",
    "        \n",
    "batches = generate_batches(embeddings, 10, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-a362c9c6f237>:4: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-8-a362c9c6f237>:8: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-10-f819b2969627>:14: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f007bafc358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f007bafc358>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f007bafc358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f007bafc358>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00898e3198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00898e3198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00898e3198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00898e3198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00898e12e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00898e12e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00898e12e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00898e12e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-8-a362c9c6f237>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "def cal_optimizer(loss, learning_rate, clipping):\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), clipping)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "inputs, targets, keep_prob = placeholders(batch_size, num_steps)\n",
    "cell, initial_state = lstms(lstm_size, num_layers, batch_size, keep_prob)\n",
    "one_hot_inputs = tf.one_hot(inputs, num_classes)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell, one_hot_inputs, initial_state = initial_state)\n",
    "final_state = state\n",
    "predictions, logits = rnn_output(outputs, lstm_size, num_classes)\n",
    "loss = losses(logits, targets, lstm_size, num_classes)\n",
    "optimizer = cal_optimizer(loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100...  Training Step: 1...  Training loss: 4.3839...  0.6318 sec/batch\n",
      "Epoch: 1/100...  Training Step: 2...  Training loss: 4.3027...  0.4848 sec/batch\n",
      "Epoch: 1/100...  Training Step: 3...  Training loss: 3.8550...  0.4845 sec/batch\n",
      "Epoch: 1/100...  Training Step: 4...  Training loss: 5.5001...  0.4903 sec/batch\n",
      "Epoch: 1/100...  Training Step: 5...  Training loss: 4.1873...  0.4768 sec/batch\n",
      "Epoch: 1/100...  Training Step: 6...  Training loss: 3.8436...  0.5012 sec/batch\n",
      "Epoch: 1/100...  Training Step: 7...  Training loss: 3.7455...  0.6614 sec/batch\n",
      "Epoch: 1/100...  Training Step: 8...  Training loss: 3.6523...  0.5864 sec/batch\n",
      "Epoch: 1/100...  Training Step: 9...  Training loss: 3.5230...  0.5633 sec/batch\n",
      "Epoch: 1/100...  Training Step: 10...  Training loss: 3.4486...  0.4742 sec/batch\n",
      "Epoch: 1/100...  Training Step: 11...  Training loss: 3.4354...  0.5082 sec/batch\n",
      "Epoch: 1/100...  Training Step: 12...  Training loss: 3.4255...  0.5222 sec/batch\n",
      "Epoch: 1/100...  Training Step: 13...  Training loss: 3.4158...  0.5267 sec/batch\n",
      "Epoch: 1/100...  Training Step: 14...  Training loss: 3.3487...  0.5369 sec/batch\n",
      "Epoch: 1/100...  Training Step: 15...  Training loss: 3.3279...  0.5209 sec/batch\n",
      "Epoch: 1/100...  Training Step: 16...  Training loss: 3.3313...  0.4934 sec/batch\n",
      "Epoch: 1/100...  Training Step: 17...  Training loss: 3.2962...  0.4789 sec/batch\n",
      "Epoch: 1/100...  Training Step: 18...  Training loss: 3.3233...  0.5011 sec/batch\n",
      "Epoch: 1/100...  Training Step: 19...  Training loss: 3.3007...  0.4999 sec/batch\n",
      "Epoch: 1/100...  Training Step: 20...  Training loss: 3.2972...  0.5386 sec/batch\n",
      "Epoch: 1/100...  Training Step: 21...  Training loss: 3.2672...  0.5127 sec/batch\n",
      "Epoch: 1/100...  Training Step: 22...  Training loss: 3.2791...  0.5027 sec/batch\n",
      "Epoch: 1/100...  Training Step: 23...  Training loss: 3.2567...  0.4899 sec/batch\n",
      "Epoch: 1/100...  Training Step: 24...  Training loss: 3.2935...  0.4918 sec/batch\n",
      "Epoch: 1/100...  Training Step: 25...  Training loss: 3.2480...  0.4876 sec/batch\n",
      "Epoch: 1/100...  Training Step: 26...  Training loss: 3.2655...  0.4955 sec/batch\n",
      "Epoch: 1/100...  Training Step: 27...  Training loss: 3.2890...  0.4728 sec/batch\n",
      "Epoch: 1/100...  Training Step: 28...  Training loss: 3.2314...  0.4789 sec/batch\n",
      "Epoch: 1/100...  Training Step: 29...  Training loss: 3.2409...  0.4844 sec/batch\n",
      "Epoch: 1/100...  Training Step: 30...  Training loss: 3.2836...  0.4776 sec/batch\n",
      "Epoch: 1/100...  Training Step: 31...  Training loss: 3.2596...  0.4655 sec/batch\n",
      "Epoch: 1/100...  Training Step: 32...  Training loss: 3.2436...  0.4651 sec/batch\n",
      "Epoch: 1/100...  Training Step: 33...  Training loss: 3.2318...  0.4848 sec/batch\n",
      "Epoch: 1/100...  Training Step: 34...  Training loss: 3.2449...  0.4559 sec/batch\n",
      "Epoch: 2/100...  Training Step: 35...  Training loss: 3.3338...  0.4415 sec/batch\n",
      "Epoch: 2/100...  Training Step: 36...  Training loss: 3.2228...  0.4601 sec/batch\n",
      "Epoch: 2/100...  Training Step: 37...  Training loss: 3.2158...  0.4718 sec/batch\n",
      "Epoch: 2/100...  Training Step: 38...  Training loss: 3.2186...  0.4576 sec/batch\n",
      "Epoch: 2/100...  Training Step: 39...  Training loss: 3.2323...  0.4524 sec/batch\n",
      "Epoch: 2/100...  Training Step: 40...  Training loss: 3.2126...  0.4493 sec/batch\n",
      "Epoch: 2/100...  Training Step: 41...  Training loss: 3.2073...  0.4586 sec/batch\n",
      "Epoch: 2/100...  Training Step: 42...  Training loss: 3.2272...  0.4501 sec/batch\n",
      "Epoch: 2/100...  Training Step: 43...  Training loss: 3.2066...  0.4681 sec/batch\n",
      "Epoch: 2/100...  Training Step: 44...  Training loss: 3.2226...  0.4637 sec/batch\n",
      "Epoch: 2/100...  Training Step: 45...  Training loss: 3.2180...  0.4609 sec/batch\n",
      "Epoch: 2/100...  Training Step: 46...  Training loss: 3.2067...  0.4573 sec/batch\n",
      "Epoch: 2/100...  Training Step: 47...  Training loss: 3.2242...  0.4594 sec/batch\n",
      "Epoch: 2/100...  Training Step: 48...  Training loss: 3.1705...  0.4644 sec/batch\n",
      "Epoch: 2/100...  Training Step: 49...  Training loss: 3.1793...  0.4598 sec/batch\n",
      "Epoch: 2/100...  Training Step: 50...  Training loss: 3.1986...  0.4562 sec/batch\n",
      "Epoch: 2/100...  Training Step: 51...  Training loss: 3.1767...  0.4596 sec/batch\n",
      "Epoch: 2/100...  Training Step: 52...  Training loss: 3.2094...  0.4599 sec/batch\n",
      "Epoch: 2/100...  Training Step: 53...  Training loss: 3.1961...  0.4615 sec/batch\n",
      "Epoch: 2/100...  Training Step: 54...  Training loss: 3.1861...  0.4503 sec/batch\n",
      "Epoch: 2/100...  Training Step: 55...  Training loss: 3.1741...  0.4467 sec/batch\n",
      "Epoch: 2/100...  Training Step: 56...  Training loss: 3.1847...  0.4676 sec/batch\n",
      "Epoch: 2/100...  Training Step: 57...  Training loss: 3.1653...  0.4641 sec/batch\n",
      "Epoch: 2/100...  Training Step: 58...  Training loss: 3.2008...  0.4687 sec/batch\n",
      "Epoch: 2/100...  Training Step: 59...  Training loss: 3.1609...  0.4670 sec/batch\n",
      "Epoch: 2/100...  Training Step: 60...  Training loss: 3.1887...  0.4665 sec/batch\n",
      "Epoch: 2/100...  Training Step: 61...  Training loss: 3.1989...  0.4589 sec/batch\n",
      "Epoch: 2/100...  Training Step: 62...  Training loss: 3.1594...  0.4615 sec/batch\n",
      "Epoch: 2/100...  Training Step: 63...  Training loss: 3.1678...  0.4626 sec/batch\n",
      "Epoch: 2/100...  Training Step: 64...  Training loss: 3.2179...  0.4648 sec/batch\n",
      "Epoch: 2/100...  Training Step: 65...  Training loss: 3.1908...  0.4600 sec/batch\n",
      "Epoch: 2/100...  Training Step: 66...  Training loss: 3.1706...  0.4549 sec/batch\n",
      "Epoch: 2/100...  Training Step: 67...  Training loss: 3.1545...  0.4691 sec/batch\n",
      "Epoch: 2/100...  Training Step: 68...  Training loss: 3.2805...  0.4660 sec/batch\n",
      "Epoch: 3/100...  Training Step: 69...  Training loss: 3.3161...  0.4741 sec/batch\n",
      "Epoch: 3/100...  Training Step: 70...  Training loss: 3.2712...  0.4947 sec/batch\n",
      "Epoch: 3/100...  Training Step: 71...  Training loss: 3.2298...  0.4693 sec/batch\n",
      "Epoch: 3/100...  Training Step: 72...  Training loss: 3.1683...  0.4679 sec/batch\n",
      "Epoch: 3/100...  Training Step: 73...  Training loss: 3.1682...  0.4640 sec/batch\n",
      "Epoch: 3/100...  Training Step: 74...  Training loss: 3.1554...  0.4678 sec/batch\n",
      "Epoch: 3/100...  Training Step: 75...  Training loss: 3.1478...  0.4553 sec/batch\n",
      "Epoch: 3/100...  Training Step: 76...  Training loss: 3.1567...  0.4680 sec/batch\n",
      "Epoch: 3/100...  Training Step: 77...  Training loss: 3.1392...  0.4672 sec/batch\n",
      "Epoch: 3/100...  Training Step: 78...  Training loss: 3.1518...  0.4677 sec/batch\n",
      "Epoch: 3/100...  Training Step: 79...  Training loss: 3.1475...  0.4716 sec/batch\n",
      "Epoch: 3/100...  Training Step: 80...  Training loss: 3.1476...  0.4766 sec/batch\n",
      "Epoch: 3/100...  Training Step: 81...  Training loss: 3.1684...  0.4613 sec/batch\n",
      "Epoch: 3/100...  Training Step: 82...  Training loss: 3.1252...  0.4608 sec/batch\n",
      "Epoch: 3/100...  Training Step: 83...  Training loss: 3.1154...  0.4602 sec/batch\n",
      "Epoch: 3/100...  Training Step: 84...  Training loss: 3.1286...  0.4471 sec/batch\n",
      "Epoch: 3/100...  Training Step: 85...  Training loss: 3.1157...  0.4658 sec/batch\n",
      "Epoch: 3/100...  Training Step: 86...  Training loss: 3.1543...  0.4506 sec/batch\n",
      "Epoch: 3/100...  Training Step: 87...  Training loss: 3.1305...  0.4582 sec/batch\n",
      "Epoch: 3/100...  Training Step: 88...  Training loss: 3.1176...  0.4636 sec/batch\n",
      "Epoch: 3/100...  Training Step: 89...  Training loss: 3.0985...  0.4400 sec/batch\n",
      "Epoch: 3/100...  Training Step: 90...  Training loss: 3.1105...  0.4527 sec/batch\n",
      "Epoch: 3/100...  Training Step: 91...  Training loss: 3.0866...  0.4434 sec/batch\n",
      "Epoch: 3/100...  Training Step: 92...  Training loss: 3.1165...  0.4551 sec/batch\n",
      "Epoch: 3/100...  Training Step: 93...  Training loss: 3.0741...  0.4543 sec/batch\n",
      "Epoch: 3/100...  Training Step: 94...  Training loss: 3.0910...  0.4593 sec/batch\n",
      "Epoch: 3/100...  Training Step: 95...  Training loss: 3.0934...  0.4618 sec/batch\n",
      "Epoch: 3/100...  Training Step: 96...  Training loss: 3.0452...  0.4708 sec/batch\n",
      "Epoch: 3/100...  Training Step: 97...  Training loss: 3.0898...  0.4607 sec/batch\n",
      "Epoch: 3/100...  Training Step: 98...  Training loss: 3.1204...  0.4643 sec/batch\n",
      "Epoch: 3/100...  Training Step: 99...  Training loss: 3.1127...  0.4623 sec/batch\n",
      "Epoch: 3/100...  Training Step: 100...  Training loss: 3.0758...  0.4607 sec/batch\n",
      "Epoch: 3/100...  Training Step: 101...  Training loss: 3.0578...  0.4633 sec/batch\n",
      "Epoch: 3/100...  Training Step: 102...  Training loss: 3.0731...  0.4513 sec/batch\n",
      "Epoch: 4/100...  Training Step: 103...  Training loss: 3.1239...  0.4615 sec/batch\n",
      "Epoch: 4/100...  Training Step: 104...  Training loss: 3.0454...  0.4574 sec/batch\n",
      "Epoch: 4/100...  Training Step: 105...  Training loss: 3.0323...  0.4529 sec/batch\n",
      "Epoch: 4/100...  Training Step: 106...  Training loss: 3.0381...  0.4574 sec/batch\n",
      "Epoch: 4/100...  Training Step: 107...  Training loss: 3.0246...  0.4531 sec/batch\n",
      "Epoch: 4/100...  Training Step: 108...  Training loss: 3.0102...  0.4581 sec/batch\n",
      "Epoch: 4/100...  Training Step: 109...  Training loss: 2.9909...  0.4563 sec/batch\n",
      "Epoch: 4/100...  Training Step: 110...  Training loss: 2.9957...  0.4510 sec/batch\n",
      "Epoch: 4/100...  Training Step: 111...  Training loss: 2.9584...  0.4792 sec/batch\n",
      "Epoch: 4/100...  Training Step: 112...  Training loss: 2.9683...  0.4540 sec/batch\n",
      "Epoch: 4/100...  Training Step: 113...  Training loss: 2.9671...  0.4519 sec/batch\n",
      "Epoch: 4/100...  Training Step: 114...  Training loss: 2.9453...  0.4566 sec/batch\n",
      "Epoch: 4/100...  Training Step: 115...  Training loss: 2.9480...  0.4663 sec/batch\n",
      "Epoch: 4/100...  Training Step: 116...  Training loss: 2.8961...  0.4575 sec/batch\n",
      "Epoch: 4/100...  Training Step: 117...  Training loss: 3.0343...  0.4551 sec/batch\n",
      "Epoch: 4/100...  Training Step: 118...  Training loss: 2.9803...  0.4566 sec/batch\n",
      "Epoch: 4/100...  Training Step: 119...  Training loss: 2.9734...  0.4508 sec/batch\n",
      "Epoch: 4/100...  Training Step: 120...  Training loss: 2.9739...  0.4540 sec/batch\n",
      "Epoch: 4/100...  Training Step: 121...  Training loss: 2.9433...  0.4396 sec/batch\n",
      "Epoch: 4/100...  Training Step: 122...  Training loss: 2.9283...  0.4473 sec/batch\n",
      "Epoch: 4/100...  Training Step: 123...  Training loss: 2.9196...  0.4747 sec/batch\n",
      "Epoch: 4/100...  Training Step: 124...  Training loss: 2.9218...  0.4590 sec/batch\n",
      "Epoch: 4/100...  Training Step: 125...  Training loss: 2.9025...  0.4533 sec/batch\n",
      "Epoch: 4/100...  Training Step: 126...  Training loss: 2.9247...  0.4531 sec/batch\n",
      "Epoch: 4/100...  Training Step: 127...  Training loss: 2.8734...  0.4528 sec/batch\n",
      "Epoch: 4/100...  Training Step: 128...  Training loss: 2.8930...  0.4521 sec/batch\n",
      "Epoch: 4/100...  Training Step: 129...  Training loss: 2.8972...  0.4531 sec/batch\n",
      "Epoch: 4/100...  Training Step: 130...  Training loss: 2.8483...  0.4598 sec/batch\n",
      "Epoch: 4/100...  Training Step: 131...  Training loss: 2.8523...  0.4448 sec/batch\n",
      "Epoch: 4/100...  Training Step: 132...  Training loss: 2.8965...  0.4438 sec/batch\n",
      "Epoch: 4/100...  Training Step: 133...  Training loss: 2.8558...  0.4371 sec/batch\n",
      "Epoch: 4/100...  Training Step: 134...  Training loss: 2.8448...  0.4490 sec/batch\n",
      "Epoch: 4/100...  Training Step: 135...  Training loss: 2.8384...  0.4689 sec/batch\n",
      "Epoch: 4/100...  Training Step: 136...  Training loss: 2.8260...  0.4622 sec/batch\n",
      "Epoch: 5/100...  Training Step: 137...  Training loss: 2.8819...  0.4613 sec/batch\n",
      "Epoch: 5/100...  Training Step: 138...  Training loss: 2.8085...  0.4499 sec/batch\n",
      "Epoch: 5/100...  Training Step: 139...  Training loss: 2.7934...  0.4597 sec/batch\n",
      "Epoch: 5/100...  Training Step: 140...  Training loss: 2.8126...  0.4375 sec/batch\n",
      "Epoch: 5/100...  Training Step: 141...  Training loss: 2.8307...  0.4540 sec/batch\n",
      "Epoch: 5/100...  Training Step: 142...  Training loss: 2.7807...  0.4549 sec/batch\n",
      "Epoch: 5/100...  Training Step: 143...  Training loss: 2.7878...  0.4435 sec/batch\n",
      "Epoch: 5/100...  Training Step: 144...  Training loss: 2.7737...  0.4709 sec/batch\n",
      "Epoch: 5/100...  Training Step: 145...  Training loss: 2.7676...  0.4625 sec/batch\n",
      "Epoch: 5/100...  Training Step: 146...  Training loss: 2.7713...  0.4596 sec/batch\n",
      "Epoch: 5/100...  Training Step: 147...  Training loss: 2.7807...  0.4566 sec/batch\n",
      "Epoch: 5/100...  Training Step: 148...  Training loss: 2.7551...  0.4596 sec/batch\n",
      "Epoch: 5/100...  Training Step: 149...  Training loss: 2.7709...  0.4556 sec/batch\n",
      "Epoch: 5/100...  Training Step: 150...  Training loss: 2.7032...  0.4474 sec/batch\n",
      "Epoch: 5/100...  Training Step: 151...  Training loss: 2.7027...  0.4603 sec/batch\n",
      "Epoch: 5/100...  Training Step: 152...  Training loss: 2.7327...  0.4569 sec/batch\n",
      "Epoch: 5/100...  Training Step: 153...  Training loss: 2.7157...  0.4525 sec/batch\n",
      "Epoch: 5/100...  Training Step: 154...  Training loss: 2.7301...  0.4552 sec/batch\n",
      "Epoch: 5/100...  Training Step: 155...  Training loss: 2.7027...  0.4595 sec/batch\n",
      "Epoch: 5/100...  Training Step: 156...  Training loss: 2.6698...  0.4659 sec/batch\n",
      "Epoch: 5/100...  Training Step: 157...  Training loss: 2.6734...  0.4543 sec/batch\n",
      "Epoch: 5/100...  Training Step: 158...  Training loss: 2.6756...  0.4535 sec/batch\n",
      "Epoch: 5/100...  Training Step: 159...  Training loss: 2.6668...  0.4523 sec/batch\n",
      "Epoch: 5/100...  Training Step: 160...  Training loss: 2.6790...  0.4556 sec/batch\n",
      "Epoch: 5/100...  Training Step: 161...  Training loss: 2.6287...  0.4643 sec/batch\n",
      "Epoch: 5/100...  Training Step: 162...  Training loss: 2.6467...  0.4549 sec/batch\n",
      "Epoch: 5/100...  Training Step: 163...  Training loss: 2.6456...  0.4530 sec/batch\n",
      "Epoch: 5/100...  Training Step: 164...  Training loss: 2.6355...  0.4505 sec/batch\n",
      "Epoch: 5/100...  Training Step: 165...  Training loss: 2.6396...  0.4594 sec/batch\n",
      "Epoch: 5/100...  Training Step: 166...  Training loss: 2.6633...  0.4597 sec/batch\n",
      "Epoch: 5/100...  Training Step: 167...  Training loss: 2.6362...  0.4503 sec/batch\n",
      "Epoch: 5/100...  Training Step: 168...  Training loss: 2.6198...  0.4546 sec/batch\n",
      "Epoch: 5/100...  Training Step: 169...  Training loss: 2.6278...  0.4519 sec/batch\n",
      "Epoch: 5/100...  Training Step: 170...  Training loss: 2.6029...  0.4534 sec/batch\n",
      "Epoch: 6/100...  Training Step: 171...  Training loss: 2.6619...  0.4623 sec/batch\n",
      "Epoch: 6/100...  Training Step: 172...  Training loss: 2.5865...  0.4436 sec/batch\n",
      "Epoch: 6/100...  Training Step: 173...  Training loss: 2.5781...  0.4540 sec/batch\n",
      "Epoch: 6/100...  Training Step: 174...  Training loss: 2.5828...  0.4658 sec/batch\n",
      "Epoch: 6/100...  Training Step: 175...  Training loss: 2.5779...  0.4618 sec/batch\n",
      "Epoch: 6/100...  Training Step: 176...  Training loss: 2.5474...  0.4575 sec/batch\n",
      "Epoch: 6/100...  Training Step: 177...  Training loss: 2.5566...  0.4598 sec/batch\n",
      "Epoch: 6/100...  Training Step: 178...  Training loss: 2.5425...  0.4557 sec/batch\n",
      "Epoch: 6/100...  Training Step: 179...  Training loss: 2.5345...  0.4526 sec/batch\n",
      "Epoch: 6/100...  Training Step: 180...  Training loss: 2.5274...  0.4617 sec/batch\n",
      "Epoch: 6/100...  Training Step: 181...  Training loss: 2.5504...  0.4364 sec/batch\n",
      "Epoch: 6/100...  Training Step: 182...  Training loss: 2.5307...  0.4599 sec/batch\n",
      "Epoch: 6/100...  Training Step: 183...  Training loss: 2.5374...  0.4600 sec/batch\n",
      "Epoch: 6/100...  Training Step: 184...  Training loss: 2.4711...  0.4544 sec/batch\n",
      "Epoch: 6/100...  Training Step: 185...  Training loss: 2.4785...  0.4584 sec/batch\n",
      "Epoch: 6/100...  Training Step: 186...  Training loss: 2.5254...  0.4569 sec/batch\n",
      "Epoch: 6/100...  Training Step: 187...  Training loss: 2.4930...  0.4609 sec/batch\n",
      "Epoch: 6/100...  Training Step: 188...  Training loss: 2.5145...  0.4512 sec/batch\n",
      "Epoch: 6/100...  Training Step: 189...  Training loss: 2.4833...  0.4518 sec/batch\n",
      "Epoch: 6/100...  Training Step: 190...  Training loss: 2.4615...  0.4539 sec/batch\n",
      "Epoch: 6/100...  Training Step: 191...  Training loss: 2.4567...  0.4525 sec/batch\n",
      "Epoch: 6/100...  Training Step: 192...  Training loss: 2.4501...  0.4538 sec/batch\n",
      "Epoch: 6/100...  Training Step: 193...  Training loss: 2.4515...  0.4626 sec/batch\n",
      "Epoch: 6/100...  Training Step: 194...  Training loss: 2.4799...  0.4333 sec/batch\n",
      "Epoch: 6/100...  Training Step: 195...  Training loss: 2.4374...  0.4553 sec/batch\n",
      "Epoch: 6/100...  Training Step: 196...  Training loss: 2.4590...  0.4444 sec/batch\n",
      "Epoch: 6/100...  Training Step: 197...  Training loss: 2.4464...  0.4565 sec/batch\n",
      "Epoch: 6/100...  Training Step: 198...  Training loss: 2.4181...  0.4577 sec/batch\n",
      "Epoch: 6/100...  Training Step: 199...  Training loss: 2.4343...  0.4539 sec/batch\n",
      "Epoch: 6/100...  Training Step: 200...  Training loss: 2.4742...  0.4577 sec/batch\n",
      "Epoch: 6/100...  Training Step: 201...  Training loss: 2.4435...  0.4443 sec/batch\n",
      "Epoch: 6/100...  Training Step: 202...  Training loss: 2.4423...  0.4516 sec/batch\n",
      "Epoch: 6/100...  Training Step: 203...  Training loss: 2.4404...  0.4485 sec/batch\n",
      "Epoch: 6/100...  Training Step: 204...  Training loss: 2.4284...  0.4558 sec/batch\n",
      "Epoch: 7/100...  Training Step: 205...  Training loss: 2.5147...  0.4599 sec/batch\n",
      "Epoch: 7/100...  Training Step: 206...  Training loss: 2.4136...  0.4526 sec/batch\n",
      "Epoch: 7/100...  Training Step: 207...  Training loss: 2.4269...  0.4500 sec/batch\n",
      "Epoch: 7/100...  Training Step: 208...  Training loss: 2.4357...  0.4503 sec/batch\n",
      "Epoch: 7/100...  Training Step: 209...  Training loss: 2.4237...  0.4565 sec/batch\n",
      "Epoch: 7/100...  Training Step: 210...  Training loss: 2.3901...  0.4562 sec/batch\n",
      "Epoch: 7/100...  Training Step: 211...  Training loss: 2.4075...  0.4583 sec/batch\n",
      "Epoch: 7/100...  Training Step: 212...  Training loss: 2.3967...  0.4539 sec/batch\n",
      "Epoch: 7/100...  Training Step: 213...  Training loss: 2.3883...  0.4506 sec/batch\n",
      "Epoch: 7/100...  Training Step: 214...  Training loss: 2.3884...  0.4585 sec/batch\n",
      "Epoch: 7/100...  Training Step: 215...  Training loss: 2.4069...  0.4550 sec/batch\n",
      "Epoch: 7/100...  Training Step: 216...  Training loss: 2.4059...  0.4610 sec/batch\n",
      "Epoch: 7/100...  Training Step: 217...  Training loss: 2.4118...  0.4543 sec/batch\n",
      "Epoch: 7/100...  Training Step: 218...  Training loss: 2.3466...  0.4548 sec/batch\n",
      "Epoch: 7/100...  Training Step: 219...  Training loss: 2.3640...  0.4484 sec/batch\n",
      "Epoch: 7/100...  Training Step: 220...  Training loss: 2.3951...  0.4522 sec/batch\n",
      "Epoch: 7/100...  Training Step: 221...  Training loss: 2.3745...  0.4632 sec/batch\n",
      "Epoch: 7/100...  Training Step: 222...  Training loss: 2.3824...  0.4521 sec/batch\n",
      "Epoch: 7/100...  Training Step: 223...  Training loss: 2.3680...  0.4630 sec/batch\n",
      "Epoch: 7/100...  Training Step: 224...  Training loss: 2.3495...  0.4552 sec/batch\n",
      "Epoch: 7/100...  Training Step: 225...  Training loss: 2.3462...  0.4678 sec/batch\n",
      "Epoch: 7/100...  Training Step: 226...  Training loss: 2.3344...  0.4675 sec/batch\n",
      "Epoch: 7/100...  Training Step: 227...  Training loss: 2.3419...  0.4567 sec/batch\n",
      "Epoch: 7/100...  Training Step: 228...  Training loss: 2.3727...  0.4336 sec/batch\n",
      "Epoch: 7/100...  Training Step: 229...  Training loss: 2.3238...  0.4514 sec/batch\n",
      "Epoch: 7/100...  Training Step: 230...  Training loss: 2.3481...  0.4543 sec/batch\n",
      "Epoch: 7/100...  Training Step: 231...  Training loss: 2.3414...  0.4569 sec/batch\n",
      "Epoch: 7/100...  Training Step: 232...  Training loss: 2.3241...  0.4563 sec/batch\n",
      "Epoch: 7/100...  Training Step: 233...  Training loss: 2.3353...  0.4582 sec/batch\n",
      "Epoch: 7/100...  Training Step: 234...  Training loss: 2.3720...  0.4574 sec/batch\n",
      "Epoch: 7/100...  Training Step: 235...  Training loss: 2.3415...  0.4587 sec/batch\n",
      "Epoch: 7/100...  Training Step: 236...  Training loss: 2.3338...  0.4577 sec/batch\n",
      "Epoch: 7/100...  Training Step: 237...  Training loss: 2.3541...  0.4475 sec/batch\n",
      "Epoch: 7/100...  Training Step: 238...  Training loss: 2.3157...  0.4608 sec/batch\n",
      "Epoch: 8/100...  Training Step: 239...  Training loss: 2.4086...  0.4558 sec/batch\n",
      "Epoch: 8/100...  Training Step: 240...  Training loss: 2.3173...  0.4600 sec/batch\n",
      "Epoch: 8/100...  Training Step: 241...  Training loss: 2.3134...  0.4593 sec/batch\n",
      "Epoch: 8/100...  Training Step: 242...  Training loss: 2.3376...  0.4577 sec/batch\n",
      "Epoch: 8/100...  Training Step: 243...  Training loss: 2.3431...  0.4628 sec/batch\n",
      "Epoch: 8/100...  Training Step: 244...  Training loss: 2.2939...  0.4707 sec/batch\n",
      "Epoch: 8/100...  Training Step: 245...  Training loss: 2.3104...  0.4575 sec/batch\n",
      "Epoch: 8/100...  Training Step: 246...  Training loss: 2.2956...  0.4592 sec/batch\n",
      "Epoch: 8/100...  Training Step: 247...  Training loss: 2.2885...  0.4497 sec/batch\n",
      "Epoch: 8/100...  Training Step: 248...  Training loss: 2.3032...  0.4567 sec/batch\n",
      "Epoch: 8/100...  Training Step: 249...  Training loss: 2.3206...  0.4581 sec/batch\n",
      "Epoch: 8/100...  Training Step: 250...  Training loss: 2.3107...  0.4634 sec/batch\n",
      "Epoch: 8/100...  Training Step: 251...  Training loss: 2.3079...  0.4531 sec/batch\n",
      "Epoch: 8/100...  Training Step: 252...  Training loss: 2.2616...  0.4538 sec/batch\n",
      "Epoch: 8/100...  Training Step: 253...  Training loss: 2.2586...  0.4663 sec/batch\n",
      "Epoch: 8/100...  Training Step: 254...  Training loss: 2.3037...  0.4609 sec/batch\n",
      "Epoch: 8/100...  Training Step: 255...  Training loss: 2.2828...  0.4525 sec/batch\n",
      "Epoch: 8/100...  Training Step: 256...  Training loss: 2.2951...  0.4515 sec/batch\n",
      "Epoch: 8/100...  Training Step: 257...  Training loss: 2.2822...  0.4533 sec/batch\n",
      "Epoch: 8/100...  Training Step: 258...  Training loss: 2.2523...  0.4492 sec/batch\n",
      "Epoch: 8/100...  Training Step: 259...  Training loss: 2.2534...  0.4558 sec/batch\n",
      "Epoch: 8/100...  Training Step: 260...  Training loss: 2.2381...  0.4542 sec/batch\n",
      "Epoch: 8/100...  Training Step: 261...  Training loss: 2.2514...  0.4605 sec/batch\n",
      "Epoch: 8/100...  Training Step: 262...  Training loss: 2.2727...  0.4727 sec/batch\n",
      "Epoch: 8/100...  Training Step: 263...  Training loss: 2.2341...  0.4592 sec/batch\n",
      "Epoch: 8/100...  Training Step: 264...  Training loss: 2.2682...  0.4597 sec/batch\n",
      "Epoch: 8/100...  Training Step: 265...  Training loss: 2.2554...  0.4387 sec/batch\n",
      "Epoch: 8/100...  Training Step: 266...  Training loss: 2.2376...  0.4633 sec/batch\n",
      "Epoch: 8/100...  Training Step: 267...  Training loss: 2.2569...  0.4517 sec/batch\n",
      "Epoch: 8/100...  Training Step: 268...  Training loss: 2.2794...  0.4544 sec/batch\n",
      "Epoch: 8/100...  Training Step: 269...  Training loss: 2.2537...  0.4602 sec/batch\n",
      "Epoch: 8/100...  Training Step: 270...  Training loss: 2.2638...  0.4563 sec/batch\n",
      "Epoch: 8/100...  Training Step: 271...  Training loss: 2.2762...  0.4547 sec/batch\n",
      "Epoch: 8/100...  Training Step: 272...  Training loss: 2.2434...  0.4578 sec/batch\n",
      "Epoch: 9/100...  Training Step: 273...  Training loss: 2.3266...  0.4588 sec/batch\n",
      "Epoch: 9/100...  Training Step: 274...  Training loss: 2.2406...  0.4618 sec/batch\n",
      "Epoch: 9/100...  Training Step: 275...  Training loss: 2.2359...  0.4560 sec/batch\n",
      "Epoch: 9/100...  Training Step: 276...  Training loss: 2.2604...  0.4509 sec/batch\n",
      "Epoch: 9/100...  Training Step: 277...  Training loss: 2.2534...  0.4497 sec/batch\n",
      "Epoch: 9/100...  Training Step: 278...  Training loss: 2.2057...  0.4569 sec/batch\n",
      "Epoch: 9/100...  Training Step: 279...  Training loss: 2.2349...  0.4558 sec/batch\n",
      "Epoch: 9/100...  Training Step: 280...  Training loss: 2.2119...  0.4375 sec/batch\n",
      "Epoch: 9/100...  Training Step: 281...  Training loss: 2.1989...  0.4570 sec/batch\n",
      "Epoch: 9/100...  Training Step: 282...  Training loss: 2.2295...  0.4637 sec/batch\n",
      "Epoch: 9/100...  Training Step: 283...  Training loss: 2.2415...  0.4623 sec/batch\n",
      "Epoch: 9/100...  Training Step: 284...  Training loss: 2.2324...  0.4535 sec/batch\n",
      "Epoch: 9/100...  Training Step: 285...  Training loss: 2.2328...  0.4646 sec/batch\n",
      "Epoch: 9/100...  Training Step: 286...  Training loss: 2.1889...  0.4645 sec/batch\n",
      "Epoch: 9/100...  Training Step: 287...  Training loss: 2.1950...  0.4539 sec/batch\n",
      "Epoch: 9/100...  Training Step: 288...  Training loss: 2.2258...  0.4556 sec/batch\n",
      "Epoch: 9/100...  Training Step: 289...  Training loss: 2.2035...  0.4577 sec/batch\n",
      "Epoch: 9/100...  Training Step: 290...  Training loss: 2.2165...  0.4666 sec/batch\n",
      "Epoch: 9/100...  Training Step: 291...  Training loss: 2.1999...  0.4552 sec/batch\n",
      "Epoch: 9/100...  Training Step: 292...  Training loss: 2.1867...  0.4466 sec/batch\n",
      "Epoch: 9/100...  Training Step: 293...  Training loss: 2.1859...  0.4479 sec/batch\n",
      "Epoch: 9/100...  Training Step: 294...  Training loss: 2.1583...  0.4509 sec/batch\n",
      "Epoch: 9/100...  Training Step: 295...  Training loss: 2.1819...  0.4536 sec/batch\n",
      "Epoch: 9/100...  Training Step: 296...  Training loss: 2.1987...  0.4645 sec/batch\n",
      "Epoch: 9/100...  Training Step: 297...  Training loss: 2.1520...  0.4572 sec/batch\n",
      "Epoch: 9/100...  Training Step: 298...  Training loss: 2.1761...  0.4545 sec/batch\n",
      "Epoch: 9/100...  Training Step: 299...  Training loss: 2.1828...  0.4640 sec/batch\n",
      "Epoch: 9/100...  Training Step: 300...  Training loss: 2.1635...  0.4556 sec/batch\n",
      "Epoch: 9/100...  Training Step: 301...  Training loss: 2.1899...  0.4649 sec/batch\n",
      "Epoch: 9/100...  Training Step: 302...  Training loss: 2.2091...  0.4599 sec/batch\n",
      "Epoch: 9/100...  Training Step: 303...  Training loss: 2.1896...  0.4589 sec/batch\n",
      "Epoch: 9/100...  Training Step: 304...  Training loss: 2.1880...  0.4499 sec/batch\n",
      "Epoch: 9/100...  Training Step: 305...  Training loss: 2.1947...  0.4576 sec/batch\n",
      "Epoch: 9/100...  Training Step: 306...  Training loss: 2.1614...  0.4531 sec/batch\n",
      "Epoch: 10/100...  Training Step: 307...  Training loss: 2.2543...  0.4671 sec/batch\n",
      "Epoch: 10/100...  Training Step: 308...  Training loss: 2.1642...  0.4580 sec/batch\n",
      "Epoch: 10/100...  Training Step: 309...  Training loss: 2.1581...  0.4547 sec/batch\n",
      "Epoch: 10/100...  Training Step: 310...  Training loss: 2.1827...  0.4591 sec/batch\n",
      "Epoch: 10/100...  Training Step: 311...  Training loss: 2.1889...  0.4613 sec/batch\n",
      "Epoch: 10/100...  Training Step: 312...  Training loss: 2.1398...  0.4534 sec/batch\n",
      "Epoch: 10/100...  Training Step: 313...  Training loss: 2.1593...  0.4544 sec/batch\n",
      "Epoch: 10/100...  Training Step: 314...  Training loss: 2.1434...  0.4643 sec/batch\n",
      "Epoch: 10/100...  Training Step: 315...  Training loss: 2.1353...  0.4607 sec/batch\n",
      "Epoch: 10/100...  Training Step: 316...  Training loss: 2.1595...  0.4573 sec/batch\n",
      "Epoch: 10/100...  Training Step: 317...  Training loss: 2.1758...  0.4570 sec/batch\n",
      "Epoch: 10/100...  Training Step: 318...  Training loss: 2.1725...  0.4557 sec/batch\n",
      "Epoch: 10/100...  Training Step: 319...  Training loss: 2.1682...  0.4532 sec/batch\n",
      "Epoch: 10/100...  Training Step: 320...  Training loss: 2.1308...  0.4545 sec/batch\n",
      "Epoch: 10/100...  Training Step: 321...  Training loss: 2.1181...  0.4547 sec/batch\n",
      "Epoch: 10/100...  Training Step: 322...  Training loss: 2.1575...  0.4560 sec/batch\n",
      "Epoch: 10/100...  Training Step: 323...  Training loss: 2.1421...  0.4552 sec/batch\n",
      "Epoch: 10/100...  Training Step: 324...  Training loss: 2.1416...  0.4624 sec/batch\n",
      "Epoch: 10/100...  Training Step: 325...  Training loss: 2.1373...  0.4464 sec/batch\n",
      "Epoch: 10/100...  Training Step: 326...  Training loss: 2.1179...  0.4797 sec/batch\n",
      "Epoch: 10/100...  Training Step: 327...  Training loss: 2.1259...  0.4543 sec/batch\n",
      "Epoch: 10/100...  Training Step: 328...  Training loss: 2.0977...  0.4502 sec/batch\n",
      "Epoch: 10/100...  Training Step: 329...  Training loss: 2.1138...  0.4638 sec/batch\n",
      "Epoch: 10/100...  Training Step: 330...  Training loss: 2.1397...  0.4551 sec/batch\n",
      "Epoch: 10/100...  Training Step: 331...  Training loss: 2.0905...  0.4541 sec/batch\n",
      "Epoch: 10/100...  Training Step: 332...  Training loss: 2.1182...  0.4556 sec/batch\n",
      "Epoch: 10/100...  Training Step: 333...  Training loss: 2.1157...  0.4532 sec/batch\n",
      "Epoch: 10/100...  Training Step: 334...  Training loss: 2.0974...  0.4595 sec/batch\n",
      "Epoch: 10/100...  Training Step: 335...  Training loss: 2.1198...  0.4423 sec/batch\n",
      "Epoch: 10/100...  Training Step: 336...  Training loss: 2.1480...  0.4619 sec/batch\n",
      "Epoch: 10/100...  Training Step: 337...  Training loss: 2.1206...  0.4650 sec/batch\n",
      "Epoch: 10/100...  Training Step: 338...  Training loss: 2.1236...  0.4553 sec/batch\n",
      "Epoch: 10/100...  Training Step: 339...  Training loss: 2.1412...  0.4590 sec/batch\n",
      "Epoch: 10/100...  Training Step: 340...  Training loss: 2.1008...  0.4578 sec/batch\n",
      "Epoch: 11/100...  Training Step: 341...  Training loss: 2.2016...  0.4490 sec/batch\n",
      "Epoch: 11/100...  Training Step: 342...  Training loss: 2.1101...  0.4522 sec/batch\n",
      "Epoch: 11/100...  Training Step: 343...  Training loss: 2.1013...  0.4549 sec/batch\n",
      "Epoch: 11/100...  Training Step: 344...  Training loss: 2.1280...  0.4610 sec/batch\n",
      "Epoch: 11/100...  Training Step: 345...  Training loss: 2.1271...  0.4556 sec/batch\n",
      "Epoch: 11/100...  Training Step: 346...  Training loss: 2.0832...  0.4588 sec/batch\n",
      "Epoch: 11/100...  Training Step: 347...  Training loss: 2.1111...  0.4499 sec/batch\n",
      "Epoch: 11/100...  Training Step: 348...  Training loss: 2.0750...  0.4599 sec/batch\n",
      "Epoch: 11/100...  Training Step: 349...  Training loss: 2.0791...  0.4592 sec/batch\n",
      "Epoch: 11/100...  Training Step: 350...  Training loss: 2.0967...  0.4340 sec/batch\n",
      "Epoch: 11/100...  Training Step: 351...  Training loss: 2.1113...  0.4576 sec/batch\n",
      "Epoch: 11/100...  Training Step: 352...  Training loss: 2.1034...  0.4630 sec/batch\n",
      "Epoch: 11/100...  Training Step: 353...  Training loss: 2.1056...  0.4530 sec/batch\n",
      "Epoch: 11/100...  Training Step: 354...  Training loss: 2.0798...  0.4560 sec/batch\n",
      "Epoch: 11/100...  Training Step: 355...  Training loss: 2.0646...  0.4535 sec/batch\n",
      "Epoch: 11/100...  Training Step: 356...  Training loss: 2.0994...  0.4580 sec/batch\n",
      "Epoch: 11/100...  Training Step: 357...  Training loss: 2.0841...  0.4583 sec/batch\n",
      "Epoch: 11/100...  Training Step: 358...  Training loss: 2.0928...  0.4528 sec/batch\n",
      "Epoch: 11/100...  Training Step: 359...  Training loss: 2.0754...  0.4421 sec/batch\n",
      "Epoch: 11/100...  Training Step: 360...  Training loss: 2.0694...  0.4563 sec/batch\n",
      "Epoch: 11/100...  Training Step: 361...  Training loss: 2.0719...  0.4543 sec/batch\n",
      "Epoch: 11/100...  Training Step: 362...  Training loss: 2.0462...  0.4465 sec/batch\n",
      "Epoch: 11/100...  Training Step: 363...  Training loss: 2.0636...  0.4502 sec/batch\n",
      "Epoch: 11/100...  Training Step: 364...  Training loss: 2.0712...  0.4551 sec/batch\n",
      "Epoch: 11/100...  Training Step: 365...  Training loss: 2.0294...  0.4582 sec/batch\n",
      "Epoch: 11/100...  Training Step: 366...  Training loss: 2.0550...  0.4516 sec/batch\n",
      "Epoch: 11/100...  Training Step: 367...  Training loss: 2.0567...  0.4458 sec/batch\n",
      "Epoch: 11/100...  Training Step: 368...  Training loss: 2.0444...  0.4498 sec/batch\n",
      "Epoch: 11/100...  Training Step: 369...  Training loss: 2.0700...  0.4546 sec/batch\n",
      "Epoch: 11/100...  Training Step: 370...  Training loss: 2.0889...  0.4549 sec/batch\n",
      "Epoch: 11/100...  Training Step: 371...  Training loss: 2.0706...  0.4579 sec/batch\n",
      "Epoch: 11/100...  Training Step: 372...  Training loss: 2.0705...  0.4584 sec/batch\n",
      "Epoch: 11/100...  Training Step: 373...  Training loss: 2.0923...  0.4482 sec/batch\n",
      "Epoch: 11/100...  Training Step: 374...  Training loss: 2.0340...  0.4515 sec/batch\n",
      "Epoch: 12/100...  Training Step: 375...  Training loss: 2.1484...  0.4570 sec/batch\n",
      "Epoch: 12/100...  Training Step: 376...  Training loss: 2.0483...  0.4649 sec/batch\n",
      "Epoch: 12/100...  Training Step: 377...  Training loss: 2.0437...  0.4652 sec/batch\n",
      "Epoch: 12/100...  Training Step: 378...  Training loss: 2.0730...  0.4558 sec/batch\n",
      "Epoch: 12/100...  Training Step: 379...  Training loss: 2.0694...  0.4538 sec/batch\n",
      "Epoch: 12/100...  Training Step: 380...  Training loss: 2.0196...  0.4600 sec/batch\n",
      "Epoch: 12/100...  Training Step: 381...  Training loss: 2.0420...  0.4530 sec/batch\n",
      "Epoch: 12/100...  Training Step: 382...  Training loss: 2.0244...  0.4578 sec/batch\n",
      "Epoch: 12/100...  Training Step: 383...  Training loss: 2.0201...  0.4588 sec/batch\n",
      "Epoch: 12/100...  Training Step: 384...  Training loss: 2.0455...  0.4556 sec/batch\n",
      "Epoch: 12/100...  Training Step: 385...  Training loss: 2.0554...  0.4565 sec/batch\n",
      "Epoch: 12/100...  Training Step: 386...  Training loss: 2.0542...  0.4535 sec/batch\n",
      "Epoch: 12/100...  Training Step: 387...  Training loss: 2.0506...  0.4683 sec/batch\n",
      "Epoch: 12/100...  Training Step: 388...  Training loss: 2.0174...  0.4617 sec/batch\n",
      "Epoch: 12/100...  Training Step: 389...  Training loss: 2.0033...  0.4549 sec/batch\n",
      "Epoch: 12/100...  Training Step: 390...  Training loss: 2.0439...  0.4539 sec/batch\n",
      "Epoch: 12/100...  Training Step: 391...  Training loss: 2.0327...  0.4572 sec/batch\n",
      "Epoch: 12/100...  Training Step: 392...  Training loss: 2.0229...  0.4666 sec/batch\n",
      "Epoch: 12/100...  Training Step: 393...  Training loss: 2.0286...  0.4557 sec/batch\n",
      "Epoch: 12/100...  Training Step: 394...  Training loss: 2.0076...  0.4552 sec/batch\n",
      "Epoch: 12/100...  Training Step: 395...  Training loss: 2.0083...  0.4596 sec/batch\n",
      "Epoch: 12/100...  Training Step: 396...  Training loss: 1.9824...  0.4672 sec/batch\n",
      "Epoch: 12/100...  Training Step: 397...  Training loss: 2.0098...  0.4599 sec/batch\n",
      "Epoch: 12/100...  Training Step: 398...  Training loss: 2.0237...  0.4581 sec/batch\n",
      "Epoch: 12/100...  Training Step: 399...  Training loss: 1.9836...  0.4468 sec/batch\n",
      "Epoch: 12/100...  Training Step: 400...  Training loss: 2.0014...  0.4622 sec/batch\n",
      "Epoch: 12/100...  Training Step: 401...  Training loss: 2.0019...  0.4563 sec/batch\n",
      "Epoch: 12/100...  Training Step: 402...  Training loss: 1.9924...  0.4514 sec/batch\n",
      "Epoch: 12/100...  Training Step: 403...  Training loss: 2.0217...  0.4507 sec/batch\n",
      "Epoch: 12/100...  Training Step: 404...  Training loss: 2.0346...  0.4518 sec/batch\n",
      "Epoch: 12/100...  Training Step: 405...  Training loss: 2.0308...  0.4573 sec/batch\n",
      "Epoch: 12/100...  Training Step: 406...  Training loss: 2.0150...  0.4451 sec/batch\n",
      "Epoch: 12/100...  Training Step: 407...  Training loss: 2.0399...  0.4607 sec/batch\n",
      "Epoch: 12/100...  Training Step: 408...  Training loss: 1.9870...  0.4583 sec/batch\n",
      "Epoch: 13/100...  Training Step: 409...  Training loss: 2.0885...  0.4505 sec/batch\n",
      "Epoch: 13/100...  Training Step: 410...  Training loss: 2.0003...  0.4355 sec/batch\n",
      "Epoch: 13/100...  Training Step: 411...  Training loss: 1.9955...  0.4568 sec/batch\n",
      "Epoch: 13/100...  Training Step: 412...  Training loss: 2.0171...  0.4518 sec/batch\n",
      "Epoch: 13/100...  Training Step: 413...  Training loss: 2.0337...  0.4528 sec/batch\n",
      "Epoch: 13/100...  Training Step: 414...  Training loss: 1.9738...  0.4561 sec/batch\n",
      "Epoch: 13/100...  Training Step: 415...  Training loss: 1.9967...  0.4516 sec/batch\n",
      "Epoch: 13/100...  Training Step: 416...  Training loss: 1.9741...  0.4573 sec/batch\n",
      "Epoch: 13/100...  Training Step: 417...  Training loss: 1.9590...  0.4488 sec/batch\n",
      "Epoch: 13/100...  Training Step: 418...  Training loss: 2.0000...  0.4524 sec/batch\n",
      "Epoch: 13/100...  Training Step: 419...  Training loss: 2.0082...  0.4523 sec/batch\n",
      "Epoch: 13/100...  Training Step: 420...  Training loss: 2.0085...  0.4575 sec/batch\n",
      "Epoch: 13/100...  Training Step: 421...  Training loss: 1.9951...  0.4399 sec/batch\n",
      "Epoch: 13/100...  Training Step: 422...  Training loss: 1.9742...  0.4594 sec/batch\n",
      "Epoch: 13/100...  Training Step: 423...  Training loss: 1.9553...  0.4442 sec/batch\n",
      "Epoch: 13/100...  Training Step: 424...  Training loss: 1.9907...  0.4498 sec/batch\n",
      "Epoch: 13/100...  Training Step: 425...  Training loss: 1.9832...  0.4624 sec/batch\n",
      "Epoch: 13/100...  Training Step: 426...  Training loss: 1.9737...  0.4603 sec/batch\n",
      "Epoch: 13/100...  Training Step: 427...  Training loss: 1.9849...  0.4577 sec/batch\n",
      "Epoch: 13/100...  Training Step: 428...  Training loss: 1.9581...  0.4524 sec/batch\n",
      "Epoch: 13/100...  Training Step: 429...  Training loss: 1.9723...  0.4494 sec/batch\n",
      "Epoch: 13/100...  Training Step: 430...  Training loss: 1.9435...  0.4528 sec/batch\n",
      "Epoch: 13/100...  Training Step: 431...  Training loss: 1.9557...  0.4387 sec/batch\n",
      "Epoch: 13/100...  Training Step: 432...  Training loss: 1.9759...  0.4413 sec/batch\n",
      "Epoch: 13/100...  Training Step: 433...  Training loss: 1.9357...  0.4549 sec/batch\n",
      "Epoch: 13/100...  Training Step: 434...  Training loss: 1.9548...  0.4519 sec/batch\n",
      "Epoch: 13/100...  Training Step: 435...  Training loss: 1.9495...  0.4570 sec/batch\n",
      "Epoch: 13/100...  Training Step: 436...  Training loss: 1.9457...  0.4495 sec/batch\n",
      "Epoch: 13/100...  Training Step: 437...  Training loss: 1.9809...  0.4490 sec/batch\n",
      "Epoch: 13/100...  Training Step: 438...  Training loss: 1.9900...  0.4613 sec/batch\n",
      "Epoch: 13/100...  Training Step: 439...  Training loss: 1.9853...  0.4523 sec/batch\n",
      "Epoch: 13/100...  Training Step: 440...  Training loss: 1.9755...  0.4527 sec/batch\n",
      "Epoch: 13/100...  Training Step: 441...  Training loss: 1.9911...  0.4564 sec/batch\n",
      "Epoch: 13/100...  Training Step: 442...  Training loss: 1.9344...  0.4482 sec/batch\n",
      "Epoch: 14/100...  Training Step: 443...  Training loss: 2.0504...  0.4524 sec/batch\n",
      "Epoch: 14/100...  Training Step: 444...  Training loss: 1.9466...  0.4610 sec/batch\n",
      "Epoch: 14/100...  Training Step: 445...  Training loss: 1.9383...  0.4611 sec/batch\n",
      "Epoch: 14/100...  Training Step: 446...  Training loss: 1.9728...  0.4522 sec/batch\n",
      "Epoch: 14/100...  Training Step: 447...  Training loss: 1.9803...  0.4598 sec/batch\n",
      "Epoch: 14/100...  Training Step: 448...  Training loss: 1.9271...  0.4512 sec/batch\n",
      "Epoch: 14/100...  Training Step: 449...  Training loss: 1.9528...  0.4523 sec/batch\n",
      "Epoch: 14/100...  Training Step: 450...  Training loss: 1.9242...  0.4494 sec/batch\n",
      "Epoch: 14/100...  Training Step: 451...  Training loss: 1.9227...  0.4551 sec/batch\n",
      "Epoch: 14/100...  Training Step: 452...  Training loss: 1.9500...  0.4586 sec/batch\n",
      "Epoch: 14/100...  Training Step: 453...  Training loss: 1.9595...  0.4496 sec/batch\n",
      "Epoch: 14/100...  Training Step: 454...  Training loss: 1.9743...  0.4562 sec/batch\n",
      "Epoch: 14/100...  Training Step: 455...  Training loss: 1.9572...  0.4593 sec/batch\n",
      "Epoch: 14/100...  Training Step: 456...  Training loss: 1.9373...  0.4479 sec/batch\n",
      "Epoch: 14/100...  Training Step: 457...  Training loss: 1.9054...  0.4641 sec/batch\n",
      "Epoch: 14/100...  Training Step: 458...  Training loss: 1.9474...  0.4618 sec/batch\n",
      "Epoch: 14/100...  Training Step: 459...  Training loss: 1.9373...  0.4394 sec/batch\n",
      "Epoch: 14/100...  Training Step: 460...  Training loss: 1.9364...  0.4398 sec/batch\n",
      "Epoch: 14/100...  Training Step: 461...  Training loss: 1.9347...  0.4583 sec/batch\n",
      "Epoch: 14/100...  Training Step: 462...  Training loss: 1.9108...  0.4515 sec/batch\n",
      "Epoch: 14/100...  Training Step: 463...  Training loss: 1.9229...  0.4516 sec/batch\n",
      "Epoch: 14/100...  Training Step: 464...  Training loss: 1.8825...  0.4587 sec/batch\n",
      "Epoch: 14/100...  Training Step: 465...  Training loss: 1.9124...  0.4637 sec/batch\n",
      "Epoch: 14/100...  Training Step: 466...  Training loss: 1.9277...  0.4701 sec/batch\n",
      "Epoch: 14/100...  Training Step: 467...  Training loss: 1.8814...  0.4588 sec/batch\n",
      "Epoch: 14/100...  Training Step: 468...  Training loss: 1.8996...  0.4543 sec/batch\n",
      "Epoch: 14/100...  Training Step: 469...  Training loss: 1.9040...  0.4572 sec/batch\n",
      "Epoch: 14/100...  Training Step: 470...  Training loss: 1.9006...  0.4539 sec/batch\n",
      "Epoch: 14/100...  Training Step: 471...  Training loss: 1.9403...  0.4539 sec/batch\n",
      "Epoch: 14/100...  Training Step: 472...  Training loss: 1.9436...  0.4514 sec/batch\n",
      "Epoch: 14/100...  Training Step: 473...  Training loss: 1.9294...  0.4511 sec/batch\n",
      "Epoch: 14/100...  Training Step: 474...  Training loss: 1.9369...  0.4529 sec/batch\n",
      "Epoch: 14/100...  Training Step: 475...  Training loss: 1.9626...  0.4557 sec/batch\n",
      "Epoch: 14/100...  Training Step: 476...  Training loss: 1.9019...  0.4577 sec/batch\n",
      "Epoch: 15/100...  Training Step: 477...  Training loss: 2.0070...  0.4744 sec/batch\n",
      "Epoch: 15/100...  Training Step: 478...  Training loss: 1.9055...  0.4499 sec/batch\n",
      "Epoch: 15/100...  Training Step: 479...  Training loss: 1.9053...  0.4659 sec/batch\n",
      "Epoch: 15/100...  Training Step: 480...  Training loss: 1.9276...  0.4614 sec/batch\n",
      "Epoch: 15/100...  Training Step: 481...  Training loss: 1.9390...  0.4536 sec/batch\n",
      "Epoch: 15/100...  Training Step: 482...  Training loss: 1.8952...  0.4517 sec/batch\n",
      "Epoch: 15/100...  Training Step: 483...  Training loss: 1.9108...  0.4542 sec/batch\n",
      "Epoch: 15/100...  Training Step: 484...  Training loss: 1.8873...  0.4587 sec/batch\n",
      "Epoch: 15/100...  Training Step: 485...  Training loss: 1.8842...  0.4571 sec/batch\n",
      "Epoch: 15/100...  Training Step: 486...  Training loss: 1.9105...  0.4613 sec/batch\n",
      "Epoch: 15/100...  Training Step: 487...  Training loss: 1.9273...  0.4567 sec/batch\n",
      "Epoch: 15/100...  Training Step: 488...  Training loss: 1.9273...  0.4573 sec/batch\n",
      "Epoch: 15/100...  Training Step: 489...  Training loss: 1.9176...  0.4486 sec/batch\n",
      "Epoch: 15/100...  Training Step: 490...  Training loss: 1.8937...  0.4622 sec/batch\n",
      "Epoch: 15/100...  Training Step: 491...  Training loss: 1.8755...  0.4523 sec/batch\n",
      "Epoch: 15/100...  Training Step: 492...  Training loss: 1.9102...  0.4433 sec/batch\n",
      "Epoch: 15/100...  Training Step: 493...  Training loss: 1.9045...  0.4337 sec/batch\n",
      "Epoch: 15/100...  Training Step: 494...  Training loss: 1.8922...  0.4556 sec/batch\n",
      "Epoch: 15/100...  Training Step: 495...  Training loss: 1.9021...  0.4562 sec/batch\n",
      "Epoch: 15/100...  Training Step: 496...  Training loss: 1.8751...  0.4617 sec/batch\n",
      "Epoch: 15/100...  Training Step: 497...  Training loss: 1.8947...  0.4513 sec/batch\n",
      "Epoch: 15/100...  Training Step: 498...  Training loss: 1.8455...  0.4558 sec/batch\n",
      "Epoch: 15/100...  Training Step: 499...  Training loss: 1.8794...  0.4617 sec/batch\n",
      "Epoch: 15/100...  Training Step: 500...  Training loss: 1.8944...  0.4641 sec/batch\n",
      "Epoch: 15/100...  Training Step: 501...  Training loss: 1.8532...  0.4617 sec/batch\n",
      "Epoch: 15/100...  Training Step: 502...  Training loss: 1.8676...  0.4677 sec/batch\n",
      "Epoch: 15/100...  Training Step: 503...  Training loss: 1.8682...  0.4524 sec/batch\n",
      "Epoch: 15/100...  Training Step: 504...  Training loss: 1.8606...  0.4618 sec/batch\n",
      "Epoch: 15/100...  Training Step: 505...  Training loss: 1.8973...  0.4563 sec/batch\n",
      "Epoch: 15/100...  Training Step: 506...  Training loss: 1.9160...  0.4538 sec/batch\n",
      "Epoch: 15/100...  Training Step: 507...  Training loss: 1.8931...  0.4523 sec/batch\n",
      "Epoch: 15/100...  Training Step: 508...  Training loss: 1.9006...  0.4567 sec/batch\n",
      "Epoch: 15/100...  Training Step: 509...  Training loss: 1.9137...  0.4527 sec/batch\n",
      "Epoch: 15/100...  Training Step: 510...  Training loss: 1.8564...  0.4573 sec/batch\n",
      "Epoch: 16/100...  Training Step: 511...  Training loss: 1.9602...  0.4637 sec/batch\n",
      "Epoch: 16/100...  Training Step: 512...  Training loss: 1.8779...  0.4394 sec/batch\n",
      "Epoch: 16/100...  Training Step: 513...  Training loss: 1.8671...  0.4537 sec/batch\n",
      "Epoch: 16/100...  Training Step: 514...  Training loss: 1.9040...  0.4551 sec/batch\n",
      "Epoch: 16/100...  Training Step: 515...  Training loss: 1.9010...  0.4537 sec/batch\n",
      "Epoch: 16/100...  Training Step: 516...  Training loss: 1.8551...  0.4481 sec/batch\n",
      "Epoch: 16/100...  Training Step: 517...  Training loss: 1.8721...  0.4633 sec/batch\n",
      "Epoch: 16/100...  Training Step: 518...  Training loss: 1.8543...  0.4580 sec/batch\n",
      "Epoch: 16/100...  Training Step: 519...  Training loss: 1.8443...  0.4597 sec/batch\n",
      "Epoch: 16/100...  Training Step: 520...  Training loss: 1.8751...  0.4424 sec/batch\n",
      "Epoch: 16/100...  Training Step: 521...  Training loss: 1.8851...  0.4611 sec/batch\n",
      "Epoch: 16/100...  Training Step: 522...  Training loss: 1.8835...  0.4573 sec/batch\n",
      "Epoch: 16/100...  Training Step: 523...  Training loss: 1.8802...  0.4556 sec/batch\n",
      "Epoch: 16/100...  Training Step: 524...  Training loss: 1.8589...  0.4601 sec/batch\n",
      "Epoch: 16/100...  Training Step: 525...  Training loss: 1.8287...  0.4468 sec/batch\n",
      "Epoch: 16/100...  Training Step: 526...  Training loss: 1.8675...  0.4462 sec/batch\n",
      "Epoch: 16/100...  Training Step: 527...  Training loss: 1.8650...  0.4484 sec/batch\n",
      "Epoch: 16/100...  Training Step: 528...  Training loss: 1.8457...  0.4567 sec/batch\n",
      "Epoch: 16/100...  Training Step: 529...  Training loss: 1.8630...  0.4672 sec/batch\n",
      "Epoch: 16/100...  Training Step: 530...  Training loss: 1.8306...  0.4553 sec/batch\n",
      "Epoch: 16/100...  Training Step: 531...  Training loss: 1.8601...  0.4450 sec/batch\n",
      "Epoch: 16/100...  Training Step: 532...  Training loss: 1.8220...  0.4545 sec/batch\n",
      "Epoch: 16/100...  Training Step: 533...  Training loss: 1.8425...  0.4522 sec/batch\n",
      "Epoch: 16/100...  Training Step: 534...  Training loss: 1.8493...  0.4590 sec/batch\n",
      "Epoch: 16/100...  Training Step: 535...  Training loss: 1.8112...  0.4393 sec/batch\n",
      "Epoch: 16/100...  Training Step: 536...  Training loss: 1.8306...  0.4487 sec/batch\n",
      "Epoch: 16/100...  Training Step: 537...  Training loss: 1.8219...  0.4340 sec/batch\n",
      "Epoch: 16/100...  Training Step: 538...  Training loss: 1.8262...  0.4627 sec/batch\n",
      "Epoch: 16/100...  Training Step: 539...  Training loss: 1.8656...  0.4574 sec/batch\n",
      "Epoch: 16/100...  Training Step: 540...  Training loss: 1.8818...  0.4507 sec/batch\n",
      "Epoch: 16/100...  Training Step: 541...  Training loss: 1.8631...  0.4587 sec/batch\n",
      "Epoch: 16/100...  Training Step: 542...  Training loss: 1.8656...  0.4493 sec/batch\n",
      "Epoch: 16/100...  Training Step: 543...  Training loss: 1.8762...  0.4561 sec/batch\n",
      "Epoch: 16/100...  Training Step: 544...  Training loss: 1.8198...  0.4522 sec/batch\n",
      "Epoch: 17/100...  Training Step: 545...  Training loss: 1.9262...  0.4375 sec/batch\n",
      "Epoch: 17/100...  Training Step: 546...  Training loss: 1.8341...  0.4551 sec/batch\n",
      "Epoch: 17/100...  Training Step: 547...  Training loss: 1.8297...  0.4536 sec/batch\n",
      "Epoch: 17/100...  Training Step: 548...  Training loss: 1.8539...  0.4587 sec/batch\n",
      "Epoch: 17/100...  Training Step: 549...  Training loss: 1.8556...  0.4504 sec/batch\n",
      "Epoch: 17/100...  Training Step: 550...  Training loss: 1.8165...  0.4656 sec/batch\n",
      "Epoch: 17/100...  Training Step: 551...  Training loss: 1.8371...  0.4592 sec/batch\n",
      "Epoch: 17/100...  Training Step: 552...  Training loss: 1.8104...  0.4525 sec/batch\n",
      "Epoch: 17/100...  Training Step: 553...  Training loss: 1.8153...  0.4352 sec/batch\n",
      "Epoch: 17/100...  Training Step: 554...  Training loss: 1.8386...  0.4545 sec/batch\n",
      "Epoch: 17/100...  Training Step: 555...  Training loss: 1.8455...  0.4577 sec/batch\n",
      "Epoch: 17/100...  Training Step: 556...  Training loss: 1.8489...  0.4516 sec/batch\n",
      "Epoch: 17/100...  Training Step: 557...  Training loss: 1.8435...  0.4460 sec/batch\n",
      "Epoch: 17/100...  Training Step: 558...  Training loss: 1.8232...  0.4582 sec/batch\n",
      "Epoch: 17/100...  Training Step: 559...  Training loss: 1.8012...  0.4558 sec/batch\n",
      "Epoch: 17/100...  Training Step: 560...  Training loss: 1.8396...  0.4590 sec/batch\n",
      "Epoch: 17/100...  Training Step: 561...  Training loss: 1.8333...  0.4536 sec/batch\n",
      "Epoch: 17/100...  Training Step: 562...  Training loss: 1.8191...  0.4494 sec/batch\n",
      "Epoch: 17/100...  Training Step: 563...  Training loss: 1.8246...  0.4426 sec/batch\n",
      "Epoch: 17/100...  Training Step: 564...  Training loss: 1.8051...  0.4538 sec/batch\n",
      "Epoch: 17/100...  Training Step: 565...  Training loss: 1.8071...  0.4602 sec/batch\n",
      "Epoch: 17/100...  Training Step: 566...  Training loss: 1.7797...  0.4523 sec/batch\n",
      "Epoch: 17/100...  Training Step: 567...  Training loss: 1.8066...  0.4555 sec/batch\n",
      "Epoch: 17/100...  Training Step: 568...  Training loss: 1.8169...  0.4566 sec/batch\n",
      "Epoch: 17/100...  Training Step: 569...  Training loss: 1.7783...  0.4611 sec/batch\n",
      "Epoch: 17/100...  Training Step: 570...  Training loss: 1.7933...  0.4728 sec/batch\n",
      "Epoch: 17/100...  Training Step: 571...  Training loss: 1.7959...  0.4552 sec/batch\n",
      "Epoch: 17/100...  Training Step: 572...  Training loss: 1.7922...  0.4521 sec/batch\n",
      "Epoch: 17/100...  Training Step: 573...  Training loss: 1.8264...  0.4515 sec/batch\n",
      "Epoch: 17/100...  Training Step: 574...  Training loss: 1.8385...  0.4582 sec/batch\n",
      "Epoch: 17/100...  Training Step: 575...  Training loss: 1.8252...  0.4555 sec/batch\n",
      "Epoch: 17/100...  Training Step: 576...  Training loss: 1.8252...  0.4560 sec/batch\n",
      "Epoch: 17/100...  Training Step: 577...  Training loss: 1.8513...  0.4562 sec/batch\n",
      "Epoch: 17/100...  Training Step: 578...  Training loss: 1.7976...  0.4563 sec/batch\n",
      "Epoch: 18/100...  Training Step: 579...  Training loss: 1.8829...  0.4565 sec/batch\n",
      "Epoch: 18/100...  Training Step: 580...  Training loss: 1.8068...  0.4566 sec/batch\n",
      "Epoch: 18/100...  Training Step: 581...  Training loss: 1.8047...  0.4598 sec/batch\n",
      "Epoch: 18/100...  Training Step: 582...  Training loss: 1.8213...  0.4512 sec/batch\n",
      "Epoch: 18/100...  Training Step: 583...  Training loss: 1.8238...  0.4430 sec/batch\n",
      "Epoch: 18/100...  Training Step: 584...  Training loss: 1.7825...  0.4569 sec/batch\n",
      "Epoch: 18/100...  Training Step: 585...  Training loss: 1.8018...  0.4563 sec/batch\n",
      "Epoch: 18/100...  Training Step: 586...  Training loss: 1.7800...  0.4574 sec/batch\n",
      "Epoch: 18/100...  Training Step: 587...  Training loss: 1.7784...  0.4534 sec/batch\n",
      "Epoch: 18/100...  Training Step: 588...  Training loss: 1.8121...  0.4580 sec/batch\n",
      "Epoch: 18/100...  Training Step: 589...  Training loss: 1.8129...  0.4639 sec/batch\n",
      "Epoch: 18/100...  Training Step: 590...  Training loss: 1.8208...  0.4482 sec/batch\n",
      "Epoch: 18/100...  Training Step: 591...  Training loss: 1.8118...  0.4503 sec/batch\n",
      "Epoch: 18/100...  Training Step: 592...  Training loss: 1.7943...  0.4624 sec/batch\n",
      "Epoch: 18/100...  Training Step: 593...  Training loss: 1.7706...  0.4641 sec/batch\n",
      "Epoch: 18/100...  Training Step: 594...  Training loss: 1.8198...  0.4533 sec/batch\n",
      "Epoch: 18/100...  Training Step: 595...  Training loss: 1.8198...  0.4519 sec/batch\n",
      "Epoch: 18/100...  Training Step: 596...  Training loss: 1.7895...  0.4392 sec/batch\n",
      "Epoch: 18/100...  Training Step: 597...  Training loss: 1.8006...  0.4525 sec/batch\n",
      "Epoch: 18/100...  Training Step: 598...  Training loss: 1.7737...  0.4541 sec/batch\n",
      "Epoch: 18/100...  Training Step: 599...  Training loss: 1.7914...  0.4600 sec/batch\n",
      "Epoch: 18/100...  Training Step: 600...  Training loss: 1.7549...  0.4493 sec/batch\n",
      "Epoch: 18/100...  Training Step: 601...  Training loss: 1.7778...  0.4619 sec/batch\n",
      "Epoch: 18/100...  Training Step: 602...  Training loss: 1.7830...  0.4590 sec/batch\n",
      "Epoch: 18/100...  Training Step: 603...  Training loss: 1.7465...  0.4612 sec/batch\n",
      "Epoch: 18/100...  Training Step: 604...  Training loss: 1.7662...  0.4527 sec/batch\n",
      "Epoch: 18/100...  Training Step: 605...  Training loss: 1.7643...  0.4578 sec/batch\n",
      "Epoch: 18/100...  Training Step: 606...  Training loss: 1.7592...  0.4513 sec/batch\n",
      "Epoch: 18/100...  Training Step: 607...  Training loss: 1.8025...  0.4490 sec/batch\n",
      "Epoch: 18/100...  Training Step: 608...  Training loss: 1.8158...  0.4563 sec/batch\n",
      "Epoch: 18/100...  Training Step: 609...  Training loss: 1.8008...  0.4509 sec/batch\n",
      "Epoch: 18/100...  Training Step: 610...  Training loss: 1.7970...  0.4482 sec/batch\n",
      "Epoch: 18/100...  Training Step: 611...  Training loss: 1.8243...  0.4515 sec/batch\n",
      "Epoch: 18/100...  Training Step: 612...  Training loss: 1.7697...  0.4585 sec/batch\n",
      "Epoch: 19/100...  Training Step: 613...  Training loss: 1.8582...  0.4536 sec/batch\n",
      "Epoch: 19/100...  Training Step: 614...  Training loss: 1.7746...  0.4499 sec/batch\n",
      "Epoch: 19/100...  Training Step: 615...  Training loss: 1.7636...  0.4590 sec/batch\n",
      "Epoch: 19/100...  Training Step: 616...  Training loss: 1.8015...  0.4439 sec/batch\n",
      "Epoch: 19/100...  Training Step: 617...  Training loss: 1.7901...  0.4576 sec/batch\n",
      "Epoch: 19/100...  Training Step: 618...  Training loss: 1.7559...  0.4458 sec/batch\n",
      "Epoch: 19/100...  Training Step: 619...  Training loss: 1.7849...  0.4592 sec/batch\n",
      "Epoch: 19/100...  Training Step: 620...  Training loss: 1.7436...  0.4558 sec/batch\n",
      "Epoch: 19/100...  Training Step: 621...  Training loss: 1.7441...  0.4516 sec/batch\n",
      "Epoch: 19/100...  Training Step: 622...  Training loss: 1.7757...  0.4633 sec/batch\n",
      "Epoch: 19/100...  Training Step: 623...  Training loss: 1.7770...  0.4576 sec/batch\n",
      "Epoch: 19/100...  Training Step: 624...  Training loss: 1.7863...  0.4527 sec/batch\n",
      "Epoch: 19/100...  Training Step: 625...  Training loss: 1.7769...  0.4559 sec/batch\n",
      "Epoch: 19/100...  Training Step: 626...  Training loss: 1.7625...  0.4494 sec/batch\n",
      "Epoch: 19/100...  Training Step: 627...  Training loss: 1.7293...  0.4581 sec/batch\n",
      "Epoch: 19/100...  Training Step: 628...  Training loss: 1.7718...  0.4519 sec/batch\n",
      "Epoch: 19/100...  Training Step: 629...  Training loss: 1.7802...  0.4539 sec/batch\n",
      "Epoch: 19/100...  Training Step: 630...  Training loss: 1.7586...  0.4481 sec/batch\n",
      "Epoch: 19/100...  Training Step: 631...  Training loss: 1.7585...  0.4525 sec/batch\n",
      "Epoch: 19/100...  Training Step: 632...  Training loss: 1.7253...  0.4537 sec/batch\n",
      "Epoch: 19/100...  Training Step: 633...  Training loss: 1.7569...  0.4549 sec/batch\n",
      "Epoch: 19/100...  Training Step: 634...  Training loss: 1.7212...  0.4536 sec/batch\n",
      "Epoch: 19/100...  Training Step: 635...  Training loss: 1.7385...  0.4539 sec/batch\n",
      "Epoch: 19/100...  Training Step: 636...  Training loss: 1.7536...  0.4572 sec/batch\n",
      "Epoch: 19/100...  Training Step: 637...  Training loss: 1.7176...  0.4568 sec/batch\n",
      "Epoch: 19/100...  Training Step: 638...  Training loss: 1.7316...  0.4430 sec/batch\n",
      "Epoch: 19/100...  Training Step: 639...  Training loss: 1.7294...  0.4379 sec/batch\n",
      "Epoch: 19/100...  Training Step: 640...  Training loss: 1.7273...  0.4515 sec/batch\n",
      "Epoch: 19/100...  Training Step: 641...  Training loss: 1.7773...  0.4567 sec/batch\n",
      "Epoch: 19/100...  Training Step: 642...  Training loss: 1.7836...  0.4547 sec/batch\n",
      "Epoch: 19/100...  Training Step: 643...  Training loss: 1.7675...  0.4536 sec/batch\n",
      "Epoch: 19/100...  Training Step: 644...  Training loss: 1.7727...  0.4487 sec/batch\n",
      "Epoch: 19/100...  Training Step: 645...  Training loss: 1.7837...  0.4555 sec/batch\n",
      "Epoch: 19/100...  Training Step: 646...  Training loss: 1.7270...  0.4574 sec/batch\n",
      "Epoch: 20/100...  Training Step: 647...  Training loss: 1.8284...  0.4517 sec/batch\n",
      "Epoch: 20/100...  Training Step: 648...  Training loss: 1.7354...  0.4546 sec/batch\n",
      "Epoch: 20/100...  Training Step: 649...  Training loss: 1.7362...  0.4565 sec/batch\n",
      "Epoch: 20/100...  Training Step: 650...  Training loss: 1.7664...  0.4348 sec/batch\n",
      "Epoch: 20/100...  Training Step: 651...  Training loss: 1.7640...  0.4515 sec/batch\n",
      "Epoch: 20/100...  Training Step: 652...  Training loss: 1.7427...  0.4579 sec/batch\n",
      "Epoch: 20/100...  Training Step: 653...  Training loss: 1.7489...  0.4404 sec/batch\n",
      "Epoch: 20/100...  Training Step: 654...  Training loss: 1.7253...  0.4514 sec/batch\n",
      "Epoch: 20/100...  Training Step: 655...  Training loss: 1.7126...  0.4608 sec/batch\n",
      "Epoch: 20/100...  Training Step: 656...  Training loss: 1.7539...  0.4467 sec/batch\n",
      "Epoch: 20/100...  Training Step: 657...  Training loss: 1.7545...  0.4359 sec/batch\n",
      "Epoch: 20/100...  Training Step: 658...  Training loss: 1.7664...  0.4551 sec/batch\n",
      "Epoch: 20/100...  Training Step: 659...  Training loss: 1.7581...  0.4584 sec/batch\n",
      "Epoch: 20/100...  Training Step: 660...  Training loss: 1.7389...  0.4596 sec/batch\n",
      "Epoch: 20/100...  Training Step: 661...  Training loss: 1.7204...  0.4611 sec/batch\n",
      "Epoch: 20/100...  Training Step: 662...  Training loss: 1.7512...  0.4517 sec/batch\n",
      "Epoch: 20/100...  Training Step: 663...  Training loss: 1.7554...  0.4585 sec/batch\n",
      "Epoch: 20/100...  Training Step: 664...  Training loss: 1.7296...  0.4642 sec/batch\n",
      "Epoch: 20/100...  Training Step: 665...  Training loss: 1.7405...  0.4537 sec/batch\n",
      "Epoch: 20/100...  Training Step: 666...  Training loss: 1.7071...  0.4538 sec/batch\n",
      "Epoch: 20/100...  Training Step: 667...  Training loss: 1.7267...  0.4549 sec/batch\n",
      "Epoch: 20/100...  Training Step: 668...  Training loss: 1.6928...  0.4594 sec/batch\n",
      "Epoch: 20/100...  Training Step: 669...  Training loss: 1.7138...  0.4560 sec/batch\n",
      "Epoch: 20/100...  Training Step: 670...  Training loss: 1.7206...  0.4578 sec/batch\n",
      "Epoch: 20/100...  Training Step: 671...  Training loss: 1.6880...  0.4516 sec/batch\n",
      "Epoch: 20/100...  Training Step: 672...  Training loss: 1.7081...  0.4553 sec/batch\n",
      "Epoch: 20/100...  Training Step: 673...  Training loss: 1.7023...  0.4581 sec/batch\n",
      "Epoch: 20/100...  Training Step: 674...  Training loss: 1.7046...  0.4617 sec/batch\n",
      "Epoch: 20/100...  Training Step: 675...  Training loss: 1.7353...  0.4550 sec/batch\n",
      "Epoch: 20/100...  Training Step: 676...  Training loss: 1.7555...  0.4585 sec/batch\n",
      "Epoch: 20/100...  Training Step: 677...  Training loss: 1.7249...  0.4418 sec/batch\n",
      "Epoch: 20/100...  Training Step: 678...  Training loss: 1.7421...  0.4546 sec/batch\n",
      "Epoch: 20/100...  Training Step: 679...  Training loss: 1.7608...  0.4551 sec/batch\n",
      "Epoch: 20/100...  Training Step: 680...  Training loss: 1.7083...  0.4536 sec/batch\n",
      "Epoch: 21/100...  Training Step: 681...  Training loss: 1.7993...  0.4567 sec/batch\n",
      "Epoch: 21/100...  Training Step: 682...  Training loss: 1.7059...  0.4531 sec/batch\n",
      "Epoch: 21/100...  Training Step: 683...  Training loss: 1.7209...  0.4502 sec/batch\n",
      "Epoch: 21/100...  Training Step: 684...  Training loss: 1.7430...  0.4553 sec/batch\n",
      "Epoch: 21/100...  Training Step: 685...  Training loss: 1.7364...  0.4592 sec/batch\n",
      "Epoch: 21/100...  Training Step: 686...  Training loss: 1.7101...  0.4570 sec/batch\n",
      "Epoch: 21/100...  Training Step: 687...  Training loss: 1.7245...  0.4517 sec/batch\n",
      "Epoch: 21/100...  Training Step: 688...  Training loss: 1.6889...  0.4568 sec/batch\n",
      "Epoch: 21/100...  Training Step: 689...  Training loss: 1.6844...  0.4544 sec/batch\n",
      "Epoch: 21/100...  Training Step: 690...  Training loss: 1.7216...  0.4602 sec/batch\n",
      "Epoch: 21/100...  Training Step: 691...  Training loss: 1.7347...  0.4532 sec/batch\n",
      "Epoch: 21/100...  Training Step: 692...  Training loss: 1.7301...  0.4551 sec/batch\n",
      "Epoch: 21/100...  Training Step: 693...  Training loss: 1.7266...  0.4520 sec/batch\n",
      "Epoch: 21/100...  Training Step: 694...  Training loss: 1.7129...  0.4520 sec/batch\n",
      "Epoch: 21/100...  Training Step: 695...  Training loss: 1.6836...  0.4562 sec/batch\n",
      "Epoch: 21/100...  Training Step: 696...  Training loss: 1.7250...  0.4515 sec/batch\n",
      "Epoch: 21/100...  Training Step: 697...  Training loss: 1.7210...  0.4657 sec/batch\n",
      "Epoch: 21/100...  Training Step: 698...  Training loss: 1.6981...  0.4542 sec/batch\n",
      "Epoch: 21/100...  Training Step: 699...  Training loss: 1.7016...  0.4559 sec/batch\n",
      "Epoch: 21/100...  Training Step: 700...  Training loss: 1.6802...  0.4514 sec/batch\n",
      "Epoch: 21/100...  Training Step: 701...  Training loss: 1.7132...  0.4640 sec/batch\n",
      "Epoch: 21/100...  Training Step: 702...  Training loss: 1.6692...  0.4648 sec/batch\n",
      "Epoch: 21/100...  Training Step: 703...  Training loss: 1.6832...  0.4520 sec/batch\n",
      "Epoch: 21/100...  Training Step: 704...  Training loss: 1.7040...  0.4548 sec/batch\n",
      "Epoch: 21/100...  Training Step: 705...  Training loss: 1.6692...  0.4507 sec/batch\n",
      "Epoch: 21/100...  Training Step: 706...  Training loss: 1.6907...  0.4510 sec/batch\n",
      "Epoch: 21/100...  Training Step: 707...  Training loss: 1.6770...  0.4548 sec/batch\n",
      "Epoch: 21/100...  Training Step: 708...  Training loss: 1.6729...  0.4561 sec/batch\n",
      "Epoch: 21/100...  Training Step: 709...  Training loss: 1.7258...  0.4575 sec/batch\n",
      "Epoch: 21/100...  Training Step: 710...  Training loss: 1.7287...  0.4608 sec/batch\n",
      "Epoch: 21/100...  Training Step: 711...  Training loss: 1.7069...  0.4478 sec/batch\n",
      "Epoch: 21/100...  Training Step: 712...  Training loss: 1.7227...  0.4288 sec/batch\n",
      "Epoch: 21/100...  Training Step: 713...  Training loss: 1.7315...  0.4546 sec/batch\n",
      "Epoch: 21/100...  Training Step: 714...  Training loss: 1.6755...  0.4566 sec/batch\n",
      "Epoch: 22/100...  Training Step: 715...  Training loss: 1.7757...  0.4514 sec/batch\n",
      "Epoch: 22/100...  Training Step: 716...  Training loss: 1.6780...  0.4491 sec/batch\n",
      "Epoch: 22/100...  Training Step: 717...  Training loss: 1.6815...  0.4553 sec/batch\n",
      "Epoch: 22/100...  Training Step: 718...  Training loss: 1.7193...  0.4391 sec/batch\n",
      "Epoch: 22/100...  Training Step: 719...  Training loss: 1.7068...  0.4540 sec/batch\n",
      "Epoch: 22/100...  Training Step: 720...  Training loss: 1.6804...  0.4500 sec/batch\n",
      "Epoch: 22/100...  Training Step: 721...  Training loss: 1.6942...  0.4538 sec/batch\n",
      "Epoch: 22/100...  Training Step: 722...  Training loss: 1.6562...  0.4542 sec/batch\n",
      "Epoch: 22/100...  Training Step: 723...  Training loss: 1.6603...  0.4652 sec/batch\n",
      "Epoch: 22/100...  Training Step: 724...  Training loss: 1.6931...  0.4484 sec/batch\n",
      "Epoch: 22/100...  Training Step: 725...  Training loss: 1.6976...  0.4460 sec/batch\n",
      "Epoch: 22/100...  Training Step: 726...  Training loss: 1.7094...  0.4597 sec/batch\n",
      "Epoch: 22/100...  Training Step: 727...  Training loss: 1.7065...  0.4577 sec/batch\n",
      "Epoch: 22/100...  Training Step: 728...  Training loss: 1.6848...  0.4617 sec/batch\n",
      "Epoch: 22/100...  Training Step: 729...  Training loss: 1.6617...  0.4643 sec/batch\n",
      "Epoch: 22/100...  Training Step: 730...  Training loss: 1.6934...  0.4550 sec/batch\n",
      "Epoch: 22/100...  Training Step: 731...  Training loss: 1.6988...  0.4542 sec/batch\n",
      "Epoch: 22/100...  Training Step: 732...  Training loss: 1.6650...  0.4568 sec/batch\n",
      "Epoch: 22/100...  Training Step: 733...  Training loss: 1.6832...  0.4606 sec/batch\n",
      "Epoch: 22/100...  Training Step: 734...  Training loss: 1.6591...  0.4315 sec/batch\n",
      "Epoch: 22/100...  Training Step: 735...  Training loss: 1.6744...  0.4558 sec/batch\n",
      "Epoch: 22/100...  Training Step: 736...  Training loss: 1.6469...  0.4382 sec/batch\n",
      "Epoch: 22/100...  Training Step: 737...  Training loss: 1.6597...  0.4542 sec/batch\n",
      "Epoch: 22/100...  Training Step: 738...  Training loss: 1.6702...  0.4512 sec/batch\n",
      "Epoch: 22/100...  Training Step: 739...  Training loss: 1.6368...  0.4516 sec/batch\n",
      "Epoch: 22/100...  Training Step: 740...  Training loss: 1.6494...  0.4552 sec/batch\n",
      "Epoch: 22/100...  Training Step: 741...  Training loss: 1.6455...  0.4495 sec/batch\n",
      "Epoch: 22/100...  Training Step: 742...  Training loss: 1.6486...  0.4525 sec/batch\n",
      "Epoch: 22/100...  Training Step: 743...  Training loss: 1.6857...  0.4487 sec/batch\n",
      "Epoch: 22/100...  Training Step: 744...  Training loss: 1.7076...  0.4501 sec/batch\n",
      "Epoch: 22/100...  Training Step: 745...  Training loss: 1.6778...  0.4521 sec/batch\n",
      "Epoch: 22/100...  Training Step: 746...  Training loss: 1.6959...  0.4519 sec/batch\n",
      "Epoch: 22/100...  Training Step: 747...  Training loss: 1.6966...  0.4567 sec/batch\n",
      "Epoch: 22/100...  Training Step: 748...  Training loss: 1.6536...  0.4582 sec/batch\n",
      "Epoch: 23/100...  Training Step: 749...  Training loss: 1.7504...  0.4575 sec/batch\n",
      "Epoch: 23/100...  Training Step: 750...  Training loss: 1.6536...  0.4460 sec/batch\n",
      "Epoch: 23/100...  Training Step: 751...  Training loss: 1.6666...  0.4430 sec/batch\n",
      "Epoch: 23/100...  Training Step: 752...  Training loss: 1.6881...  0.4620 sec/batch\n",
      "Epoch: 23/100...  Training Step: 753...  Training loss: 1.6818...  0.4538 sec/batch\n",
      "Epoch: 23/100...  Training Step: 754...  Training loss: 1.6593...  0.4544 sec/batch\n",
      "Epoch: 23/100...  Training Step: 755...  Training loss: 1.6745...  0.4555 sec/batch\n",
      "Epoch: 23/100...  Training Step: 756...  Training loss: 1.6427...  0.4620 sec/batch\n",
      "Epoch: 23/100...  Training Step: 757...  Training loss: 1.6359...  0.4640 sec/batch\n",
      "Epoch: 23/100...  Training Step: 758...  Training loss: 1.6750...  0.4593 sec/batch\n",
      "Epoch: 23/100...  Training Step: 759...  Training loss: 1.6758...  0.4570 sec/batch\n",
      "Epoch: 23/100...  Training Step: 760...  Training loss: 1.6725...  0.4656 sec/batch\n",
      "Epoch: 23/100...  Training Step: 761...  Training loss: 1.6737...  0.4516 sec/batch\n",
      "Epoch: 23/100...  Training Step: 762...  Training loss: 1.6642...  0.4547 sec/batch\n",
      "Epoch: 23/100...  Training Step: 763...  Training loss: 1.6301...  0.4550 sec/batch\n",
      "Epoch: 23/100...  Training Step: 764...  Training loss: 1.6745...  0.4553 sec/batch\n",
      "Epoch: 23/100...  Training Step: 765...  Training loss: 1.6811...  0.4564 sec/batch\n",
      "Epoch: 23/100...  Training Step: 766...  Training loss: 1.6503...  0.4638 sec/batch\n",
      "Epoch: 23/100...  Training Step: 767...  Training loss: 1.6586...  0.4449 sec/batch\n",
      "Epoch: 23/100...  Training Step: 768...  Training loss: 1.6299...  0.4647 sec/batch\n",
      "Epoch: 23/100...  Training Step: 769...  Training loss: 1.6532...  0.4555 sec/batch\n",
      "Epoch: 23/100...  Training Step: 770...  Training loss: 1.6124...  0.4506 sec/batch\n",
      "Epoch: 23/100...  Training Step: 771...  Training loss: 1.6326...  0.4634 sec/batch\n",
      "Epoch: 23/100...  Training Step: 772...  Training loss: 1.6325...  0.4616 sec/batch\n",
      "Epoch: 23/100...  Training Step: 773...  Training loss: 1.6144...  0.4422 sec/batch\n",
      "Epoch: 23/100...  Training Step: 774...  Training loss: 1.6243...  0.4530 sec/batch\n",
      "Epoch: 23/100...  Training Step: 775...  Training loss: 1.6144...  0.4537 sec/batch\n",
      "Epoch: 23/100...  Training Step: 776...  Training loss: 1.6211...  0.4542 sec/batch\n",
      "Epoch: 23/100...  Training Step: 777...  Training loss: 1.6707...  0.4568 sec/batch\n",
      "Epoch: 23/100...  Training Step: 778...  Training loss: 1.6618...  0.4539 sec/batch\n",
      "Epoch: 23/100...  Training Step: 779...  Training loss: 1.6571...  0.4542 sec/batch\n",
      "Epoch: 23/100...  Training Step: 780...  Training loss: 1.6676...  0.4500 sec/batch\n",
      "Epoch: 23/100...  Training Step: 781...  Training loss: 1.6816...  0.4497 sec/batch\n",
      "Epoch: 23/100...  Training Step: 782...  Training loss: 1.6356...  0.4596 sec/batch\n",
      "Epoch: 24/100...  Training Step: 783...  Training loss: 1.7309...  0.4517 sec/batch\n",
      "Epoch: 24/100...  Training Step: 784...  Training loss: 1.6336...  0.4512 sec/batch\n",
      "Epoch: 24/100...  Training Step: 785...  Training loss: 1.6380...  0.4596 sec/batch\n",
      "Epoch: 24/100...  Training Step: 786...  Training loss: 1.6758...  0.4621 sec/batch\n",
      "Epoch: 24/100...  Training Step: 787...  Training loss: 1.6669...  0.4594 sec/batch\n",
      "Epoch: 24/100...  Training Step: 788...  Training loss: 1.6379...  0.4508 sec/batch\n",
      "Epoch: 24/100...  Training Step: 789...  Training loss: 1.6430...  0.4533 sec/batch\n",
      "Epoch: 24/100...  Training Step: 790...  Training loss: 1.6171...  0.4549 sec/batch\n",
      "Epoch: 24/100...  Training Step: 791...  Training loss: 1.6180...  0.4503 sec/batch\n",
      "Epoch: 24/100...  Training Step: 792...  Training loss: 1.6520...  0.4652 sec/batch\n",
      "Epoch: 24/100...  Training Step: 793...  Training loss: 1.6512...  0.4602 sec/batch\n",
      "Epoch: 24/100...  Training Step: 794...  Training loss: 1.6530...  0.4542 sec/batch\n",
      "Epoch: 24/100...  Training Step: 795...  Training loss: 1.6567...  0.4491 sec/batch\n",
      "Epoch: 24/100...  Training Step: 796...  Training loss: 1.6449...  0.4509 sec/batch\n",
      "Epoch: 24/100...  Training Step: 797...  Training loss: 1.6199...  0.4507 sec/batch\n",
      "Epoch: 24/100...  Training Step: 798...  Training loss: 1.6537...  0.4606 sec/batch\n",
      "Epoch: 24/100...  Training Step: 799...  Training loss: 1.6620...  0.4544 sec/batch\n",
      "Epoch: 24/100...  Training Step: 800...  Training loss: 1.6265...  0.4480 sec/batch\n",
      "Epoch: 24/100...  Training Step: 801...  Training loss: 1.6383...  0.4497 sec/batch\n",
      "Epoch: 24/100...  Training Step: 802...  Training loss: 1.6139...  0.4501 sec/batch\n",
      "Epoch: 24/100...  Training Step: 803...  Training loss: 1.6324...  0.4583 sec/batch\n",
      "Epoch: 24/100...  Training Step: 804...  Training loss: 1.6003...  0.4478 sec/batch\n",
      "Epoch: 24/100...  Training Step: 805...  Training loss: 1.6177...  0.4555 sec/batch\n",
      "Epoch: 24/100...  Training Step: 806...  Training loss: 1.6303...  0.4567 sec/batch\n",
      "Epoch: 24/100...  Training Step: 807...  Training loss: 1.5966...  0.4569 sec/batch\n",
      "Epoch: 24/100...  Training Step: 808...  Training loss: 1.6073...  0.4579 sec/batch\n",
      "Epoch: 24/100...  Training Step: 809...  Training loss: 1.5960...  0.4496 sec/batch\n",
      "Epoch: 24/100...  Training Step: 810...  Training loss: 1.6065...  0.4499 sec/batch\n",
      "Epoch: 24/100...  Training Step: 811...  Training loss: 1.6546...  0.4528 sec/batch\n",
      "Epoch: 24/100...  Training Step: 812...  Training loss: 1.6526...  0.4470 sec/batch\n",
      "Epoch: 24/100...  Training Step: 813...  Training loss: 1.6347...  0.4541 sec/batch\n",
      "Epoch: 24/100...  Training Step: 814...  Training loss: 1.6516...  0.4551 sec/batch\n",
      "Epoch: 24/100...  Training Step: 815...  Training loss: 1.6549...  0.4512 sec/batch\n",
      "Epoch: 24/100...  Training Step: 816...  Training loss: 1.6084...  0.4542 sec/batch\n",
      "Epoch: 25/100...  Training Step: 817...  Training loss: 1.7087...  0.4614 sec/batch\n",
      "Epoch: 25/100...  Training Step: 818...  Training loss: 1.6113...  0.4585 sec/batch\n",
      "Epoch: 25/100...  Training Step: 819...  Training loss: 1.6195...  0.4550 sec/batch\n",
      "Epoch: 25/100...  Training Step: 820...  Training loss: 1.6616...  0.4525 sec/batch\n",
      "Epoch: 25/100...  Training Step: 821...  Training loss: 1.6330...  0.4616 sec/batch\n",
      "Epoch: 25/100...  Training Step: 822...  Training loss: 1.6090...  0.4585 sec/batch\n",
      "Epoch: 25/100...  Training Step: 823...  Training loss: 1.6315...  0.4560 sec/batch\n",
      "Epoch: 25/100...  Training Step: 824...  Training loss: 1.5972...  0.4577 sec/batch\n",
      "Epoch: 25/100...  Training Step: 825...  Training loss: 1.5924...  0.4520 sec/batch\n",
      "Epoch: 25/100...  Training Step: 826...  Training loss: 1.6208...  0.4522 sec/batch\n",
      "Epoch: 25/100...  Training Step: 827...  Training loss: 1.6266...  0.4567 sec/batch\n",
      "Epoch: 25/100...  Training Step: 828...  Training loss: 1.6347...  0.4584 sec/batch\n",
      "Epoch: 25/100...  Training Step: 829...  Training loss: 1.6364...  0.4568 sec/batch\n",
      "Epoch: 25/100...  Training Step: 830...  Training loss: 1.6218...  0.4534 sec/batch\n",
      "Epoch: 25/100...  Training Step: 831...  Training loss: 1.5951...  0.4574 sec/batch\n",
      "Epoch: 25/100...  Training Step: 832...  Training loss: 1.6300...  0.4537 sec/batch\n",
      "Epoch: 25/100...  Training Step: 833...  Training loss: 1.6372...  0.4578 sec/batch\n",
      "Epoch: 25/100...  Training Step: 834...  Training loss: 1.6085...  0.4526 sec/batch\n",
      "Epoch: 25/100...  Training Step: 835...  Training loss: 1.6113...  0.4479 sec/batch\n",
      "Epoch: 25/100...  Training Step: 836...  Training loss: 1.5856...  0.4415 sec/batch\n",
      "Epoch: 25/100...  Training Step: 837...  Training loss: 1.6155...  0.4566 sec/batch\n",
      "Epoch: 25/100...  Training Step: 838...  Training loss: 1.5748...  0.4553 sec/batch\n",
      "Epoch: 25/100...  Training Step: 839...  Training loss: 1.5997...  0.4557 sec/batch\n",
      "Epoch: 25/100...  Training Step: 840...  Training loss: 1.6010...  0.4500 sec/batch\n",
      "Epoch: 25/100...  Training Step: 841...  Training loss: 1.5829...  0.4529 sec/batch\n",
      "Epoch: 25/100...  Training Step: 842...  Training loss: 1.5851...  0.4740 sec/batch\n",
      "Epoch: 25/100...  Training Step: 843...  Training loss: 1.5814...  0.4474 sec/batch\n",
      "Epoch: 25/100...  Training Step: 844...  Training loss: 1.5821...  0.4503 sec/batch\n",
      "Epoch: 25/100...  Training Step: 845...  Training loss: 1.6326...  0.4558 sec/batch\n",
      "Epoch: 25/100...  Training Step: 846...  Training loss: 1.6345...  0.4496 sec/batch\n",
      "Epoch: 25/100...  Training Step: 847...  Training loss: 1.6111...  0.4532 sec/batch\n",
      "Epoch: 25/100...  Training Step: 848...  Training loss: 1.6288...  0.4483 sec/batch\n",
      "Epoch: 25/100...  Training Step: 849...  Training loss: 1.6423...  0.4603 sec/batch\n",
      "Epoch: 25/100...  Training Step: 850...  Training loss: 1.6010...  0.4547 sec/batch\n",
      "Epoch: 26/100...  Training Step: 851...  Training loss: 1.6845...  0.4570 sec/batch\n",
      "Epoch: 26/100...  Training Step: 852...  Training loss: 1.5869...  0.4475 sec/batch\n",
      "Epoch: 26/100...  Training Step: 853...  Training loss: 1.6019...  0.4482 sec/batch\n",
      "Epoch: 26/100...  Training Step: 854...  Training loss: 1.6270...  0.4531 sec/batch\n",
      "Epoch: 26/100...  Training Step: 855...  Training loss: 1.6189...  0.4460 sec/batch\n",
      "Epoch: 26/100...  Training Step: 856...  Training loss: 1.5857...  0.4477 sec/batch\n",
      "Epoch: 26/100...  Training Step: 857...  Training loss: 1.6007...  0.4489 sec/batch\n",
      "Epoch: 26/100...  Training Step: 858...  Training loss: 1.5759...  0.4453 sec/batch\n",
      "Epoch: 26/100...  Training Step: 859...  Training loss: 1.5679...  0.4602 sec/batch\n",
      "Epoch: 26/100...  Training Step: 860...  Training loss: 1.6204...  0.4482 sec/batch\n",
      "Epoch: 26/100...  Training Step: 861...  Training loss: 1.6109...  0.4524 sec/batch\n",
      "Epoch: 26/100...  Training Step: 862...  Training loss: 1.6021...  0.4538 sec/batch\n",
      "Epoch: 26/100...  Training Step: 863...  Training loss: 1.6158...  0.4519 sec/batch\n",
      "Epoch: 26/100...  Training Step: 864...  Training loss: 1.5956...  0.4575 sec/batch\n",
      "Epoch: 26/100...  Training Step: 865...  Training loss: 1.5686...  0.4521 sec/batch\n",
      "Epoch: 26/100...  Training Step: 866...  Training loss: 1.6075...  0.4543 sec/batch\n",
      "Epoch: 26/100...  Training Step: 867...  Training loss: 1.6085...  0.4502 sec/batch\n",
      "Epoch: 26/100...  Training Step: 868...  Training loss: 1.5843...  0.4609 sec/batch\n",
      "Epoch: 26/100...  Training Step: 869...  Training loss: 1.5962...  0.4592 sec/batch\n",
      "Epoch: 26/100...  Training Step: 870...  Training loss: 1.5706...  0.4437 sec/batch\n",
      "Epoch: 26/100...  Training Step: 871...  Training loss: 1.5829...  0.4550 sec/batch\n",
      "Epoch: 26/100...  Training Step: 872...  Training loss: 1.5551...  0.4532 sec/batch\n",
      "Epoch: 26/100...  Training Step: 873...  Training loss: 1.5726...  0.4544 sec/batch\n",
      "Epoch: 26/100...  Training Step: 874...  Training loss: 1.5820...  0.4555 sec/batch\n",
      "Epoch: 26/100...  Training Step: 875...  Training loss: 1.5625...  0.4553 sec/batch\n",
      "Epoch: 26/100...  Training Step: 876...  Training loss: 1.5747...  0.4582 sec/batch\n",
      "Epoch: 26/100...  Training Step: 877...  Training loss: 1.5607...  0.4450 sec/batch\n",
      "Epoch: 26/100...  Training Step: 878...  Training loss: 1.5711...  0.4585 sec/batch\n",
      "Epoch: 26/100...  Training Step: 879...  Training loss: 1.6082...  0.4600 sec/batch\n",
      "Epoch: 26/100...  Training Step: 880...  Training loss: 1.6152...  0.4539 sec/batch\n",
      "Epoch: 26/100...  Training Step: 881...  Training loss: 1.5913...  0.4462 sec/batch\n",
      "Epoch: 26/100...  Training Step: 882...  Training loss: 1.6086...  0.4532 sec/batch\n",
      "Epoch: 26/100...  Training Step: 883...  Training loss: 1.6207...  0.4543 sec/batch\n",
      "Epoch: 26/100...  Training Step: 884...  Training loss: 1.5744...  0.4543 sec/batch\n",
      "Epoch: 27/100...  Training Step: 885...  Training loss: 1.6558...  0.4467 sec/batch\n",
      "Epoch: 27/100...  Training Step: 886...  Training loss: 1.5626...  0.4453 sec/batch\n",
      "Epoch: 27/100...  Training Step: 887...  Training loss: 1.5810...  0.4551 sec/batch\n",
      "Epoch: 27/100...  Training Step: 888...  Training loss: 1.6206...  0.4508 sec/batch\n",
      "Epoch: 27/100...  Training Step: 889...  Training loss: 1.5964...  0.4535 sec/batch\n",
      "Epoch: 27/100...  Training Step: 890...  Training loss: 1.5712...  0.4581 sec/batch\n",
      "Epoch: 27/100...  Training Step: 891...  Training loss: 1.5855...  0.4525 sec/batch\n",
      "Epoch: 27/100...  Training Step: 892...  Training loss: 1.5530...  0.4442 sec/batch\n",
      "Epoch: 27/100...  Training Step: 893...  Training loss: 1.5538...  0.4587 sec/batch\n",
      "Epoch: 27/100...  Training Step: 894...  Training loss: 1.5960...  0.4579 sec/batch\n",
      "Epoch: 27/100...  Training Step: 895...  Training loss: 1.5851...  0.4505 sec/batch\n",
      "Epoch: 27/100...  Training Step: 896...  Training loss: 1.5893...  0.4516 sec/batch\n",
      "Epoch: 27/100...  Training Step: 897...  Training loss: 1.5979...  0.4615 sec/batch\n",
      "Epoch: 27/100...  Training Step: 898...  Training loss: 1.5875...  0.4529 sec/batch\n",
      "Epoch: 27/100...  Training Step: 899...  Training loss: 1.5544...  0.4538 sec/batch\n",
      "Epoch: 27/100...  Training Step: 900...  Training loss: 1.5889...  0.4501 sec/batch\n",
      "Epoch: 27/100...  Training Step: 901...  Training loss: 1.6037...  0.4551 sec/batch\n",
      "Epoch: 27/100...  Training Step: 902...  Training loss: 1.5730...  0.4638 sec/batch\n",
      "Epoch: 27/100...  Training Step: 903...  Training loss: 1.5804...  0.4525 sec/batch\n",
      "Epoch: 27/100...  Training Step: 904...  Training loss: 1.5498...  0.4597 sec/batch\n",
      "Epoch: 27/100...  Training Step: 905...  Training loss: 1.5731...  0.4527 sec/batch\n",
      "Epoch: 27/100...  Training Step: 906...  Training loss: 1.5381...  0.4582 sec/batch\n",
      "Epoch: 27/100...  Training Step: 907...  Training loss: 1.5618...  0.4497 sec/batch\n",
      "Epoch: 27/100...  Training Step: 908...  Training loss: 1.5646...  0.4553 sec/batch\n",
      "Epoch: 27/100...  Training Step: 909...  Training loss: 1.5484...  0.4556 sec/batch\n",
      "Epoch: 27/100...  Training Step: 910...  Training loss: 1.5567...  0.4521 sec/batch\n",
      "Epoch: 27/100...  Training Step: 911...  Training loss: 1.5517...  0.4509 sec/batch\n",
      "Epoch: 27/100...  Training Step: 912...  Training loss: 1.5413...  0.4548 sec/batch\n",
      "Epoch: 27/100...  Training Step: 913...  Training loss: 1.5848...  0.4494 sec/batch\n",
      "Epoch: 27/100...  Training Step: 914...  Training loss: 1.6018...  0.4588 sec/batch\n",
      "Epoch: 27/100...  Training Step: 915...  Training loss: 1.5629...  0.4528 sec/batch\n",
      "Epoch: 27/100...  Training Step: 916...  Training loss: 1.5999...  0.4562 sec/batch\n",
      "Epoch: 27/100...  Training Step: 917...  Training loss: 1.6080...  0.4574 sec/batch\n",
      "Epoch: 27/100...  Training Step: 918...  Training loss: 1.5648...  0.4486 sec/batch\n",
      "Epoch: 28/100...  Training Step: 919...  Training loss: 1.6431...  0.4518 sec/batch\n",
      "Epoch: 28/100...  Training Step: 920...  Training loss: 1.5565...  0.4475 sec/batch\n",
      "Epoch: 28/100...  Training Step: 921...  Training loss: 1.5726...  0.4532 sec/batch\n",
      "Epoch: 28/100...  Training Step: 922...  Training loss: 1.5892...  0.4502 sec/batch\n",
      "Epoch: 28/100...  Training Step: 923...  Training loss: 1.5798...  0.4535 sec/batch\n",
      "Epoch: 28/100...  Training Step: 924...  Training loss: 1.5586...  0.4581 sec/batch\n",
      "Epoch: 28/100...  Training Step: 925...  Training loss: 1.5726...  0.4581 sec/batch\n",
      "Epoch: 28/100...  Training Step: 926...  Training loss: 1.5424...  0.4515 sec/batch\n",
      "Epoch: 28/100...  Training Step: 927...  Training loss: 1.5368...  0.4565 sec/batch\n",
      "Epoch: 28/100...  Training Step: 928...  Training loss: 1.5803...  0.4529 sec/batch\n",
      "Epoch: 28/100...  Training Step: 929...  Training loss: 1.5736...  0.4588 sec/batch\n",
      "Epoch: 28/100...  Training Step: 930...  Training loss: 1.5780...  0.4512 sec/batch\n",
      "Epoch: 28/100...  Training Step: 931...  Training loss: 1.5801...  0.4564 sec/batch\n",
      "Epoch: 28/100...  Training Step: 932...  Training loss: 1.5646...  0.4520 sec/batch\n",
      "Epoch: 28/100...  Training Step: 933...  Training loss: 1.5398...  0.4561 sec/batch\n",
      "Epoch: 28/100...  Training Step: 934...  Training loss: 1.5735...  0.4560 sec/batch\n",
      "Epoch: 28/100...  Training Step: 935...  Training loss: 1.5805...  0.4529 sec/batch\n",
      "Epoch: 28/100...  Training Step: 936...  Training loss: 1.5543...  0.4542 sec/batch\n",
      "Epoch: 28/100...  Training Step: 937...  Training loss: 1.5551...  0.4516 sec/batch\n",
      "Epoch: 28/100...  Training Step: 938...  Training loss: 1.5393...  0.4539 sec/batch\n",
      "Epoch: 28/100...  Training Step: 939...  Training loss: 1.5608...  0.4544 sec/batch\n",
      "Epoch: 28/100...  Training Step: 940...  Training loss: 1.5164...  0.4550 sec/batch\n",
      "Epoch: 28/100...  Training Step: 941...  Training loss: 1.5404...  0.4456 sec/batch\n",
      "Epoch: 28/100...  Training Step: 942...  Training loss: 1.5421...  0.4575 sec/batch\n",
      "Epoch: 28/100...  Training Step: 943...  Training loss: 1.5139...  0.4520 sec/batch\n",
      "Epoch: 28/100...  Training Step: 944...  Training loss: 1.5260...  0.4556 sec/batch\n",
      "Epoch: 28/100...  Training Step: 945...  Training loss: 1.5256...  0.4573 sec/batch\n",
      "Epoch: 28/100...  Training Step: 946...  Training loss: 1.5225...  0.4561 sec/batch\n",
      "Epoch: 28/100...  Training Step: 947...  Training loss: 1.5674...  0.4492 sec/batch\n",
      "Epoch: 28/100...  Training Step: 948...  Training loss: 1.5717...  0.4542 sec/batch\n",
      "Epoch: 28/100...  Training Step: 949...  Training loss: 1.5485...  0.4552 sec/batch\n",
      "Epoch: 28/100...  Training Step: 950...  Training loss: 1.5749...  0.4515 sec/batch\n",
      "Epoch: 28/100...  Training Step: 951...  Training loss: 1.5776...  0.4522 sec/batch\n",
      "Epoch: 28/100...  Training Step: 952...  Training loss: 1.5404...  0.4524 sec/batch\n",
      "Epoch: 29/100...  Training Step: 953...  Training loss: 1.6187...  0.4523 sec/batch\n",
      "Epoch: 29/100...  Training Step: 954...  Training loss: 1.5297...  0.4565 sec/batch\n",
      "Epoch: 29/100...  Training Step: 955...  Training loss: 1.5356...  0.4499 sec/batch\n",
      "Epoch: 29/100...  Training Step: 956...  Training loss: 1.5816...  0.4525 sec/batch\n",
      "Epoch: 29/100...  Training Step: 957...  Training loss: 1.5823...  0.4507 sec/batch\n",
      "Epoch: 29/100...  Training Step: 958...  Training loss: 1.5365...  0.4422 sec/batch\n",
      "Epoch: 29/100...  Training Step: 959...  Training loss: 1.5554...  0.4553 sec/batch\n",
      "Epoch: 29/100...  Training Step: 960...  Training loss: 1.5252...  0.4568 sec/batch\n",
      "Epoch: 29/100...  Training Step: 961...  Training loss: 1.5115...  0.4577 sec/batch\n",
      "Epoch: 29/100...  Training Step: 962...  Training loss: 1.5540...  0.4536 sec/batch\n",
      "Epoch: 29/100...  Training Step: 963...  Training loss: 1.5516...  0.4511 sec/batch\n",
      "Epoch: 29/100...  Training Step: 964...  Training loss: 1.5582...  0.4515 sec/batch\n",
      "Epoch: 29/100...  Training Step: 965...  Training loss: 1.5713...  0.4566 sec/batch\n",
      "Epoch: 29/100...  Training Step: 966...  Training loss: 1.5501...  0.4553 sec/batch\n",
      "Epoch: 29/100...  Training Step: 967...  Training loss: 1.5216...  0.4481 sec/batch\n",
      "Epoch: 29/100...  Training Step: 968...  Training loss: 1.5651...  0.4562 sec/batch\n",
      "Epoch: 29/100...  Training Step: 969...  Training loss: 1.5676...  0.4315 sec/batch\n",
      "Epoch: 29/100...  Training Step: 970...  Training loss: 1.5339...  0.4632 sec/batch\n",
      "Epoch: 29/100...  Training Step: 971...  Training loss: 1.5559...  0.4618 sec/batch\n",
      "Epoch: 29/100...  Training Step: 972...  Training loss: 1.5174...  0.4507 sec/batch\n",
      "Epoch: 29/100...  Training Step: 973...  Training loss: 1.5497...  0.4505 sec/batch\n",
      "Epoch: 29/100...  Training Step: 974...  Training loss: 1.5048...  0.4564 sec/batch\n",
      "Epoch: 29/100...  Training Step: 975...  Training loss: 1.5243...  0.4517 sec/batch\n",
      "Epoch: 29/100...  Training Step: 976...  Training loss: 1.5281...  0.4546 sec/batch\n",
      "Epoch: 29/100...  Training Step: 977...  Training loss: 1.5050...  0.4574 sec/batch\n",
      "Epoch: 29/100...  Training Step: 978...  Training loss: 1.5152...  0.4534 sec/batch\n",
      "Epoch: 29/100...  Training Step: 979...  Training loss: 1.5124...  0.4471 sec/batch\n",
      "Epoch: 29/100...  Training Step: 980...  Training loss: 1.5078...  0.4517 sec/batch\n",
      "Epoch: 29/100...  Training Step: 981...  Training loss: 1.5552...  0.4548 sec/batch\n",
      "Epoch: 29/100...  Training Step: 982...  Training loss: 1.5497...  0.4416 sec/batch\n",
      "Epoch: 29/100...  Training Step: 983...  Training loss: 1.5394...  0.4587 sec/batch\n",
      "Epoch: 29/100...  Training Step: 984...  Training loss: 1.5601...  0.4397 sec/batch\n",
      "Epoch: 29/100...  Training Step: 985...  Training loss: 1.5627...  0.4537 sec/batch\n",
      "Epoch: 29/100...  Training Step: 986...  Training loss: 1.5240...  0.4504 sec/batch\n",
      "Epoch: 30/100...  Training Step: 987...  Training loss: 1.6041...  0.4533 sec/batch\n",
      "Epoch: 30/100...  Training Step: 988...  Training loss: 1.5211...  0.4544 sec/batch\n",
      "Epoch: 30/100...  Training Step: 989...  Training loss: 1.5236...  0.4368 sec/batch\n",
      "Epoch: 30/100...  Training Step: 990...  Training loss: 1.5556...  0.4504 sec/batch\n",
      "Epoch: 30/100...  Training Step: 991...  Training loss: 1.5440...  0.4474 sec/batch\n",
      "Epoch: 30/100...  Training Step: 992...  Training loss: 1.5192...  0.4378 sec/batch\n",
      "Epoch: 30/100...  Training Step: 993...  Training loss: 1.5350...  0.4546 sec/batch\n",
      "Epoch: 30/100...  Training Step: 994...  Training loss: 1.5039...  0.4552 sec/batch\n",
      "Epoch: 30/100...  Training Step: 995...  Training loss: 1.4910...  0.4533 sec/batch\n",
      "Epoch: 30/100...  Training Step: 996...  Training loss: 1.5361...  0.4535 sec/batch\n",
      "Epoch: 30/100...  Training Step: 997...  Training loss: 1.5301...  0.4464 sec/batch\n",
      "Epoch: 30/100...  Training Step: 998...  Training loss: 1.5303...  0.4563 sec/batch\n",
      "Epoch: 30/100...  Training Step: 999...  Training loss: 1.5481...  0.4541 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1000...  Training loss: 1.5305...  0.4519 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1001...  Training loss: 1.5019...  0.4562 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1002...  Training loss: 1.5427...  0.4401 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1003...  Training loss: 1.5481...  0.4600 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1004...  Training loss: 1.5169...  0.4491 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1005...  Training loss: 1.5173...  0.4527 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1006...  Training loss: 1.4900...  0.4609 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1007...  Training loss: 1.5218...  0.4529 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1008...  Training loss: 1.4885...  0.4518 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1009...  Training loss: 1.5063...  0.4580 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1010...  Training loss: 1.5046...  0.4520 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1011...  Training loss: 1.4778...  0.4473 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1012...  Training loss: 1.4952...  0.4593 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1013...  Training loss: 1.4875...  0.4418 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1014...  Training loss: 1.4917...  0.4490 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1015...  Training loss: 1.5376...  0.4373 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1016...  Training loss: 1.5299...  0.4532 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1017...  Training loss: 1.5142...  0.4478 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1018...  Training loss: 1.5412...  0.4571 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1019...  Training loss: 1.5464...  0.4551 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1020...  Training loss: 1.5104...  0.4521 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1021...  Training loss: 1.5758...  0.4524 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1022...  Training loss: 1.4970...  0.4564 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1023...  Training loss: 1.5108...  0.4536 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1024...  Training loss: 1.5422...  0.4571 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1025...  Training loss: 1.5232...  0.4550 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1026...  Training loss: 1.5036...  0.4530 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1027...  Training loss: 1.5204...  0.4512 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1028...  Training loss: 1.4814...  0.4316 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1029...  Training loss: 1.4731...  0.4557 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1030...  Training loss: 1.5117...  0.4609 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1031...  Training loss: 1.5232...  0.4504 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1032...  Training loss: 1.5226...  0.4508 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1033...  Training loss: 1.5147...  0.4542 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1034...  Training loss: 1.5070...  0.4559 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1035...  Training loss: 1.4837...  0.4538 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1036...  Training loss: 1.5249...  0.4551 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1037...  Training loss: 1.5240...  0.4450 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1038...  Training loss: 1.4972...  0.4574 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1039...  Training loss: 1.5037...  0.4512 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1040...  Training loss: 1.4774...  0.4523 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1041...  Training loss: 1.5101...  0.4480 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1042...  Training loss: 1.4716...  0.4463 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1043...  Training loss: 1.4894...  0.4615 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1044...  Training loss: 1.4909...  0.4604 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1045...  Training loss: 1.4746...  0.4560 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1046...  Training loss: 1.4719...  0.4519 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1047...  Training loss: 1.4738...  0.4605 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1048...  Training loss: 1.4803...  0.4491 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1049...  Training loss: 1.5277...  0.4610 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1050...  Training loss: 1.5184...  0.4530 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1051...  Training loss: 1.4962...  0.4334 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1052...  Training loss: 1.5175...  0.4425 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1053...  Training loss: 1.5210...  0.4544 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1054...  Training loss: 1.4966...  0.4596 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1055...  Training loss: 1.5705...  0.4358 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1056...  Training loss: 1.4766...  0.4524 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1057...  Training loss: 1.4923...  0.4571 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1058...  Training loss: 1.5291...  0.4455 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1059...  Training loss: 1.5110...  0.4546 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1060...  Training loss: 1.4937...  0.4538 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1061...  Training loss: 1.5003...  0.4456 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1062...  Training loss: 1.4680...  0.4455 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1063...  Training loss: 1.4583...  0.4528 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1064...  Training loss: 1.5026...  0.4557 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1065...  Training loss: 1.5030...  0.4564 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1066...  Training loss: 1.5003...  0.4518 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1067...  Training loss: 1.4997...  0.4498 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1068...  Training loss: 1.5013...  0.4501 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1069...  Training loss: 1.4695...  0.4472 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1070...  Training loss: 1.5059...  0.4511 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1071...  Training loss: 1.5228...  0.4418 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1072...  Training loss: 1.4871...  0.4516 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1073...  Training loss: 1.4878...  0.4484 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1074...  Training loss: 1.4676...  0.4498 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1075...  Training loss: 1.4901...  0.4616 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1076...  Training loss: 1.4567...  0.4531 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1077...  Training loss: 1.4806...  0.4627 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1078...  Training loss: 1.4726...  0.4546 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1079...  Training loss: 1.4584...  0.4576 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1080...  Training loss: 1.4600...  0.4526 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1081...  Training loss: 1.4577...  0.4521 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1082...  Training loss: 1.4670...  0.4526 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1083...  Training loss: 1.5079...  0.4513 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1084...  Training loss: 1.5055...  0.4530 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1085...  Training loss: 1.4803...  0.4491 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1086...  Training loss: 1.5091...  0.4542 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1087...  Training loss: 1.5101...  0.4495 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1088...  Training loss: 1.4776...  0.4419 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1089...  Training loss: 1.5436...  0.4553 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1090...  Training loss: 1.4651...  0.4439 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1091...  Training loss: 1.4718...  0.4544 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1092...  Training loss: 1.5126...  0.4582 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1093...  Training loss: 1.5031...  0.4568 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1094...  Training loss: 1.4754...  0.4570 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1095...  Training loss: 1.4842...  0.4615 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1096...  Training loss: 1.4459...  0.4554 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1097...  Training loss: 1.4379...  0.4577 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1098...  Training loss: 1.4813...  0.4469 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1099...  Training loss: 1.4851...  0.4609 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1100...  Training loss: 1.4862...  0.4523 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1101...  Training loss: 1.4903...  0.4558 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1102...  Training loss: 1.4830...  0.4553 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1103...  Training loss: 1.4581...  0.4401 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1104...  Training loss: 1.4942...  0.4544 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1105...  Training loss: 1.5044...  0.4570 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1106...  Training loss: 1.4692...  0.4532 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1107...  Training loss: 1.4742...  0.4583 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1108...  Training loss: 1.4579...  0.4638 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1109...  Training loss: 1.4788...  0.4591 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1110...  Training loss: 1.4322...  0.4525 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1111...  Training loss: 1.4572...  0.4309 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1112...  Training loss: 1.4618...  0.4552 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1113...  Training loss: 1.4425...  0.4509 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1114...  Training loss: 1.4523...  0.4602 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1115...  Training loss: 1.4404...  0.4450 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1116...  Training loss: 1.4410...  0.4434 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1117...  Training loss: 1.4761...  0.4589 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1118...  Training loss: 1.4930...  0.4403 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1119...  Training loss: 1.4723...  0.4536 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1120...  Training loss: 1.4954...  0.4537 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1121...  Training loss: 1.4966...  0.4574 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1122...  Training loss: 1.4650...  0.4511 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1123...  Training loss: 1.5331...  0.4549 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1124...  Training loss: 1.4506...  0.4484 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1125...  Training loss: 1.4641...  0.4523 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1126...  Training loss: 1.5008...  0.4618 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1127...  Training loss: 1.4705...  0.4587 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1128...  Training loss: 1.4523...  0.4483 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1129...  Training loss: 1.4699...  0.4612 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1130...  Training loss: 1.4357...  0.4519 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1131...  Training loss: 1.4297...  0.4577 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1132...  Training loss: 1.4645...  0.4482 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1133...  Training loss: 1.4751...  0.4562 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1134...  Training loss: 1.4647...  0.4577 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1135...  Training loss: 1.4670...  0.4542 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1136...  Training loss: 1.4748...  0.4527 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1137...  Training loss: 1.4403...  0.4634 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1138...  Training loss: 1.4648...  0.4657 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1139...  Training loss: 1.4919...  0.4545 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1140...  Training loss: 1.4542...  0.4533 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1141...  Training loss: 1.4463...  0.4454 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1142...  Training loss: 1.4349...  0.4622 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1143...  Training loss: 1.4596...  0.4468 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1144...  Training loss: 1.4272...  0.4603 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1145...  Training loss: 1.4527...  0.4593 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1146...  Training loss: 1.4541...  0.4493 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1147...  Training loss: 1.4367...  0.4487 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1148...  Training loss: 1.4330...  0.4587 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1149...  Training loss: 1.4211...  0.4442 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1150...  Training loss: 1.4341...  0.4539 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1151...  Training loss: 1.4718...  0.4500 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1152...  Training loss: 1.4774...  0.4513 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1153...  Training loss: 1.4536...  0.4579 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1154...  Training loss: 1.4738...  0.4615 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1155...  Training loss: 1.4858...  0.4415 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1156...  Training loss: 1.4506...  0.4522 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1157...  Training loss: 1.5176...  0.4519 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1158...  Training loss: 1.4322...  0.4557 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1159...  Training loss: 1.4410...  0.4556 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1160...  Training loss: 1.4803...  0.4610 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1161...  Training loss: 1.4608...  0.4517 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1162...  Training loss: 1.4454...  0.4452 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1163...  Training loss: 1.4640...  0.4584 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1164...  Training loss: 1.4269...  0.4527 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1165...  Training loss: 1.4155...  0.4585 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1166...  Training loss: 1.4608...  0.4597 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1167...  Training loss: 1.4584...  0.4519 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1168...  Training loss: 1.4651...  0.4590 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1169...  Training loss: 1.4539...  0.4619 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1170...  Training loss: 1.4608...  0.4520 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1171...  Training loss: 1.4281...  0.4535 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1172...  Training loss: 1.4652...  0.4524 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1173...  Training loss: 1.4788...  0.4518 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1174...  Training loss: 1.4364...  0.4531 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1175...  Training loss: 1.4416...  0.4543 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1176...  Training loss: 1.4212...  0.4598 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1177...  Training loss: 1.4435...  0.4480 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1178...  Training loss: 1.4122...  0.4561 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1179...  Training loss: 1.4313...  0.4547 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1180...  Training loss: 1.4360...  0.4530 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1181...  Training loss: 1.4246...  0.4560 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1182...  Training loss: 1.4199...  0.4596 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1183...  Training loss: 1.4148...  0.4553 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1184...  Training loss: 1.4114...  0.4484 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1185...  Training loss: 1.4559...  0.4506 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1186...  Training loss: 1.4651...  0.4565 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1187...  Training loss: 1.4324...  0.4654 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1188...  Training loss: 1.4679...  0.4410 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1189...  Training loss: 1.4661...  0.4492 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1190...  Training loss: 1.4370...  0.4453 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1191...  Training loss: 1.5177...  0.4390 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1192...  Training loss: 1.4239...  0.4463 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1193...  Training loss: 1.4406...  0.4575 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1194...  Training loss: 1.4711...  0.4513 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1195...  Training loss: 1.4610...  0.4602 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1196...  Training loss: 1.4294...  0.4596 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1197...  Training loss: 1.4458...  0.4515 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1198...  Training loss: 1.4118...  0.4549 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1199...  Training loss: 1.4053...  0.4657 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1200...  Training loss: 1.4392...  0.4522 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1201...  Training loss: 1.4487...  0.4597 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1202...  Training loss: 1.4441...  0.4587 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1203...  Training loss: 1.4471...  0.4537 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1204...  Training loss: 1.4499...  0.4532 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1205...  Training loss: 1.4182...  0.4524 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1206...  Training loss: 1.4470...  0.4514 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1207...  Training loss: 1.4544...  0.4559 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1208...  Training loss: 1.4295...  0.4621 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1209...  Training loss: 1.4336...  0.4606 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1210...  Training loss: 1.3941...  0.4691 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1211...  Training loss: 1.4324...  0.4554 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1212...  Training loss: 1.4028...  0.4604 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1213...  Training loss: 1.4288...  0.4492 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1214...  Training loss: 1.4200...  0.4532 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1215...  Training loss: 1.4009...  0.4575 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1216...  Training loss: 1.4039...  0.4434 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1217...  Training loss: 1.3954...  0.4561 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1218...  Training loss: 1.4032...  0.4554 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1219...  Training loss: 1.4446...  0.4573 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1220...  Training loss: 1.4592...  0.4558 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1221...  Training loss: 1.4230...  0.4489 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1222...  Training loss: 1.4464...  0.4598 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1223...  Training loss: 1.4508...  0.4473 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1224...  Training loss: 1.4174...  0.4523 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1225...  Training loss: 1.4901...  0.4501 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1226...  Training loss: 1.4080...  0.4594 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1227...  Training loss: 1.4289...  0.4600 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1228...  Training loss: 1.4662...  0.4547 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1229...  Training loss: 1.4458...  0.4577 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1230...  Training loss: 1.4153...  0.4546 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1231...  Training loss: 1.4321...  0.4507 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1232...  Training loss: 1.3949...  0.4638 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1233...  Training loss: 1.3885...  0.4563 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1234...  Training loss: 1.4267...  0.4666 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1235...  Training loss: 1.4440...  0.4533 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1236...  Training loss: 1.4302...  0.4633 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1237...  Training loss: 1.4354...  0.4564 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1238...  Training loss: 1.4350...  0.4625 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1239...  Training loss: 1.4025...  0.4571 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1240...  Training loss: 1.4340...  0.4493 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1241...  Training loss: 1.4370...  0.4562 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1242...  Training loss: 1.4187...  0.4566 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1243...  Training loss: 1.4206...  0.4539 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1244...  Training loss: 1.4021...  0.4543 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1245...  Training loss: 1.4278...  0.4524 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1246...  Training loss: 1.3898...  0.4542 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1247...  Training loss: 1.4052...  0.4589 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1248...  Training loss: 1.4092...  0.4525 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1249...  Training loss: 1.3899...  0.4372 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1250...  Training loss: 1.4014...  0.4533 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1251...  Training loss: 1.3849...  0.4492 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1252...  Training loss: 1.3923...  0.4582 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1253...  Training loss: 1.4256...  0.4609 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1254...  Training loss: 1.4267...  0.4582 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1255...  Training loss: 1.4130...  0.4506 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1256...  Training loss: 1.4346...  0.4570 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1257...  Training loss: 1.4390...  0.4549 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1258...  Training loss: 1.4177...  0.4536 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1259...  Training loss: 1.4851...  0.4521 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1260...  Training loss: 1.3977...  0.4565 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1261...  Training loss: 1.4109...  0.4549 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1262...  Training loss: 1.4460...  0.4611 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1263...  Training loss: 1.4339...  0.4543 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1264...  Training loss: 1.4045...  0.4506 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1265...  Training loss: 1.4219...  0.4564 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1266...  Training loss: 1.3961...  0.4540 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1267...  Training loss: 1.3749...  0.4568 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1268...  Training loss: 1.4160...  0.4533 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1269...  Training loss: 1.4188...  0.4489 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1270...  Training loss: 1.4183...  0.4526 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1271...  Training loss: 1.4238...  0.4471 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1272...  Training loss: 1.4155...  0.4455 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1273...  Training loss: 1.3901...  0.4531 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1274...  Training loss: 1.4263...  0.4532 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1275...  Training loss: 1.4378...  0.4553 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1276...  Training loss: 1.4017...  0.4547 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1277...  Training loss: 1.4094...  0.4589 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1278...  Training loss: 1.3905...  0.4411 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1279...  Training loss: 1.4106...  0.4545 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1280...  Training loss: 1.3738...  0.4344 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1281...  Training loss: 1.4057...  0.4523 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1282...  Training loss: 1.3982...  0.4503 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1283...  Training loss: 1.3765...  0.4575 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1284...  Training loss: 1.3825...  0.4446 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1285...  Training loss: 1.3793...  0.4579 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1286...  Training loss: 1.3784...  0.4553 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1287...  Training loss: 1.4183...  0.4568 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1288...  Training loss: 1.4180...  0.4591 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1289...  Training loss: 1.3907...  0.4583 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1290...  Training loss: 1.4255...  0.4511 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1291...  Training loss: 1.4297...  0.4551 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1292...  Training loss: 1.3971...  0.4549 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1293...  Training loss: 1.4726...  0.4564 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1294...  Training loss: 1.3877...  0.4538 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1295...  Training loss: 1.3928...  0.4510 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1296...  Training loss: 1.4354...  0.4511 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1297...  Training loss: 1.4233...  0.4520 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1298...  Training loss: 1.3795...  0.4571 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1299...  Training loss: 1.4079...  0.4451 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1300...  Training loss: 1.3769...  0.4491 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1301...  Training loss: 1.3653...  0.4533 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1302...  Training loss: 1.4073...  0.4587 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1303...  Training loss: 1.4064...  0.4522 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1304...  Training loss: 1.4101...  0.4512 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1305...  Training loss: 1.4064...  0.4527 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1306...  Training loss: 1.3999...  0.4522 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1307...  Training loss: 1.3659...  0.4592 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1308...  Training loss: 1.4124...  0.4408 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1309...  Training loss: 1.4261...  0.4543 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1310...  Training loss: 1.3880...  0.4494 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1311...  Training loss: 1.3924...  0.4488 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1312...  Training loss: 1.3795...  0.4455 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1313...  Training loss: 1.3975...  0.4564 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1314...  Training loss: 1.3629...  0.4556 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1315...  Training loss: 1.3921...  0.4573 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1316...  Training loss: 1.3868...  0.4636 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1317...  Training loss: 1.3656...  0.4472 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1318...  Training loss: 1.3801...  0.4604 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1319...  Training loss: 1.3628...  0.4525 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1320...  Training loss: 1.3669...  0.4499 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1321...  Training loss: 1.4031...  0.4550 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1322...  Training loss: 1.4128...  0.4549 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1323...  Training loss: 1.3842...  0.4544 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1324...  Training loss: 1.4113...  0.4546 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1325...  Training loss: 1.4196...  0.4397 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1326...  Training loss: 1.3835...  0.4564 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1327...  Training loss: 1.4511...  0.4473 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1328...  Training loss: 1.3725...  0.4578 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1329...  Training loss: 1.3802...  0.4656 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1330...  Training loss: 1.4224...  0.4571 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1331...  Training loss: 1.4069...  0.4550 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1332...  Training loss: 1.3787...  0.4562 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1333...  Training loss: 1.3944...  0.4497 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1334...  Training loss: 1.3698...  0.4580 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1335...  Training loss: 1.3581...  0.4589 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1336...  Training loss: 1.3919...  0.4480 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1337...  Training loss: 1.3968...  0.4553 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1338...  Training loss: 1.3925...  0.4518 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1339...  Training loss: 1.3891...  0.4487 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1340...  Training loss: 1.3926...  0.4540 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1341...  Training loss: 1.3578...  0.4502 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1342...  Training loss: 1.3997...  0.4519 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1343...  Training loss: 1.4093...  0.4502 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1344...  Training loss: 1.3855...  0.4542 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1345...  Training loss: 1.3793...  0.4522 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1346...  Training loss: 1.3664...  0.4543 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1347...  Training loss: 1.3830...  0.4486 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1348...  Training loss: 1.3541...  0.4529 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1349...  Training loss: 1.3828...  0.4541 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1350...  Training loss: 1.3693...  0.4458 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1351...  Training loss: 1.3561...  0.4512 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1352...  Training loss: 1.3676...  0.4554 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1353...  Training loss: 1.3592...  0.4589 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1354...  Training loss: 1.3643...  0.4533 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1355...  Training loss: 1.3920...  0.4544 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1356...  Training loss: 1.3923...  0.4602 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1357...  Training loss: 1.3676...  0.4471 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1358...  Training loss: 1.4118...  0.4544 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1359...  Training loss: 1.4040...  0.4538 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1360...  Training loss: 1.3758...  0.4550 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1361...  Training loss: 1.4385...  0.4568 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1362...  Training loss: 1.3615...  0.4588 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1363...  Training loss: 1.3724...  0.4626 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1364...  Training loss: 1.4127...  0.4591 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1365...  Training loss: 1.3922...  0.4576 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1366...  Training loss: 1.3604...  0.4564 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1367...  Training loss: 1.3883...  0.4637 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1368...  Training loss: 1.3573...  0.4452 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1369...  Training loss: 1.3397...  0.4605 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1370...  Training loss: 1.3783...  0.4564 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1371...  Training loss: 1.3816...  0.4540 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1372...  Training loss: 1.3862...  0.4479 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1373...  Training loss: 1.3903...  0.4504 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1374...  Training loss: 1.3820...  0.4541 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1375...  Training loss: 1.3556...  0.4553 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1376...  Training loss: 1.3888...  0.4529 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1377...  Training loss: 1.4023...  0.4586 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1378...  Training loss: 1.3791...  0.4555 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1379...  Training loss: 1.3650...  0.4484 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1380...  Training loss: 1.3498...  0.4500 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1381...  Training loss: 1.3684...  0.4530 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1382...  Training loss: 1.3384...  0.4456 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1383...  Training loss: 1.3636...  0.4540 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1384...  Training loss: 1.3613...  0.4567 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1385...  Training loss: 1.3409...  0.4583 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1386...  Training loss: 1.3578...  0.4566 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1387...  Training loss: 1.3326...  0.4477 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1388...  Training loss: 1.3493...  0.4550 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1389...  Training loss: 1.3769...  0.4533 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1390...  Training loss: 1.3873...  0.4495 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1391...  Training loss: 1.3535...  0.4557 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1392...  Training loss: 1.3912...  0.4559 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1393...  Training loss: 1.3892...  0.4514 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1394...  Training loss: 1.3650...  0.4555 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1395...  Training loss: 1.4368...  0.4566 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1396...  Training loss: 1.3494...  0.4553 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1397...  Training loss: 1.3586...  0.4581 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1398...  Training loss: 1.3948...  0.4541 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1399...  Training loss: 1.3797...  0.4389 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1400...  Training loss: 1.3477...  0.4565 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1401...  Training loss: 1.3636...  0.4588 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1402...  Training loss: 1.3355...  0.4500 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1403...  Training loss: 1.3383...  0.4525 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1404...  Training loss: 1.3707...  0.4573 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1405...  Training loss: 1.3684...  0.4564 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1406...  Training loss: 1.3771...  0.4523 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1407...  Training loss: 1.3783...  0.4573 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1408...  Training loss: 1.3688...  0.4597 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1409...  Training loss: 1.3485...  0.4512 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1410...  Training loss: 1.3750...  0.4594 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1411...  Training loss: 1.3861...  0.4582 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1412...  Training loss: 1.3601...  0.4516 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1413...  Training loss: 1.3630...  0.4448 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1414...  Training loss: 1.3475...  0.4551 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1415...  Training loss: 1.3644...  0.4575 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1416...  Training loss: 1.3400...  0.4566 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1417...  Training loss: 1.3552...  0.4569 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1418...  Training loss: 1.3470...  0.4461 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1419...  Training loss: 1.3441...  0.4517 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1420...  Training loss: 1.3466...  0.4565 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1421...  Training loss: 1.3380...  0.4634 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1422...  Training loss: 1.3354...  0.4611 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1423...  Training loss: 1.3700...  0.4573 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1424...  Training loss: 1.3781...  0.4521 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1425...  Training loss: 1.3504...  0.4563 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1426...  Training loss: 1.3830...  0.4495 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1427...  Training loss: 1.3809...  0.4536 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1428...  Training loss: 1.3567...  0.4453 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1429...  Training loss: 1.4174...  0.4527 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1430...  Training loss: 1.3468...  0.4568 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1431...  Training loss: 1.3498...  0.4522 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1432...  Training loss: 1.3939...  0.4486 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1433...  Training loss: 1.3712...  0.4693 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1434...  Training loss: 1.3362...  0.4641 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1435...  Training loss: 1.3660...  0.4542 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1436...  Training loss: 1.3331...  0.4495 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1437...  Training loss: 1.3200...  0.4522 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1438...  Training loss: 1.3576...  0.4515 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1439...  Training loss: 1.3673...  0.4656 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1440...  Training loss: 1.3700...  0.4600 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1441...  Training loss: 1.3691...  0.4584 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1442...  Training loss: 1.3586...  0.4490 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1443...  Training loss: 1.3344...  0.4479 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1444...  Training loss: 1.3730...  0.4540 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1445...  Training loss: 1.3772...  0.4485 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1446...  Training loss: 1.3586...  0.4499 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1447...  Training loss: 1.3488...  0.4533 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1448...  Training loss: 1.3300...  0.4548 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1449...  Training loss: 1.3615...  0.4530 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1450...  Training loss: 1.3243...  0.4483 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1451...  Training loss: 1.3426...  0.4509 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1452...  Training loss: 1.3386...  0.4541 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1453...  Training loss: 1.3297...  0.4567 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1454...  Training loss: 1.3314...  0.4533 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1455...  Training loss: 1.3257...  0.4581 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1456...  Training loss: 1.3159...  0.4466 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1457...  Training loss: 1.3591...  0.4560 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1458...  Training loss: 1.3620...  0.4533 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1459...  Training loss: 1.3250...  0.4516 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1460...  Training loss: 1.3694...  0.4583 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1461...  Training loss: 1.3779...  0.4564 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1462...  Training loss: 1.3483...  0.4564 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1463...  Training loss: 1.4230...  0.4568 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1464...  Training loss: 1.3354...  0.4546 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1465...  Training loss: 1.3409...  0.4577 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1466...  Training loss: 1.3728...  0.4543 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1467...  Training loss: 1.3542...  0.4561 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1468...  Training loss: 1.3284...  0.4518 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1469...  Training loss: 1.3551...  0.4533 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1470...  Training loss: 1.3274...  0.4566 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1471...  Training loss: 1.3124...  0.4529 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1472...  Training loss: 1.3508...  0.4636 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1473...  Training loss: 1.3581...  0.4550 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1474...  Training loss: 1.3512...  0.4531 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1475...  Training loss: 1.3596...  0.4615 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1476...  Training loss: 1.3488...  0.4507 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1477...  Training loss: 1.3125...  0.4556 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1478...  Training loss: 1.3521...  0.4496 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1479...  Training loss: 1.3658...  0.4552 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1480...  Training loss: 1.3513...  0.4588 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1481...  Training loss: 1.3407...  0.4587 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1482...  Training loss: 1.3302...  0.4532 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1483...  Training loss: 1.3418...  0.4571 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1484...  Training loss: 1.3171...  0.4445 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1485...  Training loss: 1.3356...  0.4585 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1486...  Training loss: 1.3262...  0.4581 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1487...  Training loss: 1.3212...  0.4603 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1488...  Training loss: 1.3294...  0.4492 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1489...  Training loss: 1.3153...  0.4556 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1490...  Training loss: 1.3198...  0.4528 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1491...  Training loss: 1.3421...  0.4566 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1492...  Training loss: 1.3543...  0.4436 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1493...  Training loss: 1.3278...  0.4530 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1494...  Training loss: 1.3630...  0.4459 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1495...  Training loss: 1.3622...  0.4525 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1496...  Training loss: 1.3468...  0.4544 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1497...  Training loss: 1.4001...  0.4604 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1498...  Training loss: 1.3184...  0.4485 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1499...  Training loss: 1.3265...  0.4611 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1500...  Training loss: 1.3677...  0.4617 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1501...  Training loss: 1.3534...  0.4509 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1502...  Training loss: 1.3259...  0.4418 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1503...  Training loss: 1.3492...  0.4553 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1504...  Training loss: 1.3148...  0.4526 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1505...  Training loss: 1.2970...  0.4533 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1506...  Training loss: 1.3333...  0.4575 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1507...  Training loss: 1.3487...  0.4550 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1508...  Training loss: 1.3367...  0.4583 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1509...  Training loss: 1.3406...  0.4582 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1510...  Training loss: 1.3388...  0.4589 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1511...  Training loss: 1.3097...  0.4565 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1512...  Training loss: 1.3513...  0.4608 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1513...  Training loss: 1.3623...  0.4595 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1514...  Training loss: 1.3292...  0.4556 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1515...  Training loss: 1.3311...  0.4558 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1516...  Training loss: 1.3121...  0.4576 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1517...  Training loss: 1.3350...  0.4488 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1518...  Training loss: 1.3021...  0.4488 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1519...  Training loss: 1.3265...  0.4547 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1520...  Training loss: 1.3073...  0.4360 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1521...  Training loss: 1.3129...  0.4560 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1522...  Training loss: 1.3171...  0.4628 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1523...  Training loss: 1.3001...  0.4559 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1524...  Training loss: 1.3057...  0.4524 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1525...  Training loss: 1.3417...  0.4634 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1526...  Training loss: 1.3434...  0.4596 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1527...  Training loss: 1.3090...  0.4385 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1528...  Training loss: 1.3501...  0.4510 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1529...  Training loss: 1.3478...  0.4595 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1530...  Training loss: 1.3207...  0.4507 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1531...  Training loss: 1.3927...  0.4585 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1532...  Training loss: 1.3182...  0.4588 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1533...  Training loss: 1.3189...  0.4568 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1534...  Training loss: 1.3641...  0.4371 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1535...  Training loss: 1.3349...  0.4525 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1536...  Training loss: 1.3139...  0.4627 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1537...  Training loss: 1.3294...  0.4501 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1538...  Training loss: 1.2989...  0.4528 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1539...  Training loss: 1.2987...  0.4563 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1540...  Training loss: 1.3225...  0.4633 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1541...  Training loss: 1.3361...  0.4467 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1542...  Training loss: 1.3287...  0.4541 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1543...  Training loss: 1.3377...  0.4580 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1544...  Training loss: 1.3319...  0.4571 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1545...  Training loss: 1.3050...  0.4592 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1546...  Training loss: 1.3302...  0.4570 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1547...  Training loss: 1.3495...  0.4609 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1548...  Training loss: 1.3247...  0.4674 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1549...  Training loss: 1.3217...  0.4660 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1550...  Training loss: 1.3013...  0.4449 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1551...  Training loss: 1.3254...  0.4525 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1552...  Training loss: 1.2914...  0.4459 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1553...  Training loss: 1.3184...  0.4625 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1554...  Training loss: 1.3045...  0.4577 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1555...  Training loss: 1.3097...  0.4520 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1556...  Training loss: 1.3093...  0.4593 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1557...  Training loss: 1.2944...  0.4509 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1558...  Training loss: 1.2949...  0.4641 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1559...  Training loss: 1.3280...  0.4504 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1560...  Training loss: 1.3332...  0.4610 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1561...  Training loss: 1.3010...  0.4563 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1562...  Training loss: 1.3472...  0.4620 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1563...  Training loss: 1.3422...  0.4485 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1564...  Training loss: 1.3105...  0.4610 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1565...  Training loss: 1.3828...  0.4537 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1566...  Training loss: 1.2992...  0.4598 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1567...  Training loss: 1.3185...  0.4590 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1568...  Training loss: 1.3422...  0.4544 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1569...  Training loss: 1.3213...  0.4519 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1570...  Training loss: 1.3029...  0.4601 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1571...  Training loss: 1.3252...  0.4585 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1572...  Training loss: 1.2992...  0.4508 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1573...  Training loss: 1.2809...  0.4558 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1574...  Training loss: 1.3238...  0.4659 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1575...  Training loss: 1.3283...  0.4587 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1576...  Training loss: 1.3200...  0.4547 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1577...  Training loss: 1.3271...  0.4581 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1578...  Training loss: 1.3212...  0.4589 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1579...  Training loss: 1.2930...  0.4559 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1580...  Training loss: 1.3289...  0.4524 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1581...  Training loss: 1.3486...  0.4516 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1582...  Training loss: 1.3189...  0.4591 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1583...  Training loss: 1.3111...  0.4446 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1584...  Training loss: 1.3007...  0.4539 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1585...  Training loss: 1.3176...  0.4577 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1586...  Training loss: 1.2774...  0.4605 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1587...  Training loss: 1.3095...  0.4544 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1588...  Training loss: 1.2876...  0.4587 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1589...  Training loss: 1.2864...  0.4569 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1590...  Training loss: 1.3037...  0.4520 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1591...  Training loss: 1.2847...  0.4527 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1592...  Training loss: 1.2951...  0.4542 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1593...  Training loss: 1.3168...  0.4605 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1594...  Training loss: 1.3204...  0.4560 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1595...  Training loss: 1.2965...  0.4592 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1596...  Training loss: 1.3277...  0.4616 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1597...  Training loss: 1.3247...  0.4516 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1598...  Training loss: 1.3009...  0.4525 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1599...  Training loss: 1.3749...  0.4566 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1600...  Training loss: 1.2989...  0.4514 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1601...  Training loss: 1.3030...  0.4580 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1602...  Training loss: 1.3350...  0.4591 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1603...  Training loss: 1.3179...  0.4374 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1604...  Training loss: 1.2892...  0.4508 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1605...  Training loss: 1.3188...  0.4535 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1606...  Training loss: 1.2762...  0.4404 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1607...  Training loss: 1.2706...  0.4563 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1608...  Training loss: 1.3087...  0.4602 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1609...  Training loss: 1.3126...  0.4556 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1610...  Training loss: 1.3135...  0.4588 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1611...  Training loss: 1.3198...  0.4599 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1612...  Training loss: 1.3182...  0.4582 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1613...  Training loss: 1.2896...  0.4588 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1614...  Training loss: 1.3094...  0.4563 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1615...  Training loss: 1.3306...  0.4550 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1616...  Training loss: 1.2975...  0.4563 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1617...  Training loss: 1.2986...  0.4532 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1618...  Training loss: 1.2748...  0.4538 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1619...  Training loss: 1.3079...  0.4537 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1620...  Training loss: 1.2721...  0.4546 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1621...  Training loss: 1.2933...  0.4574 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1622...  Training loss: 1.2970...  0.4544 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1623...  Training loss: 1.2803...  0.4607 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1624...  Training loss: 1.2830...  0.4543 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1625...  Training loss: 1.2733...  0.4557 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1626...  Training loss: 1.2804...  0.4553 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1627...  Training loss: 1.3031...  0.4497 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1628...  Training loss: 1.3087...  0.4552 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1629...  Training loss: 1.2890...  0.4514 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1630...  Training loss: 1.3159...  0.4519 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1631...  Training loss: 1.3215...  0.4562 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1632...  Training loss: 1.2839...  0.4568 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1633...  Training loss: 1.3639...  0.4543 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1634...  Training loss: 1.2906...  0.4517 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1635...  Training loss: 1.2972...  0.4429 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1636...  Training loss: 1.3321...  0.4531 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1637...  Training loss: 1.3078...  0.4593 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1638...  Training loss: 1.2834...  0.4665 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1639...  Training loss: 1.3143...  0.4424 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1640...  Training loss: 1.2816...  0.4557 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1641...  Training loss: 1.2567...  0.4583 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1642...  Training loss: 1.2999...  0.4589 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1643...  Training loss: 1.3073...  0.4577 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1644...  Training loss: 1.3066...  0.4621 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1645...  Training loss: 1.3025...  0.4565 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1646...  Training loss: 1.2932...  0.4535 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1647...  Training loss: 1.2741...  0.4556 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1648...  Training loss: 1.3093...  0.4551 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1649...  Training loss: 1.3132...  0.4496 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1650...  Training loss: 1.2949...  0.4600 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1651...  Training loss: 1.2947...  0.4556 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1652...  Training loss: 1.2776...  0.4498 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1653...  Training loss: 1.2925...  0.4493 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1654...  Training loss: 1.2633...  0.4543 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1655...  Training loss: 1.2956...  0.4577 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1656...  Training loss: 1.2688...  0.4426 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1657...  Training loss: 1.2756...  0.4457 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1658...  Training loss: 1.2824...  0.4553 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1659...  Training loss: 1.2729...  0.4502 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1660...  Training loss: 1.2646...  0.4521 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1661...  Training loss: 1.2968...  0.4510 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1662...  Training loss: 1.3016...  0.4523 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1663...  Training loss: 1.2743...  0.4581 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1664...  Training loss: 1.3013...  0.4539 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1665...  Training loss: 1.3039...  0.4554 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1666...  Training loss: 1.2879...  0.4476 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1667...  Training loss: 1.3618...  0.4545 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1668...  Training loss: 1.2797...  0.4584 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1669...  Training loss: 1.2761...  0.4548 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1670...  Training loss: 1.3165...  0.4561 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1671...  Training loss: 1.3108...  0.4528 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1672...  Training loss: 1.2710...  0.4581 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1673...  Training loss: 1.2978...  0.4536 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1674...  Training loss: 1.2762...  0.4597 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1675...  Training loss: 1.2576...  0.4503 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1676...  Training loss: 1.2902...  0.4537 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1677...  Training loss: 1.2884...  0.4474 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1678...  Training loss: 1.2964...  0.4547 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1679...  Training loss: 1.2977...  0.4556 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1680...  Training loss: 1.3002...  0.4534 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1681...  Training loss: 1.2623...  0.4464 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1682...  Training loss: 1.3007...  0.4294 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1683...  Training loss: 1.3183...  0.4510 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1684...  Training loss: 1.2846...  0.4483 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1685...  Training loss: 1.2812...  0.4618 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1686...  Training loss: 1.2686...  0.4523 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1687...  Training loss: 1.2867...  0.4483 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1688...  Training loss: 1.2531...  0.4397 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1689...  Training loss: 1.2829...  0.4554 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1690...  Training loss: 1.2663...  0.4573 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1691...  Training loss: 1.2595...  0.4524 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1692...  Training loss: 1.2692...  0.4542 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1693...  Training loss: 1.2657...  0.4551 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1694...  Training loss: 1.2701...  0.4482 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1695...  Training loss: 1.2761...  0.4571 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1696...  Training loss: 1.3002...  0.4564 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1697...  Training loss: 1.2750...  0.4552 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1698...  Training loss: 1.2957...  0.4540 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1699...  Training loss: 1.2855...  0.4593 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1700...  Training loss: 1.2716...  0.4542 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1701...  Training loss: 1.3411...  0.4610 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1702...  Training loss: 1.2658...  0.4575 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1703...  Training loss: 1.2740...  0.4519 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1704...  Training loss: 1.3132...  0.4553 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1705...  Training loss: 1.2939...  0.4570 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1706...  Training loss: 1.2652...  0.4554 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1707...  Training loss: 1.2873...  0.4501 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1708...  Training loss: 1.2559...  0.4517 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1709...  Training loss: 1.2498...  0.4487 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1710...  Training loss: 1.2850...  0.4367 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1711...  Training loss: 1.2888...  0.4566 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1712...  Training loss: 1.2913...  0.4586 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1713...  Training loss: 1.2918...  0.4567 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1714...  Training loss: 1.2866...  0.4540 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1715...  Training loss: 1.2601...  0.4529 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1716...  Training loss: 1.2896...  0.4602 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1717...  Training loss: 1.2982...  0.4570 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1718...  Training loss: 1.2778...  0.4497 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1719...  Training loss: 1.2811...  0.4417 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1720...  Training loss: 1.2677...  0.4640 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1721...  Training loss: 1.2798...  0.4631 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1722...  Training loss: 1.2518...  0.4519 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1723...  Training loss: 1.2825...  0.4420 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1724...  Training loss: 1.2642...  0.4511 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1725...  Training loss: 1.2559...  0.4571 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1726...  Training loss: 1.2730...  0.4493 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1727...  Training loss: 1.2525...  0.4509 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1728...  Training loss: 1.2535...  0.4492 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1729...  Training loss: 1.2809...  0.4370 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1730...  Training loss: 1.2859...  0.4577 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1731...  Training loss: 1.2590...  0.4510 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1732...  Training loss: 1.2923...  0.4563 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1733...  Training loss: 1.2878...  0.4527 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1734...  Training loss: 1.2568...  0.4577 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1735...  Training loss: 1.3320...  0.4586 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1736...  Training loss: 1.2653...  0.4485 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1737...  Training loss: 1.2665...  0.4580 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1738...  Training loss: 1.3034...  0.4586 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1739...  Training loss: 1.2906...  0.4583 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1740...  Training loss: 1.2621...  0.4566 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1741...  Training loss: 1.2870...  0.4572 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1742...  Training loss: 1.2581...  0.4578 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1743...  Training loss: 1.2442...  0.4546 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1744...  Training loss: 1.2795...  0.4582 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1745...  Training loss: 1.2787...  0.4533 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1746...  Training loss: 1.2633...  0.4505 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1747...  Training loss: 1.2742...  0.4511 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1748...  Training loss: 1.2799...  0.4541 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1749...  Training loss: 1.2428...  0.4559 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1750...  Training loss: 1.2911...  0.4519 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1751...  Training loss: 1.2867...  0.4584 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1752...  Training loss: 1.2671...  0.4645 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1753...  Training loss: 1.2664...  0.4428 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1754...  Training loss: 1.2569...  0.4575 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1755...  Training loss: 1.2645...  0.4555 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1756...  Training loss: 1.2309...  0.4377 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1757...  Training loss: 1.2740...  0.4474 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1758...  Training loss: 1.2520...  0.4516 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1759...  Training loss: 1.2410...  0.4554 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1760...  Training loss: 1.2548...  0.4386 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1761...  Training loss: 1.2421...  0.4500 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1762...  Training loss: 1.2453...  0.4505 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1763...  Training loss: 1.2715...  0.4590 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1764...  Training loss: 1.2720...  0.4570 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1765...  Training loss: 1.2493...  0.4533 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1766...  Training loss: 1.2884...  0.4501 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1767...  Training loss: 1.2760...  0.4352 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1768...  Training loss: 1.2502...  0.4509 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1769...  Training loss: 1.3194...  0.4537 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1770...  Training loss: 1.2491...  0.4544 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1771...  Training loss: 1.2579...  0.4475 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1772...  Training loss: 1.3010...  0.4600 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1773...  Training loss: 1.2703...  0.4515 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1774...  Training loss: 1.2488...  0.4556 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1775...  Training loss: 1.2667...  0.4618 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1776...  Training loss: 1.2400...  0.4529 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1777...  Training loss: 1.2322...  0.4556 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1778...  Training loss: 1.2628...  0.4611 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1779...  Training loss: 1.2653...  0.4601 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1780...  Training loss: 1.2574...  0.4529 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1781...  Training loss: 1.2754...  0.4595 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1782...  Training loss: 1.2678...  0.4606 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1783...  Training loss: 1.2402...  0.4590 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1784...  Training loss: 1.2819...  0.4594 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1785...  Training loss: 1.2866...  0.4600 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1786...  Training loss: 1.2569...  0.4579 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1787...  Training loss: 1.2604...  0.4550 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1788...  Training loss: 1.2465...  0.4436 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1789...  Training loss: 1.2685...  0.4557 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1790...  Training loss: 1.2364...  0.4659 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1791...  Training loss: 1.2556...  0.4528 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1792...  Training loss: 1.2412...  0.4586 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1793...  Training loss: 1.2366...  0.4457 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1794...  Training loss: 1.2470...  0.4562 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1795...  Training loss: 1.2424...  0.4533 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1796...  Training loss: 1.2457...  0.4560 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1797...  Training loss: 1.2540...  0.4553 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1798...  Training loss: 1.2570...  0.4500 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1799...  Training loss: 1.2460...  0.4608 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1800...  Training loss: 1.2691...  0.4335 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1801...  Training loss: 1.2690...  0.4524 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1802...  Training loss: 1.2492...  0.4555 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1803...  Training loss: 1.3085...  0.4570 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1804...  Training loss: 1.2341...  0.4553 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1805...  Training loss: 1.2416...  0.4520 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1806...  Training loss: 1.2714...  0.4569 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1807...  Training loss: 1.2653...  0.4569 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1808...  Training loss: 1.2408...  0.4619 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1809...  Training loss: 1.2602...  0.4641 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1810...  Training loss: 1.2308...  0.4608 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1811...  Training loss: 1.2257...  0.4311 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1812...  Training loss: 1.2583...  0.4522 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1813...  Training loss: 1.2591...  0.4502 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1814...  Training loss: 1.2570...  0.4567 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1815...  Training loss: 1.2573...  0.4550 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1816...  Training loss: 1.2578...  0.4641 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1817...  Training loss: 1.2285...  0.4600 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1818...  Training loss: 1.2693...  0.4633 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1819...  Training loss: 1.2776...  0.4567 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1820...  Training loss: 1.2520...  0.4572 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1821...  Training loss: 1.2453...  0.4524 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1822...  Training loss: 1.2281...  0.4600 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1823...  Training loss: 1.2504...  0.4564 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1824...  Training loss: 1.2221...  0.4582 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1825...  Training loss: 1.2476...  0.4513 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1826...  Training loss: 1.2302...  0.4602 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1827...  Training loss: 1.2302...  0.4560 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1828...  Training loss: 1.2405...  0.4542 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1829...  Training loss: 1.2232...  0.4553 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1830...  Training loss: 1.2251...  0.4620 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1831...  Training loss: 1.2469...  0.4610 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1832...  Training loss: 1.2493...  0.4647 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1833...  Training loss: 1.2329...  0.4462 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1834...  Training loss: 1.2649...  0.4591 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1835...  Training loss: 1.2625...  0.4568 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1836...  Training loss: 1.2404...  0.4544 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1837...  Training loss: 1.3009...  0.4520 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1838...  Training loss: 1.2299...  0.4605 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1839...  Training loss: 1.2386...  0.4525 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1840...  Training loss: 1.2732...  0.4504 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1841...  Training loss: 1.2664...  0.4558 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1842...  Training loss: 1.2251...  0.4413 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1843...  Training loss: 1.2552...  0.4501 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1844...  Training loss: 1.2199...  0.4654 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1845...  Training loss: 1.2197...  0.4613 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1846...  Training loss: 1.2508...  0.4543 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1847...  Training loss: 1.2579...  0.4603 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1848...  Training loss: 1.2513...  0.4571 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1849...  Training loss: 1.2531...  0.4375 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1850...  Training loss: 1.2485...  0.4592 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1851...  Training loss: 1.2189...  0.4469 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1852...  Training loss: 1.2613...  0.4542 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1853...  Training loss: 1.2618...  0.4483 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1854...  Training loss: 1.2423...  0.4537 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1855...  Training loss: 1.2313...  0.4583 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1856...  Training loss: 1.2261...  0.4616 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1857...  Training loss: 1.2389...  0.4421 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1858...  Training loss: 1.2157...  0.4575 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1859...  Training loss: 1.2355...  0.4400 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1860...  Training loss: 1.2289...  0.4585 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1861...  Training loss: 1.2214...  0.4539 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1862...  Training loss: 1.2287...  0.4528 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1863...  Training loss: 1.2166...  0.4564 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1864...  Training loss: 1.2200...  0.4583 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1865...  Training loss: 1.2384...  0.4542 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1866...  Training loss: 1.2456...  0.4533 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1867...  Training loss: 1.2270...  0.4554 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1868...  Training loss: 1.2535...  0.4623 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1869...  Training loss: 1.2509...  0.4515 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1870...  Training loss: 1.2268...  0.4543 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1871...  Training loss: 1.2917...  0.4595 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1872...  Training loss: 1.2337...  0.4578 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1873...  Training loss: 1.2353...  0.4561 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1874...  Training loss: 1.2615...  0.4558 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1875...  Training loss: 1.2489...  0.4528 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1876...  Training loss: 1.2174...  0.4503 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1877...  Training loss: 1.2524...  0.4529 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1878...  Training loss: 1.2185...  0.4573 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1879...  Training loss: 1.2002...  0.4495 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1880...  Training loss: 1.2316...  0.4630 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1881...  Training loss: 1.2499...  0.4552 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1882...  Training loss: 1.2348...  0.4539 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1883...  Training loss: 1.2439...  0.4536 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1884...  Training loss: 1.2443...  0.4575 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1885...  Training loss: 1.2218...  0.4552 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1886...  Training loss: 1.2583...  0.4584 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1887...  Training loss: 1.2513...  0.4414 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1888...  Training loss: 1.2278...  0.4521 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1889...  Training loss: 1.2243...  0.4532 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1890...  Training loss: 1.2145...  0.4531 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1891...  Training loss: 1.2303...  0.4589 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1892...  Training loss: 1.1991...  0.4579 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1893...  Training loss: 1.2248...  0.4567 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1894...  Training loss: 1.2170...  0.4550 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1895...  Training loss: 1.2135...  0.4498 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1896...  Training loss: 1.2219...  0.4533 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1897...  Training loss: 1.1995...  0.4516 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1898...  Training loss: 1.2099...  0.4623 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1899...  Training loss: 1.2319...  0.4655 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1900...  Training loss: 1.2331...  0.4494 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1901...  Training loss: 1.2174...  0.4537 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1902...  Training loss: 1.2388...  0.4535 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1903...  Training loss: 1.2453...  0.4580 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1904...  Training loss: 1.2176...  0.4617 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1905...  Training loss: 1.2802...  0.4544 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1906...  Training loss: 1.2185...  0.4467 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1907...  Training loss: 1.2302...  0.4599 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1908...  Training loss: 1.2593...  0.4536 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1909...  Training loss: 1.2396...  0.4507 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1910...  Training loss: 1.2139...  0.4498 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1911...  Training loss: 1.2331...  0.4626 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1912...  Training loss: 1.2056...  0.4592 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1913...  Training loss: 1.1891...  0.4510 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1914...  Training loss: 1.2295...  0.4553 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1915...  Training loss: 1.2444...  0.4544 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1916...  Training loss: 1.2201...  0.4506 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1917...  Training loss: 1.2236...  0.4386 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1918...  Training loss: 1.2354...  0.4586 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1919...  Training loss: 1.1982...  0.4388 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1920...  Training loss: 1.2345...  0.4556 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1921...  Training loss: 1.2460...  0.4543 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1922...  Training loss: 1.2193...  0.4526 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1923...  Training loss: 1.2150...  0.4521 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1924...  Training loss: 1.2021...  0.4471 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1925...  Training loss: 1.2202...  0.4544 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1926...  Training loss: 1.1943...  0.4562 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1927...  Training loss: 1.2291...  0.4587 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1928...  Training loss: 1.2076...  0.4584 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1929...  Training loss: 1.2035...  0.4557 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1930...  Training loss: 1.2005...  0.4517 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1931...  Training loss: 1.1958...  0.4495 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1932...  Training loss: 1.2096...  0.4547 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1933...  Training loss: 1.2242...  0.4585 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1934...  Training loss: 1.2221...  0.4570 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1935...  Training loss: 1.2071...  0.4454 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1936...  Training loss: 1.2415...  0.4577 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1937...  Training loss: 1.2382...  0.4539 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1938...  Training loss: 1.2139...  0.4522 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1939...  Training loss: 1.2796...  0.4650 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1940...  Training loss: 1.2070...  0.4619 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1941...  Training loss: 1.2090...  0.4573 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1942...  Training loss: 1.2388...  0.4495 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1943...  Training loss: 1.2315...  0.4550 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1944...  Training loss: 1.2063...  0.4447 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1945...  Training loss: 1.2314...  0.4551 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1946...  Training loss: 1.1965...  0.4559 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1947...  Training loss: 1.1852...  0.4478 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1948...  Training loss: 1.2192...  0.4469 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1949...  Training loss: 1.2192...  0.4472 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1950...  Training loss: 1.2212...  0.4505 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1951...  Training loss: 1.2186...  0.4564 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1952...  Training loss: 1.2163...  0.4631 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1953...  Training loss: 1.1959...  0.4627 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1954...  Training loss: 1.2281...  0.4554 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1955...  Training loss: 1.2427...  0.4572 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1956...  Training loss: 1.2140...  0.4516 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1957...  Training loss: 1.2036...  0.4466 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1958...  Training loss: 1.1879...  0.4567 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1959...  Training loss: 1.2128...  0.4516 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1960...  Training loss: 1.1855...  0.4540 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1961...  Training loss: 1.2192...  0.4549 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1962...  Training loss: 1.2110...  0.4541 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1963...  Training loss: 1.1936...  0.4525 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1964...  Training loss: 1.2032...  0.4533 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1965...  Training loss: 1.1888...  0.4576 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1966...  Training loss: 1.1947...  0.4529 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1967...  Training loss: 1.2018...  0.4569 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1968...  Training loss: 1.2196...  0.4449 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1969...  Training loss: 1.1931...  0.4539 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1970...  Training loss: 1.2293...  0.4579 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1971...  Training loss: 1.2143...  0.4533 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1972...  Training loss: 1.2052...  0.4562 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1973...  Training loss: 1.2685...  0.4397 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1974...  Training loss: 1.2002...  0.4440 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1975...  Training loss: 1.1969...  0.4580 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1976...  Training loss: 1.2305...  0.4591 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1977...  Training loss: 1.2176...  0.4569 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1978...  Training loss: 1.1951...  0.4563 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1979...  Training loss: 1.2203...  0.4525 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1980...  Training loss: 1.1896...  0.4589 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1981...  Training loss: 1.1737...  0.4518 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1982...  Training loss: 1.2077...  0.4570 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1983...  Training loss: 1.2232...  0.4518 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1984...  Training loss: 1.2126...  0.4535 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1985...  Training loss: 1.2236...  0.4543 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1986...  Training loss: 1.2186...  0.4508 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1987...  Training loss: 1.1897...  0.4566 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1988...  Training loss: 1.2198...  0.4510 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1989...  Training loss: 1.2352...  0.4546 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1990...  Training loss: 1.2019...  0.4575 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1991...  Training loss: 1.1946...  0.4598 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1992...  Training loss: 1.1834...  0.4574 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1993...  Training loss: 1.1954...  0.4567 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1994...  Training loss: 1.1768...  0.4529 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1995...  Training loss: 1.2067...  0.4484 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1996...  Training loss: 1.1903...  0.4574 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1997...  Training loss: 1.1856...  0.4544 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1998...  Training loss: 1.2055...  0.4487 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1999...  Training loss: 1.1825...  0.4596 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2000...  Training loss: 1.1837...  0.4534 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2001...  Training loss: 1.1976...  0.4591 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2002...  Training loss: 1.2023...  0.4507 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2003...  Training loss: 1.1859...  0.4569 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2004...  Training loss: 1.2079...  0.4562 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2005...  Training loss: 1.2194...  0.4559 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2006...  Training loss: 1.1952...  0.4550 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2007...  Training loss: 1.2610...  0.4561 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2008...  Training loss: 1.1986...  0.4513 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2009...  Training loss: 1.1950...  0.4519 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2010...  Training loss: 1.2338...  0.4465 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2011...  Training loss: 1.2165...  0.4566 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2012...  Training loss: 1.1896...  0.4560 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2013...  Training loss: 1.2180...  0.4546 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2014...  Training loss: 1.1851...  0.4549 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2015...  Training loss: 1.1730...  0.4616 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2016...  Training loss: 1.1934...  0.4533 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2017...  Training loss: 1.2094...  0.4534 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2018...  Training loss: 1.2060...  0.4513 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2019...  Training loss: 1.2057...  0.4542 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2020...  Training loss: 1.2088...  0.4485 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2021...  Training loss: 1.1881...  0.4516 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2022...  Training loss: 1.2099...  0.4566 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2023...  Training loss: 1.2273...  0.4599 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2024...  Training loss: 1.2102...  0.4515 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2025...  Training loss: 1.1923...  0.4562 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2026...  Training loss: 1.1830...  0.4547 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2027...  Training loss: 1.2044...  0.4594 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2028...  Training loss: 1.1692...  0.4526 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2029...  Training loss: 1.2054...  0.4490 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2030...  Training loss: 1.1806...  0.4355 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2031...  Training loss: 1.1881...  0.4542 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2032...  Training loss: 1.1856...  0.4539 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2033...  Training loss: 1.1820...  0.4550 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2034...  Training loss: 1.1733...  0.4556 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2035...  Training loss: 1.1971...  0.4539 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2036...  Training loss: 1.1925...  0.4421 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2037...  Training loss: 1.1821...  0.4550 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2038...  Training loss: 1.2169...  0.4612 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2039...  Training loss: 1.2076...  0.4459 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2040...  Training loss: 1.1764...  0.4591 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2041...  Training loss: 1.2575...  0.4500 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2042...  Training loss: 1.1890...  0.4544 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2043...  Training loss: 1.1735...  0.4554 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2044...  Training loss: 1.2118...  0.4627 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2045...  Training loss: 1.2028...  0.4498 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2046...  Training loss: 1.1805...  0.4564 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2047...  Training loss: 1.2003...  0.4490 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2048...  Training loss: 1.1715...  0.4523 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2049...  Training loss: 1.1567...  0.4556 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2050...  Training loss: 1.1898...  0.4446 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2051...  Training loss: 1.2021...  0.4471 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2052...  Training loss: 1.1944...  0.4491 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2053...  Training loss: 1.2003...  0.4580 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2054...  Training loss: 1.1976...  0.4534 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2055...  Training loss: 1.1714...  0.4586 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2056...  Training loss: 1.2002...  0.4519 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2057...  Training loss: 1.2139...  0.4507 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2058...  Training loss: 1.1914...  0.4542 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2059...  Training loss: 1.1892...  0.4574 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2060...  Training loss: 1.1688...  0.4565 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2061...  Training loss: 1.1851...  0.4565 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2062...  Training loss: 1.1628...  0.4612 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2063...  Training loss: 1.1894...  0.4549 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2064...  Training loss: 1.1703...  0.4510 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2065...  Training loss: 1.1669...  0.4528 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2066...  Training loss: 1.1874...  0.4513 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2067...  Training loss: 1.1559...  0.4514 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2068...  Training loss: 1.1644...  0.4530 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2069...  Training loss: 1.1872...  0.4508 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2070...  Training loss: 1.1847...  0.4545 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2071...  Training loss: 1.1631...  0.4553 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2072...  Training loss: 1.1872...  0.4483 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2073...  Training loss: 1.1939...  0.4579 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2074...  Training loss: 1.1691...  0.4386 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2075...  Training loss: 1.2425...  0.4610 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2076...  Training loss: 1.1725...  0.4489 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2077...  Training loss: 1.1639...  0.4577 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2078...  Training loss: 1.2032...  0.4625 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2079...  Training loss: 1.1950...  0.4544 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2080...  Training loss: 1.1673...  0.4580 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2081...  Training loss: 1.1960...  0.4547 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2082...  Training loss: 1.1646...  0.4510 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2083...  Training loss: 1.1454...  0.4599 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2084...  Training loss: 1.1771...  0.4621 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2085...  Training loss: 1.1859...  0.4565 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2086...  Training loss: 1.1792...  0.4541 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2087...  Training loss: 1.1923...  0.4571 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2088...  Training loss: 1.1960...  0.4540 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2089...  Training loss: 1.1643...  0.4501 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2090...  Training loss: 1.1957...  0.4656 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2091...  Training loss: 1.2014...  0.4560 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2092...  Training loss: 1.1894...  0.4478 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2093...  Training loss: 1.1744...  0.4571 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2094...  Training loss: 1.1580...  0.4503 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2095...  Training loss: 1.1742...  0.4495 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2096...  Training loss: 1.1497...  0.4494 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2097...  Training loss: 1.1806...  0.4509 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2098...  Training loss: 1.1655...  0.4518 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2099...  Training loss: 1.1678...  0.4569 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2100...  Training loss: 1.1729...  0.4498 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2101...  Training loss: 1.1555...  0.4592 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2102...  Training loss: 1.1691...  0.4586 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2103...  Training loss: 1.1781...  0.4547 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2104...  Training loss: 1.1694...  0.4518 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2105...  Training loss: 1.1656...  0.4563 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2106...  Training loss: 1.1897...  0.4555 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2107...  Training loss: 1.1855...  0.4547 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2108...  Training loss: 1.1578...  0.4506 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2109...  Training loss: 1.2408...  0.4527 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2110...  Training loss: 1.1737...  0.4557 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2111...  Training loss: 1.1612...  0.4517 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2112...  Training loss: 1.1962...  0.4596 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2113...  Training loss: 1.1904...  0.4410 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2114...  Training loss: 1.1594...  0.4545 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2115...  Training loss: 1.1757...  0.4596 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2116...  Training loss: 1.1624...  0.4534 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2117...  Training loss: 1.1392...  0.4567 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2118...  Training loss: 1.1738...  0.4538 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2119...  Training loss: 1.1872...  0.4534 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2120...  Training loss: 1.1818...  0.4583 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2121...  Training loss: 1.1781...  0.4554 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2122...  Training loss: 1.1773...  0.4586 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2123...  Training loss: 1.1565...  0.4467 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2124...  Training loss: 1.1920...  0.4545 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2125...  Training loss: 1.1894...  0.4470 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2126...  Training loss: 1.1770...  0.4546 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2127...  Training loss: 1.1637...  0.4512 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2128...  Training loss: 1.1531...  0.4471 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2129...  Training loss: 1.1701...  0.4617 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2130...  Training loss: 1.1431...  0.4491 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2131...  Training loss: 1.1726...  0.4524 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2132...  Training loss: 1.1559...  0.4596 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2133...  Training loss: 1.1510...  0.4506 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2134...  Training loss: 1.1617...  0.4608 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2135...  Training loss: 1.1512...  0.4561 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2136...  Training loss: 1.1580...  0.4589 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2137...  Training loss: 1.1606...  0.4560 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2138...  Training loss: 1.1722...  0.4600 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2139...  Training loss: 1.1513...  0.4542 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2140...  Training loss: 1.1812...  0.4596 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2141...  Training loss: 1.1758...  0.4477 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2142...  Training loss: 1.1563...  0.4562 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2143...  Training loss: 1.2380...  0.4537 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2144...  Training loss: 1.1578...  0.4536 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2145...  Training loss: 1.1585...  0.4613 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2146...  Training loss: 1.1915...  0.4578 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2147...  Training loss: 1.1755...  0.4622 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2148...  Training loss: 1.1471...  0.4521 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2149...  Training loss: 1.1819...  0.4455 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2150...  Training loss: 1.1441...  0.4567 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2151...  Training loss: 1.1345...  0.4528 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2152...  Training loss: 1.1656...  0.4553 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2153...  Training loss: 1.1852...  0.4574 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2154...  Training loss: 1.1655...  0.4569 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2155...  Training loss: 1.1694...  0.4537 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2156...  Training loss: 1.1756...  0.4491 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2157...  Training loss: 1.1396...  0.4364 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2158...  Training loss: 1.1808...  0.4610 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2159...  Training loss: 1.1910...  0.4616 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2160...  Training loss: 1.1700...  0.4548 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2161...  Training loss: 1.1593...  0.4569 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2162...  Training loss: 1.1547...  0.4555 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2163...  Training loss: 1.1596...  0.4517 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2164...  Training loss: 1.1332...  0.4543 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2165...  Training loss: 1.1733...  0.4600 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2166...  Training loss: 1.1381...  0.4493 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2167...  Training loss: 1.1548...  0.4529 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2168...  Training loss: 1.1506...  0.4579 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2169...  Training loss: 1.1380...  0.4554 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2170...  Training loss: 1.1426...  0.4465 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2171...  Training loss: 1.1582...  0.4573 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2172...  Training loss: 1.1608...  0.4531 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2173...  Training loss: 1.1408...  0.4598 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2174...  Training loss: 1.1733...  0.4489 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2175...  Training loss: 1.1675...  0.4518 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2176...  Training loss: 1.1445...  0.4557 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2177...  Training loss: 1.2188...  0.4482 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2178...  Training loss: 1.1513...  0.4475 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2179...  Training loss: 1.1464...  0.4379 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2180...  Training loss: 1.1835...  0.4569 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2181...  Training loss: 1.1719...  0.4554 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2182...  Training loss: 1.1537...  0.4613 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2183...  Training loss: 1.1684...  0.4558 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2184...  Training loss: 1.1490...  0.4492 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2185...  Training loss: 1.1269...  0.4665 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2186...  Training loss: 1.1551...  0.4455 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2187...  Training loss: 1.1619...  0.4568 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2188...  Training loss: 1.1682...  0.4527 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2189...  Training loss: 1.1642...  0.4529 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2190...  Training loss: 1.1659...  0.4585 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2191...  Training loss: 1.1413...  0.4594 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2192...  Training loss: 1.1696...  0.4587 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2193...  Training loss: 1.1766...  0.4633 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2194...  Training loss: 1.1547...  0.4491 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2195...  Training loss: 1.1529...  0.4563 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2196...  Training loss: 1.1528...  0.4612 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2197...  Training loss: 1.1562...  0.4591 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2198...  Training loss: 1.1211...  0.4509 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2199...  Training loss: 1.1688...  0.4608 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2200...  Training loss: 1.1323...  0.4582 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2201...  Training loss: 1.1345...  0.4482 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2202...  Training loss: 1.1551...  0.4580 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2203...  Training loss: 1.1351...  0.4553 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2204...  Training loss: 1.1428...  0.4518 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2205...  Training loss: 1.1452...  0.4502 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2206...  Training loss: 1.1491...  0.4521 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2207...  Training loss: 1.1342...  0.4544 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2208...  Training loss: 1.1585...  0.4572 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2209...  Training loss: 1.1532...  0.4579 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2210...  Training loss: 1.1436...  0.4570 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2211...  Training loss: 1.2153...  0.4483 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2212...  Training loss: 1.1448...  0.4572 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2213...  Training loss: 1.1357...  0.4625 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2214...  Training loss: 1.1825...  0.4530 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2215...  Training loss: 1.1697...  0.4544 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2216...  Training loss: 1.1484...  0.4524 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2217...  Training loss: 1.1550...  0.4559 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2218...  Training loss: 1.1399...  0.4540 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2219...  Training loss: 1.1181...  0.4507 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2220...  Training loss: 1.1552...  0.4488 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2221...  Training loss: 1.1575...  0.4558 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2222...  Training loss: 1.1459...  0.4566 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2223...  Training loss: 1.1497...  0.4627 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2224...  Training loss: 1.1610...  0.4555 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2225...  Training loss: 1.1286...  0.4472 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2226...  Training loss: 1.1746...  0.4583 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2227...  Training loss: 1.1671...  0.4584 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2228...  Training loss: 1.1521...  0.4525 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2229...  Training loss: 1.1463...  0.4556 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2230...  Training loss: 1.1230...  0.4572 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2231...  Training loss: 1.1501...  0.4476 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2232...  Training loss: 1.1138...  0.4542 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2233...  Training loss: 1.1508...  0.4561 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2234...  Training loss: 1.1291...  0.4569 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2235...  Training loss: 1.1440...  0.4522 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2236...  Training loss: 1.1443...  0.4589 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2237...  Training loss: 1.1268...  0.4535 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2238...  Training loss: 1.1287...  0.4529 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2239...  Training loss: 1.1422...  0.4618 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2240...  Training loss: 1.1568...  0.4566 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2241...  Training loss: 1.1350...  0.4588 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2242...  Training loss: 1.1572...  0.4487 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2243...  Training loss: 1.1520...  0.4536 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2244...  Training loss: 1.1412...  0.4577 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2245...  Training loss: 1.2089...  0.4548 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2246...  Training loss: 1.1347...  0.4619 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2247...  Training loss: 1.1347...  0.4572 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2248...  Training loss: 1.1626...  0.4477 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2249...  Training loss: 1.1690...  0.4538 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2250...  Training loss: 1.1365...  0.4547 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2251...  Training loss: 1.1547...  0.4550 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2252...  Training loss: 1.1317...  0.4673 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2253...  Training loss: 1.1133...  0.4583 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2254...  Training loss: 1.1455...  0.4514 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2255...  Training loss: 1.1460...  0.4534 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2256...  Training loss: 1.1460...  0.4550 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2257...  Training loss: 1.1431...  0.4516 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2258...  Training loss: 1.1475...  0.4565 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2259...  Training loss: 1.1226...  0.4525 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2260...  Training loss: 1.1608...  0.4609 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2261...  Training loss: 1.1700...  0.4519 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2262...  Training loss: 1.1518...  0.4576 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2263...  Training loss: 1.1383...  0.4521 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2264...  Training loss: 1.1217...  0.4594 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2265...  Training loss: 1.1385...  0.4558 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2266...  Training loss: 1.1150...  0.4560 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2267...  Training loss: 1.1440...  0.4561 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2268...  Training loss: 1.1154...  0.4609 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2269...  Training loss: 1.1363...  0.4599 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2270...  Training loss: 1.1324...  0.4463 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2271...  Training loss: 1.1211...  0.4502 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2272...  Training loss: 1.1260...  0.4499 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2273...  Training loss: 1.1394...  0.4618 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2274...  Training loss: 1.1412...  0.4575 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2275...  Training loss: 1.1242...  0.4518 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2276...  Training loss: 1.1462...  0.4635 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2277...  Training loss: 1.1512...  0.4498 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2278...  Training loss: 1.1215...  0.4505 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2279...  Training loss: 1.1928...  0.4502 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2280...  Training loss: 1.1303...  0.4563 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2281...  Training loss: 1.1295...  0.4625 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2282...  Training loss: 1.1530...  0.4538 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2283...  Training loss: 1.1462...  0.4539 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2284...  Training loss: 1.1244...  0.4537 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2285...  Training loss: 1.1415...  0.4638 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2286...  Training loss: 1.1162...  0.4529 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2287...  Training loss: 1.1176...  0.4535 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2288...  Training loss: 1.1389...  0.4548 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2289...  Training loss: 1.1423...  0.4525 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2290...  Training loss: 1.1362...  0.4454 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2291...  Training loss: 1.1409...  0.4613 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2292...  Training loss: 1.1527...  0.4517 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2293...  Training loss: 1.1193...  0.4502 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2294...  Training loss: 1.1532...  0.4567 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2295...  Training loss: 1.1575...  0.4600 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2296...  Training loss: 1.1409...  0.4627 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2297...  Training loss: 1.1401...  0.4420 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2298...  Training loss: 1.1234...  0.4367 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2299...  Training loss: 1.1433...  0.4585 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2300...  Training loss: 1.1012...  0.4452 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2301...  Training loss: 1.1396...  0.4516 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2302...  Training loss: 1.1099...  0.4462 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2303...  Training loss: 1.1234...  0.4600 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2304...  Training loss: 1.1225...  0.4525 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2305...  Training loss: 1.1055...  0.4535 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2306...  Training loss: 1.1111...  0.4542 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2307...  Training loss: 1.1405...  0.4566 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2308...  Training loss: 1.1359...  0.4574 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2309...  Training loss: 1.1206...  0.4563 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2310...  Training loss: 1.1371...  0.4569 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2311...  Training loss: 1.1406...  0.4504 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2312...  Training loss: 1.1256...  0.4555 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2313...  Training loss: 1.1962...  0.4545 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2314...  Training loss: 1.1208...  0.4554 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2315...  Training loss: 1.1231...  0.4426 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2316...  Training loss: 1.1552...  0.4559 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2317...  Training loss: 1.1479...  0.4547 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2318...  Training loss: 1.1163...  0.4584 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2319...  Training loss: 1.1454...  0.4514 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2320...  Training loss: 1.1095...  0.4499 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2321...  Training loss: 1.0865...  0.4571 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2322...  Training loss: 1.1344...  0.4555 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2323...  Training loss: 1.1337...  0.4512 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2324...  Training loss: 1.1408...  0.4526 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2325...  Training loss: 1.1330...  0.4582 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2326...  Training loss: 1.1341...  0.4578 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2327...  Training loss: 1.1021...  0.4536 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2328...  Training loss: 1.1460...  0.4539 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2329...  Training loss: 1.1427...  0.4475 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2330...  Training loss: 1.1352...  0.4537 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2331...  Training loss: 1.1187...  0.4528 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2332...  Training loss: 1.1182...  0.4542 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2333...  Training loss: 1.1232...  0.4516 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2334...  Training loss: 1.1123...  0.4534 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2335...  Training loss: 1.1329...  0.4508 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2336...  Training loss: 1.1087...  0.4525 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2337...  Training loss: 1.1117...  0.4504 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2338...  Training loss: 1.1210...  0.4482 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2339...  Training loss: 1.1032...  0.4594 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2340...  Training loss: 1.1082...  0.4523 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2341...  Training loss: 1.1160...  0.4514 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2342...  Training loss: 1.1192...  0.4486 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2343...  Training loss: 1.1184...  0.4535 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2344...  Training loss: 1.1410...  0.4592 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2345...  Training loss: 1.1342...  0.4547 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2346...  Training loss: 1.1146...  0.4528 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2347...  Training loss: 1.1823...  0.4484 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2348...  Training loss: 1.1132...  0.4472 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2349...  Training loss: 1.1176...  0.4475 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2350...  Training loss: 1.1379...  0.4489 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2351...  Training loss: 1.1534...  0.4554 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2352...  Training loss: 1.1115...  0.4564 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2353...  Training loss: 1.1330...  0.4554 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2354...  Training loss: 1.1057...  0.4547 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2355...  Training loss: 1.0866...  0.4564 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2356...  Training loss: 1.1277...  0.4593 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2357...  Training loss: 1.1417...  0.4557 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2358...  Training loss: 1.1243...  0.4597 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2359...  Training loss: 1.1252...  0.4603 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2360...  Training loss: 1.1267...  0.4327 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2361...  Training loss: 1.1007...  0.4484 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2362...  Training loss: 1.1326...  0.4539 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2363...  Training loss: 1.1380...  0.4557 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2364...  Training loss: 1.1303...  0.4555 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2365...  Training loss: 1.1264...  0.4538 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2366...  Training loss: 1.1107...  0.4555 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2367...  Training loss: 1.1158...  0.4442 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2368...  Training loss: 1.0934...  0.4505 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2369...  Training loss: 1.1194...  0.4503 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2370...  Training loss: 1.1175...  0.4562 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2371...  Training loss: 1.1115...  0.4569 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2372...  Training loss: 1.1173...  0.4483 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2373...  Training loss: 1.1014...  0.4402 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2374...  Training loss: 1.1059...  0.4463 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2375...  Training loss: 1.1050...  0.4561 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2376...  Training loss: 1.1072...  0.4578 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2377...  Training loss: 1.0992...  0.4572 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2378...  Training loss: 1.1334...  0.4634 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2379...  Training loss: 1.1241...  0.4508 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2380...  Training loss: 1.1087...  0.4568 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2381...  Training loss: 1.1697...  0.4584 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2382...  Training loss: 1.1199...  0.4495 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2383...  Training loss: 1.1179...  0.4554 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2384...  Training loss: 1.1344...  0.4609 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2385...  Training loss: 1.1243...  0.4491 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2386...  Training loss: 1.1033...  0.4367 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2387...  Training loss: 1.1389...  0.4494 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2388...  Training loss: 1.1142...  0.4514 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2389...  Training loss: 1.0978...  0.4605 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2390...  Training loss: 1.1246...  0.4562 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2391...  Training loss: 1.1289...  0.4572 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2392...  Training loss: 1.1264...  0.4501 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2393...  Training loss: 1.1177...  0.4552 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2394...  Training loss: 1.1339...  0.4412 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2395...  Training loss: 1.1020...  0.4571 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2396...  Training loss: 1.1364...  0.4539 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2397...  Training loss: 1.1389...  0.4628 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2398...  Training loss: 1.1213...  0.4551 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2399...  Training loss: 1.1211...  0.4510 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2400...  Training loss: 1.0991...  0.4582 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2401...  Training loss: 1.1192...  0.4530 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2402...  Training loss: 1.0955...  0.4543 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2403...  Training loss: 1.1283...  0.4479 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2404...  Training loss: 1.1060...  0.4511 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2405...  Training loss: 1.1124...  0.4564 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2406...  Training loss: 1.1090...  0.4559 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2407...  Training loss: 1.0930...  0.4603 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2408...  Training loss: 1.1028...  0.4580 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2409...  Training loss: 1.1179...  0.4622 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2410...  Training loss: 1.1062...  0.4550 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2411...  Training loss: 1.1041...  0.4559 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2412...  Training loss: 1.1195...  0.4482 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2413...  Training loss: 1.1230...  0.4554 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2414...  Training loss: 1.0982...  0.4557 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2415...  Training loss: 1.1644...  0.4566 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2416...  Training loss: 1.1098...  0.4585 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2417...  Training loss: 1.1143...  0.4537 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2418...  Training loss: 1.1362...  0.4460 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2419...  Training loss: 1.1346...  0.4527 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2420...  Training loss: 1.1000...  0.4379 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2421...  Training loss: 1.1277...  0.4527 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2422...  Training loss: 1.1130...  0.4548 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2423...  Training loss: 1.0937...  0.4496 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2424...  Training loss: 1.1067...  0.4504 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2425...  Training loss: 1.1212...  0.4600 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2426...  Training loss: 1.1168...  0.4428 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2427...  Training loss: 1.1077...  0.4585 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2428...  Training loss: 1.1204...  0.4562 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2429...  Training loss: 1.0901...  0.4591 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2430...  Training loss: 1.1232...  0.4460 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2431...  Training loss: 1.1272...  0.4545 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2432...  Training loss: 1.1098...  0.4322 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2433...  Training loss: 1.0999...  0.4538 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2434...  Training loss: 1.0927...  0.4517 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2435...  Training loss: 1.1065...  0.4485 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2436...  Training loss: 1.0843...  0.4518 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2437...  Training loss: 1.1093...  0.4569 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2438...  Training loss: 1.0883...  0.4620 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2439...  Training loss: 1.0936...  0.4575 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2440...  Training loss: 1.1037...  0.4549 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2441...  Training loss: 1.0868...  0.4538 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2442...  Training loss: 1.0968...  0.4599 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2443...  Training loss: 1.1058...  0.4550 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2444...  Training loss: 1.1061...  0.4492 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2445...  Training loss: 1.0720...  0.4526 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2446...  Training loss: 1.1160...  0.4543 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2447...  Training loss: 1.1132...  0.4495 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2448...  Training loss: 1.0837...  0.4531 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2449...  Training loss: 1.1662...  0.4500 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2450...  Training loss: 1.0964...  0.4592 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2451...  Training loss: 1.0866...  0.4538 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2452...  Training loss: 1.1230...  0.4474 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2453...  Training loss: 1.1190...  0.4467 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2454...  Training loss: 1.0925...  0.4432 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2455...  Training loss: 1.1042...  0.4538 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2456...  Training loss: 1.0952...  0.4572 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2457...  Training loss: 1.0868...  0.4507 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2458...  Training loss: 1.1100...  0.4540 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2459...  Training loss: 1.1145...  0.4563 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2460...  Training loss: 1.1065...  0.4609 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2461...  Training loss: 1.1121...  0.4576 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2462...  Training loss: 1.1097...  0.4494 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2463...  Training loss: 1.0871...  0.4555 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2464...  Training loss: 1.1186...  0.4625 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2465...  Training loss: 1.1258...  0.4571 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2466...  Training loss: 1.1037...  0.4394 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2467...  Training loss: 1.1107...  0.4500 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2468...  Training loss: 1.0907...  0.4543 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2469...  Training loss: 1.1026...  0.4538 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2470...  Training loss: 1.0783...  0.4567 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2471...  Training loss: 1.1060...  0.4502 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2472...  Training loss: 1.0817...  0.4468 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2473...  Training loss: 1.0958...  0.4500 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2474...  Training loss: 1.0938...  0.4564 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2475...  Training loss: 1.0690...  0.4496 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2476...  Training loss: 1.0894...  0.4526 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2477...  Training loss: 1.1088...  0.4581 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2478...  Training loss: 1.1037...  0.4630 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2479...  Training loss: 1.0796...  0.4593 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2480...  Training loss: 1.1093...  0.4560 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2481...  Training loss: 1.1062...  0.4562 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2482...  Training loss: 1.0831...  0.4571 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2483...  Training loss: 1.1550...  0.4583 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2484...  Training loss: 1.0907...  0.4571 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2485...  Training loss: 1.0967...  0.4544 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2486...  Training loss: 1.1246...  0.4532 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2487...  Training loss: 1.0979...  0.4566 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2488...  Training loss: 1.0839...  0.4425 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2489...  Training loss: 1.1152...  0.4476 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2490...  Training loss: 1.0839...  0.4592 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2491...  Training loss: 1.0810...  0.4582 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2492...  Training loss: 1.1104...  0.4601 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2493...  Training loss: 1.1169...  0.4581 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2494...  Training loss: 1.0961...  0.4595 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2495...  Training loss: 1.1012...  0.4579 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2496...  Training loss: 1.1069...  0.4475 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2497...  Training loss: 1.0804...  0.4528 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2498...  Training loss: 1.1143...  0.4597 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2499...  Training loss: 1.1205...  0.4577 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2500...  Training loss: 1.0920...  0.4529 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2501...  Training loss: 1.0953...  0.4468 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2502...  Training loss: 1.0882...  0.4528 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2503...  Training loss: 1.0986...  0.4612 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2504...  Training loss: 1.0747...  0.4543 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2505...  Training loss: 1.1066...  0.4607 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2506...  Training loss: 1.0717...  0.4547 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2507...  Training loss: 1.0788...  0.4434 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2508...  Training loss: 1.0820...  0.4624 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2509...  Training loss: 1.0759...  0.4526 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2510...  Training loss: 1.0742...  0.4657 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2511...  Training loss: 1.0903...  0.4520 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2512...  Training loss: 1.0979...  0.4463 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2513...  Training loss: 1.0629...  0.4566 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2514...  Training loss: 1.1058...  0.4485 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2515...  Training loss: 1.0970...  0.4532 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2516...  Training loss: 1.0725...  0.4512 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2517...  Training loss: 1.1454...  0.4475 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2518...  Training loss: 1.0829...  0.4472 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2519...  Training loss: 1.0807...  0.4496 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2520...  Training loss: 1.1052...  0.4506 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2521...  Training loss: 1.0993...  0.4529 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2522...  Training loss: 1.0808...  0.4596 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2523...  Training loss: 1.0997...  0.4507 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2524...  Training loss: 1.0669...  0.4520 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2525...  Training loss: 1.0741...  0.4614 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2526...  Training loss: 1.0956...  0.4487 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2527...  Training loss: 1.0976...  0.4519 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2528...  Training loss: 1.0970...  0.4721 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2529...  Training loss: 1.1028...  0.4433 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2530...  Training loss: 1.0947...  0.4579 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2531...  Training loss: 1.0669...  0.4557 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2532...  Training loss: 1.0985...  0.4493 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2533...  Training loss: 1.1030...  0.4535 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2534...  Training loss: 1.0881...  0.4580 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2535...  Training loss: 1.0780...  0.4671 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2536...  Training loss: 1.0734...  0.4488 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2537...  Training loss: 1.0944...  0.4664 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2538...  Training loss: 1.0744...  0.4418 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2539...  Training loss: 1.0940...  0.4581 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2540...  Training loss: 1.0643...  0.4580 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2541...  Training loss: 1.0772...  0.4533 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2542...  Training loss: 1.0812...  0.4591 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2543...  Training loss: 1.0733...  0.4366 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2544...  Training loss: 1.0610...  0.4493 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2545...  Training loss: 1.0829...  0.4535 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2546...  Training loss: 1.0817...  0.4490 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2547...  Training loss: 1.0670...  0.4574 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2548...  Training loss: 1.0959...  0.4505 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2549...  Training loss: 1.0841...  0.4614 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2550...  Training loss: 1.0620...  0.4492 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2551...  Training loss: 1.1393...  0.4371 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2552...  Training loss: 1.0746...  0.4508 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2553...  Training loss: 1.0655...  0.4406 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2554...  Training loss: 1.1048...  0.4600 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2555...  Training loss: 1.1005...  0.4487 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2556...  Training loss: 1.0825...  0.4482 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2557...  Training loss: 1.0829...  0.4458 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2558...  Training loss: 1.0760...  0.4552 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2559...  Training loss: 1.0512...  0.4543 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2560...  Training loss: 1.0868...  0.4606 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2561...  Training loss: 1.0925...  0.4519 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2562...  Training loss: 1.0840...  0.4602 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2563...  Training loss: 1.0736...  0.4490 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2564...  Training loss: 1.0804...  0.4495 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2565...  Training loss: 1.0576...  0.4665 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2566...  Training loss: 1.0912...  0.4547 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2567...  Training loss: 1.0981...  0.4518 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2568...  Training loss: 1.0887...  0.4513 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2569...  Training loss: 1.0818...  0.4606 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2570...  Training loss: 1.0749...  0.4597 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2571...  Training loss: 1.0807...  0.4574 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2572...  Training loss: 1.0534...  0.4552 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2573...  Training loss: 1.0915...  0.4449 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2574...  Training loss: 1.0713...  0.4560 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2575...  Training loss: 1.0696...  0.4517 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2576...  Training loss: 1.0664...  0.4584 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2577...  Training loss: 1.0547...  0.4682 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2578...  Training loss: 1.0465...  0.4586 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2579...  Training loss: 1.0846...  0.4594 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2580...  Training loss: 1.0753...  0.4516 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2581...  Training loss: 1.0577...  0.4568 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2582...  Training loss: 1.1034...  0.4501 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2583...  Training loss: 1.0776...  0.4528 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2584...  Training loss: 1.0477...  0.4559 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2585...  Training loss: 1.1253...  0.4551 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2586...  Training loss: 1.0700...  0.4427 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2587...  Training loss: 1.0634...  0.4487 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2588...  Training loss: 1.0931...  0.4589 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2589...  Training loss: 1.0912...  0.4471 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2590...  Training loss: 1.0591...  0.4570 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2591...  Training loss: 1.0809...  0.4550 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2592...  Training loss: 1.0642...  0.4681 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2593...  Training loss: 1.0530...  0.4527 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2594...  Training loss: 1.0810...  0.4495 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2595...  Training loss: 1.0839...  0.4533 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2596...  Training loss: 1.0709...  0.4526 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2597...  Training loss: 1.0709...  0.4541 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2598...  Training loss: 1.0747...  0.4458 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2599...  Training loss: 1.0499...  0.4590 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2600...  Training loss: 1.0776...  0.4540 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2601...  Training loss: 1.0979...  0.4535 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2602...  Training loss: 1.0798...  0.4518 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2603...  Training loss: 1.0656...  0.4540 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2604...  Training loss: 1.0619...  0.4602 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2605...  Training loss: 1.0754...  0.4549 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2606...  Training loss: 1.0419...  0.4600 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2607...  Training loss: 1.0774...  0.4565 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2608...  Training loss: 1.0528...  0.4382 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2609...  Training loss: 1.0613...  0.4485 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2610...  Training loss: 1.0570...  0.4550 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2611...  Training loss: 1.0607...  0.4501 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2612...  Training loss: 1.0583...  0.4460 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2613...  Training loss: 1.0725...  0.4491 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2614...  Training loss: 1.0655...  0.4500 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2615...  Training loss: 1.0527...  0.4537 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2616...  Training loss: 1.0828...  0.4709 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2617...  Training loss: 1.0734...  0.4486 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2618...  Training loss: 1.0497...  0.4522 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2619...  Training loss: 1.1219...  0.4533 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2620...  Training loss: 1.0674...  0.4505 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2621...  Training loss: 1.0522...  0.4544 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2622...  Training loss: 1.0763...  0.4610 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2623...  Training loss: 1.0777...  0.4520 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2624...  Training loss: 1.0601...  0.4524 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2625...  Training loss: 1.0679...  0.4605 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2626...  Training loss: 1.0428...  0.4537 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2627...  Training loss: 1.0398...  0.4527 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2628...  Training loss: 1.0657...  0.4526 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2629...  Training loss: 1.0791...  0.4548 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2630...  Training loss: 1.0785...  0.4527 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2631...  Training loss: 1.0745...  0.4570 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2632...  Training loss: 1.0657...  0.4535 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2633...  Training loss: 1.0442...  0.4538 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2634...  Training loss: 1.0698...  0.4686 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2635...  Training loss: 1.0753...  0.4536 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2636...  Training loss: 1.0524...  0.4544 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2637...  Training loss: 1.0579...  0.4528 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2638...  Training loss: 1.0516...  0.4366 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2639...  Training loss: 1.0705...  0.4505 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2640...  Training loss: 1.0351...  0.4570 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2641...  Training loss: 1.0677...  0.4509 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2642...  Training loss: 1.0378...  0.4525 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2643...  Training loss: 1.0533...  0.4567 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2644...  Training loss: 1.0587...  0.4492 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2645...  Training loss: 1.0320...  0.4561 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2646...  Training loss: 1.0453...  0.4592 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2647...  Training loss: 1.0697...  0.4477 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2648...  Training loss: 1.0569...  0.4605 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2649...  Training loss: 1.0430...  0.4501 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2650...  Training loss: 1.0608...  0.4628 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2651...  Training loss: 1.0705...  0.4477 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2652...  Training loss: 1.0418...  0.4548 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2653...  Training loss: 1.1100...  0.4502 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2654...  Training loss: 1.0576...  0.4574 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2655...  Training loss: 1.0469...  0.4500 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2656...  Training loss: 1.0757...  0.4582 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2657...  Training loss: 1.0655...  0.4355 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2658...  Training loss: 1.0502...  0.4540 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2659...  Training loss: 1.0672...  0.4550 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2660...  Training loss: 1.0477...  0.4481 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2661...  Training loss: 1.0220...  0.4507 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2662...  Training loss: 1.0582...  0.4655 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2663...  Training loss: 1.0696...  0.4550 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2664...  Training loss: 1.0587...  0.4537 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2665...  Training loss: 1.0630...  0.4617 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2666...  Training loss: 1.0710...  0.4470 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2667...  Training loss: 1.0340...  0.4484 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2668...  Training loss: 1.0706...  0.4484 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2669...  Training loss: 1.0698...  0.4536 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2670...  Training loss: 1.0636...  0.4611 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2671...  Training loss: 1.0515...  0.4529 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2672...  Training loss: 1.0381...  0.4515 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2673...  Training loss: 1.0572...  0.4484 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2674...  Training loss: 1.0322...  0.4537 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2675...  Training loss: 1.0648...  0.4515 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2676...  Training loss: 1.0402...  0.4522 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2677...  Training loss: 1.0432...  0.4600 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2678...  Training loss: 1.0572...  0.4521 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2679...  Training loss: 1.0361...  0.4405 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2680...  Training loss: 1.0490...  0.4561 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2681...  Training loss: 1.0492...  0.4522 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2682...  Training loss: 1.0439...  0.4539 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2683...  Training loss: 1.0404...  0.4503 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2684...  Training loss: 1.0681...  0.4546 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2685...  Training loss: 1.0580...  0.4547 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2686...  Training loss: 1.0406...  0.4546 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2687...  Training loss: 1.1113...  0.4526 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2688...  Training loss: 1.0446...  0.4575 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2689...  Training loss: 1.0404...  0.4462 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2690...  Training loss: 1.0661...  0.4547 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2691...  Training loss: 1.0567...  0.4315 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2692...  Training loss: 1.0414...  0.4554 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2693...  Training loss: 1.0635...  0.4492 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2694...  Training loss: 1.0507...  0.4357 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2695...  Training loss: 1.0219...  0.4519 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2696...  Training loss: 1.0592...  0.4493 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2697...  Training loss: 1.0521...  0.4549 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2698...  Training loss: 1.0535...  0.4544 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2699...  Training loss: 1.0550...  0.4423 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2700...  Training loss: 1.0517...  0.4528 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2701...  Training loss: 1.0167...  0.4551 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2702...  Training loss: 1.0617...  0.4548 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2703...  Training loss: 1.0613...  0.4548 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2704...  Training loss: 1.0555...  0.4579 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2705...  Training loss: 1.0446...  0.4515 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2706...  Training loss: 1.0277...  0.4498 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2707...  Training loss: 1.0364...  0.4602 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2708...  Training loss: 1.0295...  0.4514 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2709...  Training loss: 1.0535...  0.4495 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2710...  Training loss: 1.0253...  0.4556 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2711...  Training loss: 1.0420...  0.4539 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2712...  Training loss: 1.0521...  0.4295 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2713...  Training loss: 1.0195...  0.4565 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2714...  Training loss: 1.0300...  0.4513 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2715...  Training loss: 1.0523...  0.4526 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2716...  Training loss: 1.0507...  0.4527 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2717...  Training loss: 1.0400...  0.4501 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2718...  Training loss: 1.0631...  0.4593 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2719...  Training loss: 1.0473...  0.4504 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2720...  Training loss: 1.0244...  0.4558 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2721...  Training loss: 1.0928...  0.4533 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2722...  Training loss: 1.0370...  0.4583 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2723...  Training loss: 1.0279...  0.4544 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2724...  Training loss: 1.0564...  0.4523 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2725...  Training loss: 1.0489...  0.4550 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2726...  Training loss: 1.0406...  0.4490 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2727...  Training loss: 1.0578...  0.4505 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2728...  Training loss: 1.0270...  0.4545 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2729...  Training loss: 1.0249...  0.4558 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2730...  Training loss: 1.0502...  0.4477 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2731...  Training loss: 1.0470...  0.4323 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2732...  Training loss: 1.0409...  0.4517 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2733...  Training loss: 1.0477...  0.4503 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2734...  Training loss: 1.0519...  0.4561 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2735...  Training loss: 1.0249...  0.4592 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2736...  Training loss: 1.0636...  0.4518 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2737...  Training loss: 1.0451...  0.4526 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2738...  Training loss: 1.0405...  0.4498 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2739...  Training loss: 1.0356...  0.4546 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2740...  Training loss: 1.0249...  0.4502 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2741...  Training loss: 1.0503...  0.4510 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2742...  Training loss: 1.0211...  0.4595 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2743...  Training loss: 1.0506...  0.4512 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2744...  Training loss: 1.0372...  0.4515 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2745...  Training loss: 1.0330...  0.4451 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2746...  Training loss: 1.0495...  0.4602 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2747...  Training loss: 1.0242...  0.4538 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2748...  Training loss: 1.0221...  0.4535 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2749...  Training loss: 1.0378...  0.4529 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2750...  Training loss: 1.0383...  0.4558 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2751...  Training loss: 1.0247...  0.4460 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2752...  Training loss: 1.0563...  0.4562 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2753...  Training loss: 1.0557...  0.4548 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2754...  Training loss: 1.0236...  0.4554 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2755...  Training loss: 1.0933...  0.4446 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2756...  Training loss: 1.0269...  0.4512 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2757...  Training loss: 1.0291...  0.4435 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2758...  Training loss: 1.0556...  0.4430 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2759...  Training loss: 1.0598...  0.4556 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2760...  Training loss: 1.0221...  0.4400 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2761...  Training loss: 1.0541...  0.4526 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2762...  Training loss: 1.0265...  0.4497 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2763...  Training loss: 1.0028...  0.4524 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2764...  Training loss: 1.0397...  0.4458 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2765...  Training loss: 1.0512...  0.4502 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2766...  Training loss: 1.0339...  0.4582 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2767...  Training loss: 1.0408...  0.4570 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2768...  Training loss: 1.0409...  0.4522 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2769...  Training loss: 1.0289...  0.4634 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2770...  Training loss: 1.0474...  0.4539 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2771...  Training loss: 1.0572...  0.4330 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2772...  Training loss: 1.0272...  0.4500 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2773...  Training loss: 1.0354...  0.4541 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2774...  Training loss: 1.0109...  0.4339 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2775...  Training loss: 1.0368...  0.4490 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2776...  Training loss: 1.0109...  0.4524 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2777...  Training loss: 1.0545...  0.4534 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2778...  Training loss: 1.0273...  0.4535 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2779...  Training loss: 1.0218...  0.4549 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2780...  Training loss: 1.0338...  0.4485 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2781...  Training loss: 1.0190...  0.4516 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2782...  Training loss: 1.0340...  0.4535 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2783...  Training loss: 1.0371...  0.4496 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2784...  Training loss: 1.0391...  0.4454 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2785...  Training loss: 1.0220...  0.4539 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2786...  Training loss: 1.0475...  0.4562 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2787...  Training loss: 1.0419...  0.4561 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2788...  Training loss: 1.0189...  0.4428 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2789...  Training loss: 1.0912...  0.4481 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2790...  Training loss: 1.0332...  0.4411 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2791...  Training loss: 1.0116...  0.4516 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2792...  Training loss: 1.0521...  0.4369 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2793...  Training loss: 1.0424...  0.4513 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2794...  Training loss: 1.0225...  0.4490 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2795...  Training loss: 1.0478...  0.4525 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2796...  Training loss: 1.0110...  0.4509 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2797...  Training loss: 0.9945...  0.4560 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2798...  Training loss: 1.0311...  0.4536 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2799...  Training loss: 1.0422...  0.4550 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2800...  Training loss: 1.0301...  0.4510 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2801...  Training loss: 1.0309...  0.4562 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2802...  Training loss: 1.0373...  0.4504 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2803...  Training loss: 1.0070...  0.4582 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2804...  Training loss: 1.0399...  0.4553 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2805...  Training loss: 1.0324...  0.4546 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2806...  Training loss: 1.0308...  0.4521 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2807...  Training loss: 1.0281...  0.4543 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2808...  Training loss: 1.0150...  0.4555 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2809...  Training loss: 1.0277...  0.4502 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2810...  Training loss: 1.0060...  0.4433 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2811...  Training loss: 1.0449...  0.4563 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2812...  Training loss: 1.0089...  0.4509 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2813...  Training loss: 1.0288...  0.4588 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2814...  Training loss: 1.0495...  0.4516 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2815...  Training loss: 1.0120...  0.4520 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2816...  Training loss: 1.0112...  0.4580 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2817...  Training loss: 1.0310...  0.4553 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2818...  Training loss: 1.0195...  0.4547 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2819...  Training loss: 1.0231...  0.4382 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2820...  Training loss: 1.0226...  0.4521 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2821...  Training loss: 1.0353...  0.4505 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2822...  Training loss: 1.0120...  0.4585 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2823...  Training loss: 1.0844...  0.4500 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2824...  Training loss: 1.0234...  0.4573 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2825...  Training loss: 1.0052...  0.4535 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2826...  Training loss: 1.0444...  0.4545 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2827...  Training loss: 1.0411...  0.4573 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2828...  Training loss: 1.0141...  0.4588 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2829...  Training loss: 1.0346...  0.4489 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2830...  Training loss: 1.0136...  0.4528 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2831...  Training loss: 0.9886...  0.4555 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2832...  Training loss: 1.0165...  0.4596 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2833...  Training loss: 1.0239...  0.4512 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2834...  Training loss: 1.0257...  0.4528 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2835...  Training loss: 1.0307...  0.4622 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2836...  Training loss: 1.0343...  0.4535 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2837...  Training loss: 0.9988...  0.4485 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2838...  Training loss: 1.0379...  0.4497 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2839...  Training loss: 1.0411...  0.4554 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2840...  Training loss: 1.0201...  0.4558 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2841...  Training loss: 1.0254...  0.4576 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2842...  Training loss: 1.0079...  0.4492 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2843...  Training loss: 1.0215...  0.4497 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2844...  Training loss: 0.9933...  0.4504 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2845...  Training loss: 1.0328...  0.4492 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2846...  Training loss: 1.0098...  0.4530 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2847...  Training loss: 1.0202...  0.4540 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2848...  Training loss: 1.0327...  0.4559 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2849...  Training loss: 1.0027...  0.4503 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2850...  Training loss: 1.0063...  0.4550 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2851...  Training loss: 1.0197...  0.4334 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2852...  Training loss: 1.0198...  0.4538 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2853...  Training loss: 1.0135...  0.4541 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2854...  Training loss: 1.0268...  0.4562 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2855...  Training loss: 1.0395...  0.4563 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2856...  Training loss: 1.0111...  0.4462 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2857...  Training loss: 1.0741...  0.4548 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2858...  Training loss: 1.0228...  0.4506 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2859...  Training loss: 1.0125...  0.4536 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2860...  Training loss: 1.0433...  0.4527 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2861...  Training loss: 1.0331...  0.4580 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2862...  Training loss: 1.0009...  0.4543 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2863...  Training loss: 1.0256...  0.4433 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2864...  Training loss: 1.0017...  0.4585 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2865...  Training loss: 0.9846...  0.4538 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2866...  Training loss: 1.0159...  0.4614 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2867...  Training loss: 1.0222...  0.4548 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2868...  Training loss: 1.0121...  0.4512 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2869...  Training loss: 1.0219...  0.4521 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2870...  Training loss: 1.0149...  0.4548 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2871...  Training loss: 0.9925...  0.4572 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2872...  Training loss: 1.0271...  0.4584 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2873...  Training loss: 1.0319...  0.4556 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2874...  Training loss: 1.0109...  0.4565 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2875...  Training loss: 1.0046...  0.4520 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2876...  Training loss: 1.0029...  0.4534 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2877...  Training loss: 1.0144...  0.4348 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2878...  Training loss: 0.9959...  0.4531 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2879...  Training loss: 1.0191...  0.4539 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2880...  Training loss: 0.9977...  0.4494 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2881...  Training loss: 0.9949...  0.4497 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2882...  Training loss: 1.0250...  0.4500 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2883...  Training loss: 0.9917...  0.4525 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2884...  Training loss: 1.0092...  0.4380 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2885...  Training loss: 1.0211...  0.4501 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2886...  Training loss: 1.0042...  0.4399 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2887...  Training loss: 0.9959...  0.4582 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2888...  Training loss: 1.0311...  0.4515 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2889...  Training loss: 1.0232...  0.4491 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2890...  Training loss: 0.9955...  0.4528 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2891...  Training loss: 1.0621...  0.4498 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2892...  Training loss: 1.0145...  0.4528 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2893...  Training loss: 1.0121...  0.4459 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2894...  Training loss: 1.0380...  0.4604 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2895...  Training loss: 1.0284...  0.4458 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2896...  Training loss: 1.0047...  0.4566 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2897...  Training loss: 1.0286...  0.4490 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2898...  Training loss: 0.9996...  0.4575 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2899...  Training loss: 0.9826...  0.4541 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2900...  Training loss: 1.0081...  0.4482 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2901...  Training loss: 1.0080...  0.4574 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2902...  Training loss: 1.0189...  0.4475 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2903...  Training loss: 1.0041...  0.4518 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2904...  Training loss: 1.0166...  0.4483 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2905...  Training loss: 0.9897...  0.4510 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2906...  Training loss: 1.0160...  0.4553 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2907...  Training loss: 1.0243...  0.4530 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2908...  Training loss: 1.0062...  0.4518 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2909...  Training loss: 1.0008...  0.4341 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2910...  Training loss: 0.9837...  0.4596 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2911...  Training loss: 1.0218...  0.4545 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2912...  Training loss: 0.9887...  0.4500 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2913...  Training loss: 1.0221...  0.4500 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2914...  Training loss: 1.0026...  0.4525 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2915...  Training loss: 0.9981...  0.4566 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2916...  Training loss: 1.0114...  0.4533 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2917...  Training loss: 0.9849...  0.4623 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2918...  Training loss: 0.9992...  0.4479 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2919...  Training loss: 1.0159...  0.4631 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2920...  Training loss: 1.0132...  0.4560 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2921...  Training loss: 0.9812...  0.4579 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2922...  Training loss: 1.0174...  0.4551 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2923...  Training loss: 1.0184...  0.4533 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2924...  Training loss: 0.9909...  0.4569 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2925...  Training loss: 1.0587...  0.4575 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2926...  Training loss: 1.0060...  0.4543 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2927...  Training loss: 0.9867...  0.4494 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2928...  Training loss: 1.0318...  0.4570 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2929...  Training loss: 1.0168...  0.4528 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2930...  Training loss: 0.9915...  0.4573 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2931...  Training loss: 1.0064...  0.4616 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2932...  Training loss: 0.9985...  0.4545 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2933...  Training loss: 0.9792...  0.4621 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2934...  Training loss: 0.9991...  0.4566 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2935...  Training loss: 1.0122...  0.4505 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2936...  Training loss: 1.0038...  0.4477 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2937...  Training loss: 1.0067...  0.4517 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2938...  Training loss: 1.0237...  0.4558 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2939...  Training loss: 0.9861...  0.4522 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2940...  Training loss: 1.0120...  0.4514 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2941...  Training loss: 1.0114...  0.4518 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2942...  Training loss: 1.0091...  0.4464 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2943...  Training loss: 0.9929...  0.4519 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2944...  Training loss: 0.9893...  0.4482 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2945...  Training loss: 1.0044...  0.4615 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2946...  Training loss: 0.9928...  0.4631 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2947...  Training loss: 1.0216...  0.4585 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2948...  Training loss: 0.9822...  0.4562 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2949...  Training loss: 0.9965...  0.4532 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2950...  Training loss: 1.0003...  0.4542 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2951...  Training loss: 0.9905...  0.4571 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2952...  Training loss: 0.9957...  0.4476 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2953...  Training loss: 1.0054...  0.4548 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2954...  Training loss: 1.0060...  0.4529 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2955...  Training loss: 0.9845...  0.4523 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2956...  Training loss: 1.0170...  0.4529 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2957...  Training loss: 1.0083...  0.4617 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2958...  Training loss: 0.9864...  0.4492 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2959...  Training loss: 1.0578...  0.4528 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2960...  Training loss: 0.9963...  0.4618 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2961...  Training loss: 0.9850...  0.4557 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2962...  Training loss: 1.0165...  0.4613 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2963...  Training loss: 1.0167...  0.4511 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2964...  Training loss: 0.9946...  0.4571 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2965...  Training loss: 1.0057...  0.4534 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2966...  Training loss: 0.9859...  0.4568 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2967...  Training loss: 0.9683...  0.4553 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2968...  Training loss: 0.9900...  0.4534 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2969...  Training loss: 1.0019...  0.4534 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2970...  Training loss: 1.0054...  0.4543 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2971...  Training loss: 0.9968...  0.4556 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2972...  Training loss: 1.0006...  0.4473 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2973...  Training loss: 0.9710...  0.4535 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2974...  Training loss: 1.0191...  0.4444 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2975...  Training loss: 1.0146...  0.4605 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2976...  Training loss: 0.9893...  0.4602 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2977...  Training loss: 0.9993...  0.4467 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2978...  Training loss: 0.9905...  0.4539 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2979...  Training loss: 0.9948...  0.4562 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2980...  Training loss: 0.9772...  0.4500 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2981...  Training loss: 1.0099...  0.4565 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2982...  Training loss: 0.9804...  0.4529 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2983...  Training loss: 0.9896...  0.4557 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2984...  Training loss: 1.0002...  0.4597 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2985...  Training loss: 0.9798...  0.4523 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2986...  Training loss: 0.9920...  0.4525 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2987...  Training loss: 0.9932...  0.4611 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2988...  Training loss: 0.9957...  0.4455 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2989...  Training loss: 0.9820...  0.4634 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2990...  Training loss: 1.0170...  0.4545 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2991...  Training loss: 1.0085...  0.4555 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2992...  Training loss: 0.9791...  0.4572 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2993...  Training loss: 1.0465...  0.4480 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2994...  Training loss: 0.9946...  0.4531 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2995...  Training loss: 0.9744...  0.4542 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2996...  Training loss: 1.0162...  0.4532 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2997...  Training loss: 0.9983...  0.4547 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2998...  Training loss: 0.9717...  0.4533 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2999...  Training loss: 0.9976...  0.4524 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3000...  Training loss: 0.9848...  0.4553 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3001...  Training loss: 0.9684...  0.4482 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3002...  Training loss: 0.9993...  0.4597 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3003...  Training loss: 0.9951...  0.4545 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3004...  Training loss: 0.9887...  0.4530 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3005...  Training loss: 1.0012...  0.4490 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3006...  Training loss: 1.0051...  0.4511 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3007...  Training loss: 0.9787...  0.4571 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3008...  Training loss: 1.0076...  0.4563 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3009...  Training loss: 1.0096...  0.4568 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3010...  Training loss: 0.9838...  0.4586 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3011...  Training loss: 0.9843...  0.4467 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3012...  Training loss: 0.9801...  0.4480 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3013...  Training loss: 0.9989...  0.4523 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3014...  Training loss: 0.9721...  0.4505 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3015...  Training loss: 1.0123...  0.4433 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3016...  Training loss: 0.9802...  0.4596 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3017...  Training loss: 0.9998...  0.4631 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3018...  Training loss: 0.9997...  0.4519 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3019...  Training loss: 0.9762...  0.4540 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3020...  Training loss: 0.9733...  0.4337 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3021...  Training loss: 0.9937...  0.4477 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3022...  Training loss: 0.9939...  0.4444 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3023...  Training loss: 0.9913...  0.4443 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3024...  Training loss: 1.0063...  0.4601 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3025...  Training loss: 1.0029...  0.4534 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3026...  Training loss: 0.9828...  0.4505 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3027...  Training loss: 1.0489...  0.4371 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3028...  Training loss: 0.9938...  0.4526 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3029...  Training loss: 0.9882...  0.4531 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3030...  Training loss: 1.0201...  0.4514 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3031...  Training loss: 0.9935...  0.4537 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3032...  Training loss: 0.9841...  0.4507 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3033...  Training loss: 0.9909...  0.4564 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3034...  Training loss: 0.9785...  0.4515 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3035...  Training loss: 0.9632...  0.4590 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3036...  Training loss: 0.9913...  0.4522 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3037...  Training loss: 0.9833...  0.4554 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3038...  Training loss: 0.9906...  0.4454 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3039...  Training loss: 0.9868...  0.4561 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3040...  Training loss: 1.0021...  0.4546 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3041...  Training loss: 0.9770...  0.4519 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3042...  Training loss: 0.9981...  0.4558 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3043...  Training loss: 1.0065...  0.4581 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3044...  Training loss: 0.9909...  0.4584 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3045...  Training loss: 0.9786...  0.4524 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3046...  Training loss: 0.9673...  0.4514 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3047...  Training loss: 1.0003...  0.4530 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3048...  Training loss: 0.9718...  0.4552 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3049...  Training loss: 0.9949...  0.4391 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3050...  Training loss: 0.9742...  0.4523 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3051...  Training loss: 0.9817...  0.4546 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3052...  Training loss: 0.9855...  0.4631 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3053...  Training loss: 0.9798...  0.4559 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3054...  Training loss: 0.9902...  0.4514 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3055...  Training loss: 0.9844...  0.4560 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3056...  Training loss: 0.9943...  0.4581 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3057...  Training loss: 0.9753...  0.4476 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3058...  Training loss: 1.0076...  0.4533 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3059...  Training loss: 1.0012...  0.4688 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3060...  Training loss: 0.9667...  0.4605 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3061...  Training loss: 1.0409...  0.4535 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3062...  Training loss: 0.9770...  0.4613 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3063...  Training loss: 0.9705...  0.4544 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3064...  Training loss: 1.0114...  0.4525 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3065...  Training loss: 0.9912...  0.4527 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3066...  Training loss: 0.9770...  0.4553 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3067...  Training loss: 0.9820...  0.4466 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3068...  Training loss: 0.9702...  0.4521 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3069...  Training loss: 0.9523...  0.4571 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3070...  Training loss: 0.9968...  0.4511 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3071...  Training loss: 0.9947...  0.4564 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3072...  Training loss: 0.9883...  0.4483 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3073...  Training loss: 0.9745...  0.4513 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3074...  Training loss: 0.9892...  0.4659 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3075...  Training loss: 0.9607...  0.4617 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3076...  Training loss: 0.9993...  0.4642 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3077...  Training loss: 1.0003...  0.4547 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3078...  Training loss: 0.9789...  0.4568 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3079...  Training loss: 0.9812...  0.4558 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3080...  Training loss: 0.9676...  0.4563 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3081...  Training loss: 0.9802...  0.4429 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3082...  Training loss: 0.9714...  0.4450 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3083...  Training loss: 0.9992...  0.4538 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3084...  Training loss: 0.9718...  0.4495 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3085...  Training loss: 0.9619...  0.4547 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3086...  Training loss: 0.9874...  0.4607 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3087...  Training loss: 0.9704...  0.4529 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3088...  Training loss: 0.9779...  0.4580 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3089...  Training loss: 0.9991...  0.4555 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3090...  Training loss: 0.9905...  0.4510 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3091...  Training loss: 0.9694...  0.4495 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3092...  Training loss: 0.9865...  0.4507 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3093...  Training loss: 0.9944...  0.4488 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3094...  Training loss: 0.9674...  0.4501 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3095...  Training loss: 1.0380...  0.4584 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3096...  Training loss: 0.9768...  0.4483 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3097...  Training loss: 0.9610...  0.4530 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3098...  Training loss: 0.9955...  0.4493 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3099...  Training loss: 0.9847...  0.4512 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3100...  Training loss: 0.9660...  0.4421 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3101...  Training loss: 0.9864...  0.4544 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3102...  Training loss: 0.9618...  0.4528 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3103...  Training loss: 0.9500...  0.4490 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3104...  Training loss: 0.9723...  0.4544 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3105...  Training loss: 0.9864...  0.4531 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3106...  Training loss: 0.9749...  0.4545 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3107...  Training loss: 0.9725...  0.4513 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3108...  Training loss: 0.9785...  0.4551 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3109...  Training loss: 0.9479...  0.4478 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3110...  Training loss: 0.9945...  0.4491 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3111...  Training loss: 0.9829...  0.4545 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3112...  Training loss: 0.9603...  0.4576 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3113...  Training loss: 0.9717...  0.4443 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3114...  Training loss: 0.9670...  0.4538 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3115...  Training loss: 0.9742...  0.4540 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3116...  Training loss: 0.9606...  0.4525 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3117...  Training loss: 0.9769...  0.4541 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3118...  Training loss: 0.9644...  0.4570 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3119...  Training loss: 0.9762...  0.4500 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3120...  Training loss: 0.9644...  0.4526 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3121...  Training loss: 0.9520...  0.4617 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3122...  Training loss: 0.9683...  0.4462 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3123...  Training loss: 0.9748...  0.4541 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3124...  Training loss: 0.9765...  0.4512 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3125...  Training loss: 0.9699...  0.4567 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3126...  Training loss: 0.9860...  0.4498 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3127...  Training loss: 0.9882...  0.4475 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3128...  Training loss: 0.9609...  0.4508 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3129...  Training loss: 1.0290...  0.4592 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3130...  Training loss: 0.9715...  0.4581 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3131...  Training loss: 0.9647...  0.4470 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3132...  Training loss: 0.9876...  0.4542 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3133...  Training loss: 0.9800...  0.4570 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3134...  Training loss: 0.9620...  0.4480 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3135...  Training loss: 0.9686...  0.4568 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3136...  Training loss: 0.9585...  0.4563 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3137...  Training loss: 0.9345...  0.4529 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3138...  Training loss: 0.9780...  0.4501 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3139...  Training loss: 0.9632...  0.4520 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3140...  Training loss: 0.9708...  0.4424 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3141...  Training loss: 0.9721...  0.4578 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3142...  Training loss: 0.9773...  0.4553 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3143...  Training loss: 0.9521...  0.4541 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3144...  Training loss: 0.9870...  0.4521 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3145...  Training loss: 0.9839...  0.4588 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3146...  Training loss: 0.9614...  0.4526 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3147...  Training loss: 0.9581...  0.4547 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3148...  Training loss: 0.9535...  0.4355 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3149...  Training loss: 0.9694...  0.4548 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3150...  Training loss: 0.9435...  0.4534 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3151...  Training loss: 0.9650...  0.4442 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3152...  Training loss: 0.9547...  0.4460 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3153...  Training loss: 0.9486...  0.4521 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3154...  Training loss: 0.9757...  0.4439 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3155...  Training loss: 0.9499...  0.4516 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3156...  Training loss: 0.9559...  0.4548 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3157...  Training loss: 0.9728...  0.4461 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3158...  Training loss: 0.9743...  0.4534 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3159...  Training loss: 0.9557...  0.4510 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3160...  Training loss: 0.9804...  0.4629 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3161...  Training loss: 0.9825...  0.4505 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3162...  Training loss: 0.9469...  0.4440 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3163...  Training loss: 1.0222...  0.4525 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3164...  Training loss: 0.9700...  0.4474 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3165...  Training loss: 0.9545...  0.4509 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3166...  Training loss: 0.9870...  0.4554 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3167...  Training loss: 0.9738...  0.4582 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3168...  Training loss: 0.9476...  0.4564 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3169...  Training loss: 0.9569...  0.4528 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3170...  Training loss: 0.9559...  0.4531 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3171...  Training loss: 0.9337...  0.4611 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3172...  Training loss: 0.9697...  0.4545 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3173...  Training loss: 0.9690...  0.4530 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3174...  Training loss: 0.9681...  0.4573 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3175...  Training loss: 0.9519...  0.4519 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3176...  Training loss: 0.9581...  0.4537 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3177...  Training loss: 0.9421...  0.4541 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3178...  Training loss: 0.9702...  0.4545 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3179...  Training loss: 0.9765...  0.4516 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3180...  Training loss: 0.9460...  0.4514 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3181...  Training loss: 0.9452...  0.4526 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3182...  Training loss: 0.9486...  0.4494 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3183...  Training loss: 0.9628...  0.4459 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3184...  Training loss: 0.9473...  0.4587 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3185...  Training loss: 0.9639...  0.4478 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3186...  Training loss: 0.9342...  0.4579 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3187...  Training loss: 0.9516...  0.4535 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3188...  Training loss: 0.9635...  0.4516 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3189...  Training loss: 0.9528...  0.4522 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3190...  Training loss: 0.9538...  0.4515 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3191...  Training loss: 0.9648...  0.4555 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3192...  Training loss: 0.9647...  0.4559 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3193...  Training loss: 0.9514...  0.4617 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3194...  Training loss: 0.9674...  0.4608 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3195...  Training loss: 0.9772...  0.4523 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3196...  Training loss: 0.9476...  0.4516 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3197...  Training loss: 1.0112...  0.4457 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3198...  Training loss: 0.9591...  0.4544 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3199...  Training loss: 0.9477...  0.4560 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3200...  Training loss: 0.9784...  0.4528 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3201...  Training loss: 0.9848...  0.4471 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3202...  Training loss: 0.9529...  0.4595 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3203...  Training loss: 0.9583...  0.4513 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3204...  Training loss: 0.9557...  0.4555 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3205...  Training loss: 0.9355...  0.4545 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3206...  Training loss: 0.9523...  0.4537 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3207...  Training loss: 0.9492...  0.4527 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3208...  Training loss: 0.9590...  0.4610 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3209...  Training loss: 0.9420...  0.4544 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3210...  Training loss: 0.9712...  0.4539 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3211...  Training loss: 0.9354...  0.4484 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3212...  Training loss: 0.9681...  0.4538 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3213...  Training loss: 0.9652...  0.4515 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3214...  Training loss: 0.9577...  0.4552 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3215...  Training loss: 0.9495...  0.4523 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3216...  Training loss: 0.9354...  0.4469 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3217...  Training loss: 0.9626...  0.4571 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3218...  Training loss: 0.9327...  0.4486 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3219...  Training loss: 0.9613...  0.4623 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3220...  Training loss: 0.9303...  0.4538 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3221...  Training loss: 0.9510...  0.4515 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3222...  Training loss: 0.9490...  0.4503 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3223...  Training loss: 0.9345...  0.4592 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3224...  Training loss: 0.9487...  0.4535 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3225...  Training loss: 0.9476...  0.4515 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3226...  Training loss: 0.9516...  0.4536 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3227...  Training loss: 0.9310...  0.4546 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3228...  Training loss: 0.9594...  0.4549 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3229...  Training loss: 0.9554...  0.4549 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3230...  Training loss: 0.9356...  0.4568 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3231...  Training loss: 0.9980...  0.4507 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3232...  Training loss: 0.9590...  0.4600 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3233...  Training loss: 0.9498...  0.4526 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3234...  Training loss: 0.9721...  0.4559 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3235...  Training loss: 0.9556...  0.4534 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3236...  Training loss: 0.9428...  0.4556 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3237...  Training loss: 0.9552...  0.4543 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3238...  Training loss: 0.9304...  0.4477 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3239...  Training loss: 0.9124...  0.4553 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3240...  Training loss: 0.9457...  0.4534 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3241...  Training loss: 0.9544...  0.4548 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3242...  Training loss: 0.9581...  0.4503 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3243...  Training loss: 0.9357...  0.4491 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3244...  Training loss: 0.9491...  0.4530 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3245...  Training loss: 0.9298...  0.4518 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3246...  Training loss: 0.9535...  0.4555 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3247...  Training loss: 0.9642...  0.4523 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3248...  Training loss: 0.9522...  0.4558 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3249...  Training loss: 0.9363...  0.4587 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3250...  Training loss: 0.9271...  0.4540 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3251...  Training loss: 0.9486...  0.4568 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3252...  Training loss: 0.9250...  0.4516 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3253...  Training loss: 0.9557...  0.4510 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3254...  Training loss: 0.9208...  0.4523 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3255...  Training loss: 0.9245...  0.4483 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3256...  Training loss: 0.9548...  0.4527 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3257...  Training loss: 0.9381...  0.4560 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3258...  Training loss: 0.9348...  0.4501 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3259...  Training loss: 0.9487...  0.4562 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3260...  Training loss: 0.9425...  0.4499 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3261...  Training loss: 0.9394...  0.4558 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3262...  Training loss: 0.9543...  0.4560 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3263...  Training loss: 0.9566...  0.4511 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3264...  Training loss: 0.9296...  0.4570 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3265...  Training loss: 0.9943...  0.4502 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3266...  Training loss: 0.9407...  0.4598 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3267...  Training loss: 0.9300...  0.4536 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3268...  Training loss: 0.9569...  0.4501 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3269...  Training loss: 0.9602...  0.4580 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3270...  Training loss: 0.9353...  0.4504 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3271...  Training loss: 0.9567...  0.4602 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3272...  Training loss: 0.9410...  0.4533 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3273...  Training loss: 0.9170...  0.4502 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3274...  Training loss: 0.9442...  0.4476 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3275...  Training loss: 0.9476...  0.4628 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3276...  Training loss: 0.9498...  0.4525 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3277...  Training loss: 0.9377...  0.4582 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3278...  Training loss: 0.9445...  0.4567 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3279...  Training loss: 0.9187...  0.4532 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3280...  Training loss: 0.9494...  0.4528 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3281...  Training loss: 0.9518...  0.4571 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3282...  Training loss: 0.9344...  0.4450 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3283...  Training loss: 0.9275...  0.4483 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3284...  Training loss: 0.9149...  0.4524 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3285...  Training loss: 0.9289...  0.4529 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3286...  Training loss: 0.9302...  0.4590 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3287...  Training loss: 0.9533...  0.4597 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3288...  Training loss: 0.9193...  0.4450 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3289...  Training loss: 0.9363...  0.4746 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3290...  Training loss: 0.9334...  0.4550 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3291...  Training loss: 0.9306...  0.4556 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3292...  Training loss: 0.9387...  0.4513 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3293...  Training loss: 0.9422...  0.4525 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3294...  Training loss: 0.9455...  0.4447 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3295...  Training loss: 0.9222...  0.4471 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3296...  Training loss: 0.9488...  0.4401 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3297...  Training loss: 0.9497...  0.4548 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3298...  Training loss: 0.9290...  0.4524 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3299...  Training loss: 0.9850...  0.4464 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3300...  Training loss: 0.9319...  0.4559 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3301...  Training loss: 0.9279...  0.4560 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3302...  Training loss: 0.9541...  0.4524 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3303...  Training loss: 0.9513...  0.4537 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3304...  Training loss: 0.9335...  0.4491 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3305...  Training loss: 0.9396...  0.4560 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3306...  Training loss: 0.9304...  0.4461 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3307...  Training loss: 0.9012...  0.4423 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3308...  Training loss: 0.9338...  0.4512 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3309...  Training loss: 0.9305...  0.4541 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3310...  Training loss: 0.9434...  0.4536 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3311...  Training loss: 0.9387...  0.4492 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3312...  Training loss: 0.9392...  0.4505 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3313...  Training loss: 0.9067...  0.4538 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3314...  Training loss: 0.9420...  0.4440 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3315...  Training loss: 0.9421...  0.4610 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3316...  Training loss: 0.9397...  0.4597 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3317...  Training loss: 0.9291...  0.4441 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3318...  Training loss: 0.9159...  0.4396 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3319...  Training loss: 0.9355...  0.4508 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3320...  Training loss: 0.9118...  0.4604 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3321...  Training loss: 0.9431...  0.4533 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3322...  Training loss: 0.9191...  0.4568 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3323...  Training loss: 0.9185...  0.4416 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3324...  Training loss: 0.9373...  0.4593 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3325...  Training loss: 0.9056...  0.4498 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3326...  Training loss: 0.9363...  0.4478 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3327...  Training loss: 0.9365...  0.4588 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3328...  Training loss: 0.9379...  0.4494 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3329...  Training loss: 0.9217...  0.4582 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3330...  Training loss: 0.9426...  0.4487 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3331...  Training loss: 0.9356...  0.4493 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3332...  Training loss: 0.9158...  0.4680 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3333...  Training loss: 0.9845...  0.4674 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3334...  Training loss: 0.9283...  0.4564 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3335...  Training loss: 0.9148...  0.4524 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3336...  Training loss: 0.9453...  0.4540 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3337...  Training loss: 0.9390...  0.4498 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3338...  Training loss: 0.9245...  0.4545 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3339...  Training loss: 0.9308...  0.4570 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3340...  Training loss: 0.9100...  0.4549 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3341...  Training loss: 0.8987...  0.4554 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3342...  Training loss: 0.9282...  0.4583 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3343...  Training loss: 0.9260...  0.4512 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3344...  Training loss: 0.9396...  0.4523 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3345...  Training loss: 0.9256...  0.4599 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3346...  Training loss: 0.9268...  0.4495 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3347...  Training loss: 0.9042...  0.4503 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3348...  Training loss: 0.9252...  0.4547 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3349...  Training loss: 0.9329...  0.4475 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3350...  Training loss: 0.9262...  0.4523 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3351...  Training loss: 0.9242...  0.4617 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3352...  Training loss: 0.9075...  0.4538 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3353...  Training loss: 0.9263...  0.4541 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3354...  Training loss: 0.9118...  0.4463 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3355...  Training loss: 0.9433...  0.4555 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3356...  Training loss: 0.9133...  0.4487 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3357...  Training loss: 0.9175...  0.4515 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3358...  Training loss: 0.9315...  0.4545 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3359...  Training loss: 0.9012...  0.4639 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3360...  Training loss: 0.9254...  0.4531 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3361...  Training loss: 0.9230...  0.4531 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3362...  Training loss: 0.9339...  0.4405 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3363...  Training loss: 0.9180...  0.4340 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3364...  Training loss: 0.9362...  0.4565 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3365...  Training loss: 0.9417...  0.4374 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3366...  Training loss: 0.9112...  0.4542 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3367...  Training loss: 0.9746...  0.4534 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3368...  Training loss: 0.9287...  0.4533 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3369...  Training loss: 0.8981...  0.4528 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3370...  Training loss: 0.9401...  0.4541 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3371...  Training loss: 0.9391...  0.4485 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3372...  Training loss: 0.9089...  0.4415 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3373...  Training loss: 0.9271...  0.4495 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3374...  Training loss: 0.9109...  0.4551 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3375...  Training loss: 0.8997...  0.4638 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3376...  Training loss: 0.9115...  0.4634 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3377...  Training loss: 0.9241...  0.4564 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3378...  Training loss: 0.9307...  0.4509 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3379...  Training loss: 0.9132...  0.4465 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3380...  Training loss: 0.9165...  0.4484 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3381...  Training loss: 0.8943...  0.4521 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3382...  Training loss: 0.9342...  0.4504 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3383...  Training loss: 0.9273...  0.4544 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3384...  Training loss: 0.9276...  0.4514 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3385...  Training loss: 0.9074...  0.4473 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3386...  Training loss: 0.9120...  0.4434 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3387...  Training loss: 0.9161...  0.4482 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3388...  Training loss: 0.8882...  0.4363 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3389...  Training loss: 0.9321...  0.4541 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3390...  Training loss: 0.8948...  0.4485 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3391...  Training loss: 0.9041...  0.4644 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3392...  Training loss: 0.9205...  0.4524 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3393...  Training loss: 0.8954...  0.4540 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3394...  Training loss: 0.9174...  0.4599 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3395...  Training loss: 0.9275...  0.4567 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3396...  Training loss: 0.9250...  0.4569 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3397...  Training loss: 0.9182...  0.4583 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3398...  Training loss: 0.9306...  0.4503 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3399...  Training loss: 0.9310...  0.4536 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3400...  Training loss: 0.9004...  0.4517 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "save_every_n = 300\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        new_state = sess.run(initial_state)\n",
    "        for x, y in generate_batches(embeddings, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed_dict = {inputs: x,\n",
    "                    targets: y,\n",
    "                    keep_prob: prob,\n",
    "                    initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([loss, \n",
    "                                                 final_state, \n",
    "                                                 optimizer], \n",
    "                                                 feed_dict = feed_dict)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_characters(predictions, vocab_size, top = 5):\n",
    "    probs = np.squeeze(predictions)\n",
    "    probs[np.argsort(probs)[: -top]] = 0\n",
    "    probs = probs / np.sum(probs)\n",
    "    chars = np.random.choice(vocab_size, 1, p=probs)[0]\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_select(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime] \n",
    "    batch_size, num_steps = 1, 1\n",
    "    num_layers = 2\n",
    "    num_classes = len(vocab)\n",
    "    learning_rate = 0.001  \n",
    "    grad_clip = 5\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    inputs, targets, keep_prob = placeholders(batch_size, num_steps)\n",
    "    cell, initial_state = lstms(lstm_size, num_layers, batch_size, keep_prob)\n",
    "    one_hot_inputs = tf.one_hot(inputs, num_classes)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, one_hot_inputs, initial_state = initial_state)\n",
    "    final_state = state\n",
    "    prediction, logits = rnn_output(outputs, lstm_size, num_classes)\n",
    "    loss = losses(logits, targets, lstm_size, num_classes)\n",
    "    optimizer = cal_optimizer(loss, learning_rate, grad_clip)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(initial_state)\n",
    "        \n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {inputs: x,\n",
    "                    keep_prob: 1.,\n",
    "                    initial_state: new_state}\n",
    "            preds, new_state = sess.run([prediction, final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = select_characters(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {inputs: x,\n",
    "                    keep_prob: 1.,\n",
    "                    initial_state: new_state}\n",
    "            preds, new_state = sess.run([prediction, final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = select_characters(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling New Book Text Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f007b1ec6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f007b1ec6a0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f007b1ec6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f007b1ec6a0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007b311550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007b311550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007b311550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007b311550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007b1ecb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007b1ecb70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007b1ecb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007b1ecb70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i3400_l512.ckpt\n",
      "It was a powerful hand on his face. He had been clinking at them, and their for head shot. He went raised in his face. \"Then you've got the train to tell him from Gryffindor to have all about the Sorcerer's Stone, it shoused something that looked like a book full of shudly with Hagrid, though. \n",
      "\n",
      "He hadmed the floor in the far points from Gren fack, the books were glowing. The lips had brought his beht in fire pace to thy dirn't leak. He lat off his break, and the pull dooring plited of a scrook. \n",
      "\n",
      "Harry looked over his head and signer, left the cat as the theerest to his face what he was this too funly sounded them. He left to his for him. \n",
      "\n",
      "\"Yes, think you,\" Harry chanked at hard. \n",
      "\n",
      "\"I shall see you somewhere,\" said Dumbledore, \"and well -- I don't want to get the letter, Peeves,\" Harry sat down at the back of Harry's shouted, but the fastee hadn't got to tree to this, Harry wouldn't speed. The ston classed his head, he saw the father as he could have the bit of her wand as though she, she had almost seemed to see him feel in theme. \n",
      "\n",
      "\"Tell with you the endayely things?\" \n",
      "\n",
      "\"What's the past twin. I don't think we know.\" \n",
      "\n",
      "\"Why?\" \n",
      "\n",
      "\"What's what?\" Harry said to Harry. \"It won't be something about these,\" Harry said the thing branches, \"and heren't looked up at look and then say, I see you. \n",
      "\n",
      "\"Well, his name around, looked it all first.\" \n",
      "\n",
      "\"They really have to get presents at the black plain, and I couldn't be something expected you and then?\" \n",
      "\n",
      "At the stop that this told me he came out a leary teacher, so when you went to see what we seem to have a giess in the hat somewhere ensigenst and found, a fat toper. \n",
      "\n",
      "\"He's going,\" said Hagrid, \"but what happened out, Peacle, is watches.\" \n",
      "\n",
      "\"I'm sorry, a wizard, I'm not gave it to me,\" he said. \n",
      "\n",
      "\"No, sig!\" said Harry. \n",
      "\n",
      "\"They're dark, Potter,\" said Ron. \n",
      "\n",
      "\"They were,\" said Ron. \"He's not something.\" \n",
      "\n",
      "\"You don't have to speak to her feet a little click. Harry dook it to know a thanks,\" Hermione still, a sort of near the hall was a bun\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample_and_select(checkpoint, 2000, lstm_size, len(vocab), prime='It was a ')\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f00604d7ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f00604d7ac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f00604d7ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f00604d7ac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f0060446a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f0060446a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f0060446a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f0060446a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00604d7198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00604d7198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00604d7198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f00604d7198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i3400_l512.ckpt\n",
      "It was the one of the storm. They couldn't melt about he had a school closer that he was glumber back to the door, but he wasn't sure they didn't speak to the plood and sat a look withor them and the signerous, and the ball back on his hand. \n",
      "\n",
      "\"How are you got a lot here,\" Ron muttered, \"I think it's ground,\" said Hermione. \n",
      "\n",
      "\"Think yeh feel in,\" said Harry, trying to stup dow they left the fooest, all three of them weren't allowed to the portrait hole was only to him. Not manyed. He hadn't spell. \n",
      "\n",
      "\"I'm getting up?\" \n",
      "\n",
      "\"Oh, you could hard leave him off the coldants,\" said Ron. \"She hasn't heard it again. It's outta tree. He looked it again as the Stind at a somenow. I seem the table and ticket his eyes alone. \n",
      "\n",
      "His much face. \n",
      "\n",
      "\"It's toucher the platers things that ded, is too?\" said Harry. \"Wizer alone --\" \n",
      "\n",
      "\"I don't think,\" said Harry, \"I seel the Snate cloak in the face, Malfoy with the one pairsuof were scrating off coming and finished insode that our world before I got a broomstick and tell yeh that, I'd lowe at all. Bin hundred, and there's out, held up.\" \n",
      "\n",
      "\"He'll going to get a lot at once,?\" said Harry. \n",
      "\n",
      "\"Yel for it -- any me with here from the Stone.\" \n",
      "\n",
      "Harry clow helpever, and stopped down and stupped the place. It was lunking on the way down the note to toke his first players. \n",
      "\n",
      "\"He's done!\" \n",
      "\n",
      "\"You know that?\" Ron whispered to Harry. \n",
      "\n",
      "\"No, any my mustres! What's the matter any fool feet of yeurs, Fred. We're going to stay be about to get the properly brink. And she had to drago the ratily? I don't know what I was taking a prefect,\" he said. \"Yes,\" said Harry, wearing down out his hand. \n",
      "\n",
      "\"Someone, you know about the Snotch,\" he said, as if he stummed to have got up on the hopecatel, then he had to gran face in the stats behind him, something that had been left the first time they were all about the stone as they had to keep holiday. They walked through the last to see anything that wealld taken off the clat. Seeming the cluck fall. \n",
      "\n",
      "\"Yes,\" said Harry. \"I told\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i3400_l512.ckpt'\n",
    "samp = sample_and_select(checkpoint, 2000, lstm_size, len(vocab), prime=\"It was\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f0070089898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f0070089898>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f0070089898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f0070089898>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f006059a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f006059a710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f006059a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f006059a710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007010d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007010d710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007010d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f007010d710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i3000_l512.ckpt\n",
      "It was going on. \n",
      "\n",
      "Harry cuuldn't says out of the sprint where they speaked at them to the big orean. They hadn't seened to be still straight, showed mose tranning with his shoulder and had a bottle of crowd with a little motch forehead. He hed it around, but he couldn't have anyone seemed to great had to be so speace, to he clased to the direction. Harry had been londing to see yours, and they couldn't believe he was still so much. Hermione. A shave fly the school, the Professor McGonagall. Hagrid should have been a bin a seatury. There was a vigat madch in this corter, but the groan was around an into his hand and stepped drawing at the straight because they'd had it and get him for him. He winhed him a marble sigh fall. He was standing in the dark without enging in a cas he could. \"It stand what they always have to see anything, he hadn to show you.\" \n",
      "\n",
      "\"So --\" \n",
      "\n",
      "\"I know who you wait, Potter, something -- any,\" said Harry, pointing at his boots with norre. \"The Stone around you from me. It'll bo your will grounds -- all yeh -- and you're not saying I have been with at lost.\" \n",
      "\n",
      "\"Do you is in the direction!\" said Hermione anxiously, \"and that's the boy.?. I know he'll be in proudly, is him to do watch,\" Ron alwhat the firally parcing and stared at Ron, they were going to know what was nothing at there to them that there, even a tick to tee second when he was all sigh overhad they hadn't been such a given harry as he put it. \n",
      "\n",
      "\"I'm not sori,..\" said Ron. \n",
      "\n",
      "\"Never wond be what's aren theor?\" he shouted as People shared and coldled, stool hardly off the back of his face, and their way to a castle of ten it in the morention. \n",
      "\n",
      "\"Hagrid!\" said Harry, trying to free hows of anything extedsed, and to the teams with a large long late sight. \n",
      "\n",
      "\"What is the marlie wands.\" \n",
      "\n",
      "\"I mean.\" \n",
      "\n",
      "\"You don't gat anything,\" said Hagrid, \"there's a waye. The ofd at and more careful bean in the mirror, and dragon' this in the poricare that had a besterion. He won's gave. \n",
      "\n",
      "\"I think where you see hi\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i3000_l512.ckpt'\n",
    "samp = sample_and_select(checkpoint, 2000, lstm_size, len(vocab), prime=\"It was\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
